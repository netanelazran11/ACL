{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AOpGoE2T-YXS"
   },
   "source": [
    "# Neural Machine Translation with Attention\n",
    "\n",
    "Advanced Learning Fall 2025\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "For SUBMISSION:   \n",
    "\n",
    "Please upload the complete and executed `ipynb` to your git repository. Verify that all of your output can be viewed directly from github, and provide a link to that git file below.\n",
    "\n",
    "~~~\n",
    "STUDENT ID: 314992595\n",
    "~~~\n",
    "\n",
    "~~~\n",
    "STUDENT GIT LINK: https://github.com/netanelazran11/ACL/tree/main/HM3\n",
    "~~~\n",
    "In Addition, don't forget to add your ID to the files, and upload to moodle the html version:    \n",
    "  \n",
    "`PS3_Attention_2025_ID_[000000000].html`   \n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "PpJdYve9cZa6"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eecp2PAf7qJq"
   },
   "source": [
    "In this problem set we are going to jump into the depths of `seq2seq` and `attention` and build a couple of PyTorch translation mechanisms with some  twists.     \n",
    "\n",
    "\n",
    "*   Part 1 consists of a somewhat unorthodox `seq2seq` model for simple arithmetics\n",
    "*   Part 2 consists of an `seq2seq - attention` language translation model. We will use it for Hebrew and English.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "---"
   ],
   "metadata": {
    "id": "-VpUCez9gOZn"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ajNDsL5HlZN6"
   },
   "source": [
    "A **seq2seq** model (sequence-to-sequence model) is a type of neural network designed specifically to handle sequences of data. The model converts input sequences into other sequences of data. This makes them particularly useful for tasks involving language, where the input and output are naturally sequences of words.\n",
    "\n",
    "Here's a breakdown of how `seq2seq` models work:\n",
    "\n",
    "* The encoder takes the input sequence, like a sentence in English, and processes it to capture its meaning and context.\n",
    "\n",
    "* information is then passed to the decoder, which uses it to generate the output sequence, like a translation in French.\n",
    "\n",
    "* Attention mechanism (optional): Some `seq2seq` models also incorporate an attention mechanism. This allows the decoder to focus on specific parts of the input sequence that are most relevant to generating the next element in the output sequence.\n",
    "\n",
    "`seq2seq` models are used in many natural language processing (NLP) tasks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VbUDn4FObol7"
   },
   "source": [
    "imports: (feel free to add)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "crTe33wcD_Eg",
    "ExecuteTime": {
     "end_time": "2025-12-30T16:25:07.350060Z",
     "start_time": "2025-12-30T16:25:07.337246Z"
    }
   },
   "source": [
    "# from __future__ import unicode_literals, print_function, division\n",
    "# from io import open\n",
    "# import unicodedata\n",
    "import re\n",
    "import random\n",
    "import unicodedata\n",
    "\n",
    "import time\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from typing import List, Tuple\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler , Dataset\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CiwtNgENbx2g"
   },
   "source": [
    "## Part 1: Seq2Seq Arithmetic model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x1gWov3Gx67I"
   },
   "source": [
    "**Using RNN `seq2seq` model to \"learn\" simple arithmetics!**\n",
    "\n",
    "> Given the string \"54-7\", the model should return a prediction: \"47\".  \n",
    "> Given the string \"10+20\", the model should return a prediction: \"30\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dxo92ZgTy6ED"
   },
   "source": [
    "- Watch Lukas Biewald's short [video](https://youtu.be/MqugtGD605k?si=rAH34ZTJyYDj-XJ1) explaining `seq2seq` models and his toy application (somewhat outdated).\n",
    "- You can find the code for his example [here](https://github.com/lukas/ml-class/blob/master/videos/seq2seq/train.py).    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OEu_5YvqFPai"
   },
   "source": [
    "1.1) Using Lukas' code, implement a `seq2seq` network that can learn how to solve **addition AND substraction** of two numbers of maximum length of 4, using the following steps (similar to the example):      \n",
    "\n",
    "* Generate data; X: queries (two numbers), and Y: answers   \n",
    "* One-hot encode X and Y,\n",
    "* Build a `seq2seq` network (with LSTM, RepeatVector, and TimeDistributed layers)\n",
    "* Train the model.\n",
    "* While training, sample from the validation set at random so we can visualize the generated solutions against the true solutions.    \n",
    "\n",
    "Notes:  \n",
    "* The code in the example is quite old and based on Keras. You might have to adapt some of the code to overcome methods/code that is not supported anymore. Hint: for the evaluation part, review the type and format of the \"correct\" output - this will help you fix the unsupported \"model.predict_classes\".\n",
    "* Please use the parameters in the code cell below to train the model.     \n",
    "* Instead of using a `wandb.config` object, please use a simple dictionary instead.   \n",
    "* You don't need to run the model for more than 50 iterations (epochs) to get a gist of what is happening and what the algorithm is doing.\n",
    "* Extra credit if you can implement the network in PyTorch (this is not difficult).    \n",
    "* Extra credit if you are able to significantly improve the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qXJQqZbEbRup"
   },
   "source": [
    "1.2).\n",
    "\n",
    "a) Do you think this model performs well?  Why or why not?     \n",
    "b) What are its limitations?   \n",
    "c) What would you do to improve it?    \n",
    "d) Can you apply an attention mechanism to this model? Why or why not?   "
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "1.3).  \n",
    "\n",
    "Add attention to the model. Evaluate the performance against the `seq2seq` you trained above. Which one is performing better?"
   ],
   "metadata": {
    "id": "6wvRhhOcgmrQ"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "1.4)\n",
    "\n",
    "Using any neural network architecture of your liking, build  a model with the aim to beat the best performing model in 1.1 or 1.3. Compare your results in a meaningful way, and add a short explanation to why you think/thought your suggested network is better."
   ],
   "metadata": {
    "id": "AtEJK5IZkk8j"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "bwZKyzoBKl4G",
    "ExecuteTime": {
     "end_time": "2025-12-30T16:25:10.362344Z",
     "start_time": "2025-12-30T16:25:10.359784Z"
    }
   },
   "source": [
    "config = {}\n",
    "config[\"training_size\"] = 40000\n",
    "config[\"digits\"] = 4\n",
    "config[\"hidden_size\"] = 128\n",
    "config[\"batch_size\"] = 128\n",
    "config[\"iterations\"] = 50\n",
    "chars = '0123456789-+ '"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N6YxgNvo0W_o"
   },
   "source": [
    "SOLUTION:"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "### MISSING SOLUTION\n",
    "\"\"\"Seq2Seq Arithmetic in PyTorch (LSTM Encoder–Decoder) for addition AND subtraction.\n",
    "\"\"\"\n",
    "\n",
    "# Internal defaults\n",
    "SEED = int(config.get(\"seed\", 123))\n",
    "VAL_SIZE = int(config.get(\"val_size\", 10_000))\n",
    "EMB_DIM = int(config.get(\"emb_dim\", 64))\n",
    "LR = float(config.get(\"lr\", 1e-3))\n",
    "PRINT_SAMPLES = int(config.get(\"print_samples\", 12))\n",
    "\n",
    "EPOCHS = int(min(config[\"iterations\"], 50))\n",
    "DIGITS = int(config[\"digits\"])\n",
    "TRAIN_SIZE = int(config[\"training_size\"])\n",
    "BATCH_SIZE = int(config[\"batch_size\"])\n",
    "HIDDEN_SIZE = int(config[\"hidden_size\"])\n",
    "\n",
    "# Reproducibility\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "\n",
    "# Task constants\n",
    "MAX_INT = 10**DIGITS - 1\n",
    "MAX_QUERY_LEN = 2 * DIGITS + 1   # e.g., \"9999+9999\"\n",
    "MAX_ANS_LEN = DIGITS + 1         # e.g., \"19998\" or \"-9999\" (len 5)\n",
    "\n",
    "PAD = \" \"\n",
    "SOS = \"^\"  # decoder start token (internal)\n",
    "\n",
    "# Use the notebook's chars and add SOS internally\n",
    "CHARS = chars + SOS\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Vocab\n",
    "# -----------------------------\n",
    "class CharVocab:\n",
    "    def __init__(self, chars_: str):\n",
    "        self.chars = sorted(set(chars_))\n",
    "        self.stoi = {c: i for i, c in enumerate(self.chars)}\n",
    "        self.itos = {i: c for c, i in self.stoi.items()}\n",
    "        self.pad_idx = self.stoi[PAD]\n",
    "        self.sos_idx = self.stoi[SOS]\n",
    "\n",
    "    @property\n",
    "    def size(self) -> int:\n",
    "        return len(self.chars)\n",
    "\n",
    "    def encode(self, s: str, maxlen: int) -> torch.Tensor:\n",
    "        idx = [self.stoi[ch] for ch in s[:maxlen]]\n",
    "        if len(idx) < maxlen:\n",
    "            idx += [self.pad_idx] * (maxlen - len(idx))\n",
    "        return torch.tensor(idx, dtype=torch.long)\n",
    "\n",
    "    def decode(self, idx: torch.Tensor) -> str:\n",
    "        return \"\".join(self.itos[int(i)] for i in idx)\n",
    "\n",
    "\n",
    "vocab = CharVocab(CHARS)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Dataset\n",
    "# -----------------------------\n",
    "class ArithmeticSeq2SeqDataset(Dataset):\n",
    "    \"\"\"Returns (x, dec_in, y) index tensors.\n",
    "\n",
    "    x:      (T_in,) query\n",
    "    dec_in: (T_out,) decoder input = SOS + y[:-1]\n",
    "    y:      (T_out,) target answer\n",
    "\n",
    "    Loss ignores PAD positions in y.\n",
    "    \"\"\"\n",
    "    def __init__(self, n: int, seed: int):\n",
    "        self.n = n\n",
    "        rng = np.random.default_rng(seed)\n",
    "\n",
    "        self.questions: List[str] = []\n",
    "        self.answers: List[str] = []\n",
    "\n",
    "        seen: set[Tuple[str, int, int]] = set()\n",
    "\n",
    "        while len(self.questions) < n:\n",
    "            a = int(rng.integers(0, MAX_INT + 1))\n",
    "            b = int(rng.integers(0, MAX_INT + 1))\n",
    "            op = \"+\" if rng.random() < 0.5 else \"-\"\n",
    "\n",
    "            key = (op, a, b)  # do NOT sort (subtraction not commutative)\n",
    "            if key in seen:\n",
    "                continue\n",
    "            seen.add(key)\n",
    "\n",
    "            ans = a + b if op == \"+\" else a - b\n",
    "\n",
    "            q = f\"{a}{op}{b}\".ljust(MAX_QUERY_LEN)\n",
    "            y = str(ans).ljust(MAX_ANS_LEN)\n",
    "\n",
    "            self.questions.append(q)\n",
    "            self.answers.append(y)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.n\n",
    "\n",
    "    def __getitem__(self, i: int):\n",
    "        q = self.questions[i]\n",
    "        y = self.answers[i]\n",
    "\n",
    "        x = vocab.encode(q, MAX_QUERY_LEN)\n",
    "        y_tgt = vocab.encode(y, MAX_ANS_LEN)\n",
    "\n",
    "        dec_in_str = (SOS + y[:-1]).ljust(MAX_ANS_LEN)\n",
    "        dec_in = vocab.encode(dec_in_str, MAX_ANS_LEN)\n",
    "\n",
    "        return x, dec_in, y_tgt\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Model\n",
    "# -----------------------------\n",
    "class Seq2SeqLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size: int, emb_dim: int, hidden_size: int):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.encoder = nn.LSTM(emb_dim, hidden_size, batch_first=True)\n",
    "        self.decoder = nn.LSTM(emb_dim, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, dec_in: torch.Tensor) -> torch.Tensor:\n",
    "        # x: (B, T_in), dec_in: (B, T_out)\n",
    "        x_emb = self.emb(x)\n",
    "        _, (h, c) = self.encoder(x_emb)\n",
    "\n",
    "        dec_emb = self.emb(dec_in)\n",
    "        dec_out, _ = self.decoder(dec_emb, (h, c))\n",
    "        logits = self.fc(dec_out)  # (B, T_out, V)\n",
    "        return logits\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def greedy_decode(self, x: torch.Tensor, max_len: int) -> torch.Tensor:\n",
    "        x_emb = self.emb(x)\n",
    "        _, (h, c) = self.encoder(x_emb)\n",
    "\n",
    "        B = x.size(0)\n",
    "        y_pred = torch.full((B, max_len), vocab.pad_idx, dtype=torch.long, device=x.device)\n",
    "        prev = torch.full((B, 1), vocab.sos_idx, dtype=torch.long, device=x.device)\n",
    "\n",
    "        for t in range(max_len):\n",
    "            prev_emb = self.emb(prev)\n",
    "            dec_out, (h, c) = self.decoder(prev_emb, (h, c))\n",
    "            logits = self.fc(dec_out.squeeze(1))      # (B, V)\n",
    "            next_idx = torch.argmax(logits, dim=-1)   # (B,)\n",
    "            y_pred[:, t] = next_idx\n",
    "            prev = next_idx.unsqueeze(1)\n",
    "\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Utils\n",
    "# -----------------------------\n",
    "def token_acc_ignore_pad(pred: torch.Tensor, y: torch.Tensor, pad_idx: int) -> float:\n",
    "    mask = (y != pad_idx)\n",
    "    if mask.sum().item() == 0:\n",
    "        return 0.0\n",
    "    return (((pred == y) & mask).sum().item()) / mask.sum().item()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_one_epoch(model: Seq2SeqLSTM, loader: DataLoader) -> Tuple[float, float]:\n",
    "    model.eval()\n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index=vocab.pad_idx)\n",
    "\n",
    "    total_loss, total_acc, n_batches = 0.0, 0.0, 0\n",
    "\n",
    "    for x, dec_in, y in loader:\n",
    "        x, dec_in, y = x.to(device), dec_in.to(device), y.to(device)\n",
    "        logits = model(x, dec_in)\n",
    "\n",
    "        loss = loss_fn(logits.reshape(-1, vocab.size), y.reshape(-1))\n",
    "        pred_idx = torch.argmax(logits, dim=-1)\n",
    "        acc = token_acc_ignore_pad(pred_idx, y, vocab.pad_idx)\n",
    "\n",
    "        total_loss += float(loss.item())\n",
    "        total_acc += float(acc)\n",
    "        n_batches += 1\n",
    "\n",
    "    return total_loss / max(n_batches, 1), total_acc / max(n_batches, 1)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def print_samples(model: Seq2SeqLSTM, dataset: ArithmeticSeq2SeqDataset, n: int) -> None:\n",
    "    model.eval()\n",
    "    rng = np.random.default_rng(SEED + 999)\n",
    "    idxs = rng.choice(len(dataset), size=n, replace=False)\n",
    "\n",
    "    xs, qs, ys_true = [], [], []\n",
    "    for i in idxs:\n",
    "        q = dataset.questions[i]\n",
    "        y = dataset.answers[i]\n",
    "        xs.append(vocab.encode(q, MAX_QUERY_LEN))\n",
    "        qs.append(q.strip())\n",
    "        ys_true.append(y.strip())\n",
    "\n",
    "    x_batch = torch.stack(xs, dim=0).to(device)\n",
    "    y_pred_idx = model.greedy_decode(x_batch, MAX_ANS_LEN)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"Samples from validation (greedy decode)\")\n",
    "    print(\"=\" * 80)\n",
    "    for q, y_t, y_p in zip(qs, ys_true, y_pred_idx):\n",
    "        pred = vocab.decode(y_p).replace(SOS, \"\").strip()\n",
    "        ok = \"✓\" if pred == y_t else \"✗\"\n",
    "        print(f\"{ok}  Q: {q:>12}   TRUE: {y_t:>6}   PRED: {pred:>6}\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Train\n",
    "# -----------------------------\n",
    "def main():\n",
    "    print(f\"Device: {device}\")\n",
    "    print(f\"config: {config}\")\n",
    "    print(f\"chars:  {chars!r}\")\n",
    "    print(f\"Vocab size: {vocab.size}  |  Vocab: {vocab.chars}\")\n",
    "    print(f\"MAX_QUERY_LEN={MAX_QUERY_LEN}, MAX_ANS_LEN={MAX_ANS_LEN}\")\n",
    "    print(f\"TRAIN_SIZE={TRAIN_SIZE}, VAL_SIZE={VAL_SIZE}, BATCH_SIZE={BATCH_SIZE}, EPOCHS={EPOCHS}\")\n",
    "\n",
    "    train_ds = ArithmeticSeq2SeqDataset(TRAIN_SIZE, seed=SEED)\n",
    "    val_ds = ArithmeticSeq2SeqDataset(VAL_SIZE, seed=SEED + 1)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
    "\n",
    "    model = Seq2SeqLSTM(vocab.size, EMB_DIM, HIDDEN_SIZE).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index=vocab.pad_idx)\n",
    "\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        model.train()\n",
    "        run_loss, run_acc, n_batches = 0.0, 0.0, 0\n",
    "\n",
    "        for x, dec_in, y in train_loader:\n",
    "            x, dec_in, y = x.to(device), dec_in.to(device), y.to(device)\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            logits = model(x, dec_in)\n",
    "\n",
    "            loss = loss_fn(logits.reshape(-1, vocab.size), y.reshape(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            pred_idx = torch.argmax(logits, dim=-1)\n",
    "            acc = token_acc_ignore_pad(pred_idx, y, vocab.pad_idx)\n",
    "\n",
    "            run_loss += float(loss.item())\n",
    "            run_acc += float(acc)\n",
    "            n_batches += 1\n",
    "\n",
    "        train_loss = run_loss / max(n_batches, 1)\n",
    "        train_acc = run_acc / max(n_batches, 1)\n",
    "\n",
    "        val_loss, val_acc = eval_one_epoch(model, val_loader)\n",
    "\n",
    "        print(\n",
    "            f\"\\nEpoch {epoch:02d}/{EPOCHS}  |  \"\n",
    "            f\"train loss={train_loss:.4f} acc={train_acc:.4f}  |  \"\n",
    "            f\"val loss={val_loss:.4f} acc={val_acc:.4f}\"\n",
    "        )\n",
    "        print_samples(model, val_ds, n=PRINT_SAMPLES)\n",
    "\n",
    "    # Final sanity check\n",
    "    model.eval()\n",
    "    tests = [\"54-7\", \"10+20\", \"9999+9999\", \"0-9999\", \"1234-9999\", \"777+888\"]\n",
    "    print(\"\\nFinal sanity check (greedy):\")\n",
    "    for t in tests:\n",
    "        q = t.ljust(MAX_QUERY_LEN)\n",
    "        x = vocab.encode(q, MAX_QUERY_LEN).unsqueeze(0).to(device)\n",
    "        y_pred = model.greedy_decode(x, MAX_ANS_LEN)[0]\n",
    "        pred = vocab.decode(y_pred).replace(SOS, \"\").strip()\n",
    "        print(f\"Q: {t:>10}   PRED: {pred:>6}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "metadata": {
    "id": "GlF7abtLjz06",
    "ExecuteTime": {
     "end_time": "2025-12-30T16:29:02.227362Z",
     "start_time": "2025-12-30T16:25:11.415211Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "config: {'training_size': 40000, 'digits': 4, 'hidden_size': 128, 'batch_size': 128, 'iterations': 50}\n",
      "chars:  '0123456789-+ '\n",
      "Vocab size: 14  |  Vocab: [' ', '+', '-', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '^']\n",
      "MAX_QUERY_LEN=9, MAX_ANS_LEN=5\n",
      "TRAIN_SIZE=40000, VAL_SIZE=10000, BATCH_SIZE=128, EPOCHS=50\n",
      "\n",
      "Epoch 01/50  |  train loss=2.0806 acc=0.2155  |  val loss=1.9495 acc=0.2560\n",
      "\n",
      "================================================================================\n",
      "Samples from validation (greedy decode)\n",
      "================================================================================\n",
      "✗  Q:    3844+9270   TRUE:  13114   PRED:  14444\n",
      "✗  Q:    2565+6526   TRUE:   9091   PRED:  80442\n",
      "✗  Q:     801-3219   TRUE:  -2418   PRED:  -2293\n",
      "✗  Q:    3984+1770   TRUE:   5754   PRED:  64294\n",
      "✗  Q:    3735+2615   TRUE:   6350   PRED:  64294\n",
      "✗  Q:    5688+7759   TRUE:  13447   PRED:  14242\n",
      "✗  Q:    2025+3260   TRUE:   5285   PRED:  42164\n",
      "✗  Q:    8524+7403   TRUE:  15927   PRED:  14444\n",
      "✗  Q:     980+5556   TRUE:   6536   PRED:  94444\n",
      "✗  Q:    1911-2754   TRUE:   -843   PRED:  -1429\n",
      "✗  Q:    1702+1297   TRUE:   2999   PRED:  42164\n",
      "✗  Q:    6529+7415   TRUE:  13944   PRED:  14442\n",
      "\n",
      "Epoch 02/50  |  train loss=1.8697 acc=0.2902  |  val loss=1.8017 acc=0.3222\n",
      "\n",
      "================================================================================\n",
      "Samples from validation (greedy decode)\n",
      "================================================================================\n",
      "✗  Q:    3844+9270   TRUE:  13114   PRED:  13055\n",
      "✗  Q:    2565+6526   TRUE:   9091   PRED:  90150\n",
      "✗  Q:     801-3219   TRUE:  -2418   PRED:  -2052\n",
      "✗  Q:    3984+1770   TRUE:   5754   PRED:  55555\n",
      "✗  Q:    3735+2615   TRUE:   6350   PRED:  55555\n",
      "✗  Q:    5688+7759   TRUE:  13447   PRED:  13405\n",
      "✗  Q:    2025+3260   TRUE:   5285   PRED:  55555\n",
      "✗  Q:    8524+7403   TRUE:  15927   PRED:  15515\n",
      "✗  Q:     980+5556   TRUE:   6536   PRED:  89555\n",
      "✗  Q:    1911-2754   TRUE:   -843   PRED:  -1525\n",
      "✗  Q:    1702+1297   TRUE:   2999   PRED:  45152\n",
      "✗  Q:    6529+7415   TRUE:  13944   PRED:  13455\n",
      "\n",
      "Epoch 03/50  |  train loss=1.7603 acc=0.3351  |  val loss=1.7229 acc=0.3442\n",
      "\n",
      "================================================================================\n",
      "Samples from validation (greedy decode)\n",
      "================================================================================\n",
      "✗  Q:    3844+9270   TRUE:  13114   PRED:  13055\n",
      "✗  Q:    2565+6526   TRUE:   9091   PRED:  88355\n",
      "✗  Q:     801-3219   TRUE:  -2418   PRED:  -2226\n",
      "✗  Q:    3984+1770   TRUE:   5754   PRED:  60555\n",
      "✗  Q:    3735+2615   TRUE:   6350   PRED:  66655\n",
      "✗  Q:    5688+7759   TRUE:  13447   PRED:  13855\n",
      "✗  Q:    2025+3260   TRUE:   5285   PRED:  55555\n",
      "✗  Q:    8524+7403   TRUE:  15927   PRED:  15555\n",
      "✗  Q:     980+5556   TRUE:   6536   PRED:  72655\n",
      "✗  Q:    1911-2754   TRUE:   -843   PRED:  -1265\n",
      "✗  Q:    1702+1297   TRUE:   2999   PRED:  35165\n",
      "✗  Q:    6529+7415   TRUE:  13944   PRED:  13855\n",
      "\n",
      "Epoch 04/50  |  train loss=1.6894 acc=0.3566  |  val loss=1.6623 acc=0.3645\n",
      "\n",
      "================================================================================\n",
      "Samples from validation (greedy decode)\n",
      "================================================================================\n",
      "✗  Q:    3844+9270   TRUE:  13114   PRED:  13485\n",
      "✗  Q:    2565+6526   TRUE:   9091   PRED:  90108\n",
      "✗  Q:     801-3219   TRUE:  -2418   PRED:  -2215\n",
      "✗  Q:    3984+1770   TRUE:   5754   PRED:  57058\n",
      "✗  Q:    3735+2615   TRUE:   6350   PRED:  65758\n",
      "✗  Q:    5688+7759   TRUE:  13447   PRED:  13858\n",
      "✗  Q:    2025+3260   TRUE:   5285   PRED:  54108\n",
      "✗  Q:    8524+7403   TRUE:  15927   PRED:  15808\n",
      "✗  Q:     980+5556   TRUE:   6536   PRED:  72108\n",
      "✗  Q:    1911-2754   TRUE:   -843   PRED:  -1015\n",
      "✗  Q:    1702+1297   TRUE:   2999   PRED:  30758\n",
      "✗  Q:    6529+7415   TRUE:  13944   PRED:  14098\n",
      "\n",
      "Epoch 05/50  |  train loss=1.6371 acc=0.3724  |  val loss=1.6141 acc=0.3769\n",
      "\n",
      "================================================================================\n",
      "Samples from validation (greedy decode)\n",
      "================================================================================\n",
      "✗  Q:    3844+9270   TRUE:  13114   PRED:  13222\n",
      "✗  Q:    2565+6526   TRUE:   9091   PRED:  90222\n",
      "✗  Q:     801-3219   TRUE:  -2418   PRED:  -2522\n",
      "✗  Q:    3984+1770   TRUE:   5754   PRED:  55222\n",
      "✗  Q:    3735+2615   TRUE:   6350   PRED:  62222\n",
      "✗  Q:    5688+7759   TRUE:  13447   PRED:  13422\n",
      "✗  Q:    2025+3260   TRUE:   5285   PRED:  52222\n",
      "✗  Q:    8524+7403   TRUE:  15927   PRED:  15809\n",
      "✗  Q:     980+5556   TRUE:   6536   PRED:  72222\n",
      "✗  Q:    1911-2754   TRUE:   -843   PRED:  -1022\n",
      "✗  Q:    1702+1297   TRUE:   2999   PRED:  32222\n",
      "✗  Q:    6529+7415   TRUE:  13944   PRED:  14222\n",
      "\n",
      "Epoch 06/50  |  train loss=1.5889 acc=0.3891  |  val loss=1.5637 acc=0.3960\n",
      "\n",
      "================================================================================\n",
      "Samples from validation (greedy decode)\n",
      "================================================================================\n",
      "✗  Q:    3844+9270   TRUE:  13114   PRED:  13008\n",
      "✗  Q:    2565+6526   TRUE:   9091   PRED:  90108\n",
      "✗  Q:     801-3219   TRUE:  -2418   PRED:  -2088\n",
      "✗  Q:    3984+1770   TRUE:   5754   PRED:  58088\n",
      "✗  Q:    3735+2615   TRUE:   6350   PRED:  63888\n",
      "✗  Q:    5688+7759   TRUE:  13447   PRED:  13388\n",
      "✗  Q:    2025+3260   TRUE:   5285   PRED:  52888\n",
      "✗  Q:    8524+7403   TRUE:  15927   PRED:  15808\n",
      "✗  Q:     980+5556   TRUE:   6536   PRED:  62888\n",
      "✗  Q:    1911-2754   TRUE:   -843   PRED:  -1008\n",
      "✗  Q:    1702+1297   TRUE:   2999   PRED:  30088\n",
      "✗  Q:    6529+7415   TRUE:  13944   PRED:  13888\n",
      "\n",
      "Epoch 07/50  |  train loss=1.5556 acc=0.3999  |  val loss=1.5560 acc=0.3960\n",
      "\n",
      "================================================================================\n",
      "Samples from validation (greedy decode)\n",
      "================================================================================\n",
      "✗  Q:    3844+9270   TRUE:  13114   PRED:  13187\n",
      "✗  Q:    2565+6526   TRUE:   9091   PRED:  93307\n",
      "✗  Q:     801-3219   TRUE:  -2418   PRED:  -2431\n",
      "✗  Q:    3984+1770   TRUE:   5754   PRED:  60148\n",
      "✗  Q:    3735+2615   TRUE:   6350   PRED:  64872\n",
      "✗  Q:    5688+7759   TRUE:  13447   PRED:  13487\n",
      "✗  Q:    2025+3260   TRUE:   5285   PRED:  52487\n",
      "✗  Q:    8524+7403   TRUE:  15927   PRED:  16014\n",
      "✗  Q:     980+5556   TRUE:   6536   PRED:  64872\n",
      "✗  Q:    1911-2754   TRUE:   -843   PRED:  -1087\n",
      "✗  Q:    1702+1297   TRUE:   2999   PRED:  32222\n",
      "✗  Q:    6529+7415   TRUE:  13944   PRED:  14014\n",
      "\n",
      "Epoch 08/50  |  train loss=1.5282 acc=0.4081  |  val loss=1.5191 acc=0.4081\n",
      "\n",
      "================================================================================\n",
      "Samples from validation (greedy decode)\n",
      "================================================================================\n",
      "✗  Q:    3844+9270   TRUE:  13114   PRED:  13012\n",
      "✗  Q:    2565+6526   TRUE:   9091   PRED:  93114\n",
      "✗  Q:     801-3219   TRUE:  -2418   PRED:  -2437\n",
      "✗  Q:    3984+1770   TRUE:   5754   PRED:  57777\n",
      "✗  Q:    3735+2615   TRUE:   6350   PRED:  63114\n",
      "✗  Q:    5688+7759   TRUE:  13447   PRED:  13377\n",
      "✗  Q:    2025+3260   TRUE:   5285   PRED:  52471\n",
      "✗  Q:    8524+7403   TRUE:  15927   PRED:  15972\n",
      "✗  Q:     980+5556   TRUE:   6536   PRED:  61147\n",
      "✗  Q:    1911-2754   TRUE:   -843   PRED:  -1127\n",
      "✗  Q:    1702+1297   TRUE:   2999   PRED:  32147\n",
      "✗  Q:    6529+7415   TRUE:  13944   PRED:  14167\n",
      "\n",
      "Epoch 09/50  |  train loss=1.5032 acc=0.4165  |  val loss=1.4959 acc=0.4144\n",
      "\n",
      "================================================================================\n",
      "Samples from validation (greedy decode)\n",
      "================================================================================\n",
      "✗  Q:    3844+9270   TRUE:  13114   PRED:  13000\n",
      "✗  Q:    2565+6526   TRUE:   9091   PRED:  90277\n",
      "✗  Q:     801-3219   TRUE:  -2418   PRED:  -2707\n",
      "✗  Q:    3984+1770   TRUE:   5754   PRED:  55777\n",
      "✗  Q:    3735+2615   TRUE:   6350   PRED:  62377\n",
      "✗  Q:    5688+7759   TRUE:  13447   PRED:  13377\n",
      "✗  Q:    2025+3260   TRUE:   5285   PRED:  50777\n",
      "✗  Q:    8524+7403   TRUE:  15927   PRED:  15800\n",
      "✗  Q:     980+5556   TRUE:   6536   PRED:  61877\n",
      "✗  Q:    1911-2754   TRUE:   -843   PRED:  -1000\n",
      "✗  Q:    1702+1297   TRUE:   2999   PRED:  29877\n",
      "✗  Q:    6529+7415   TRUE:  13944   PRED:  14000\n",
      "\n",
      "Epoch 10/50  |  train loss=1.4851 acc=0.4228  |  val loss=1.4857 acc=0.4185\n",
      "\n",
      "================================================================================\n",
      "Samples from validation (greedy decode)\n",
      "================================================================================\n",
      "✗  Q:    3844+9270   TRUE:  13114   PRED:  13011\n",
      "✗  Q:    2565+6526   TRUE:   9091   PRED:  89114\n",
      "✗  Q:     801-3219   TRUE:  -2418   PRED:  -2411\n",
      "✗  Q:    3984+1770   TRUE:   5754   PRED:  57775\n",
      "✗  Q:    3735+2615   TRUE:   6350   PRED:  61104\n",
      "✗  Q:    5688+7759   TRUE:  13447   PRED:  13411\n",
      "✗  Q:    2025+3260   TRUE:   5285   PRED:  51141\n",
      "✗  Q:    8524+7403   TRUE:  15927   PRED:  16011\n",
      "✗  Q:     980+5556   TRUE:   6536   PRED:  69111\n",
      "✗  Q:    1911-2754   TRUE:   -843   PRED:  -1011\n",
      "✗  Q:    1702+1297   TRUE:   2999   PRED:  30114\n",
      "✗  Q:    6529+7415   TRUE:  13944   PRED:  14011\n",
      "\n",
      "Epoch 11/50  |  train loss=1.4624 acc=0.4326  |  val loss=1.4717 acc=0.4198\n",
      "\n",
      "================================================================================\n",
      "Samples from validation (greedy decode)\n",
      "================================================================================\n",
      "✗  Q:    3844+9270   TRUE:  13114   PRED:  13222\n",
      "✗  Q:    2565+6526   TRUE:   9091   PRED:  90257\n",
      "✗  Q:     801-3219   TRUE:  -2418   PRED:  -2722\n",
      "✗  Q:    3984+1770   TRUE:   5754   PRED:  57225\n",
      "✗  Q:    3735+2615   TRUE:   6350   PRED:  61104\n",
      "✗  Q:    5688+7759   TRUE:  13447   PRED:  13422\n",
      "✗  Q:    2025+3260   TRUE:   5285   PRED:  50570\n",
      "✗  Q:    8524+7403   TRUE:  15927   PRED:  15772\n",
      "✗  Q:     980+5556   TRUE:   6536   PRED:  62955\n",
      "✗  Q:    1911-2754   TRUE:   -843   PRED:  -7522\n",
      "✗  Q:    1702+1297   TRUE:   2999   PRED:  32222\n",
      "✗  Q:    6529+7415   TRUE:  13944   PRED:  14010\n",
      "\n",
      "Epoch 12/50  |  train loss=1.4496 acc=0.4371  |  val loss=1.4477 acc=0.4368\n",
      "\n",
      "================================================================================\n",
      "Samples from validation (greedy decode)\n",
      "================================================================================\n",
      "✗  Q:    3844+9270   TRUE:  13114   PRED:  13009\n",
      "✗  Q:    2565+6526   TRUE:   9091   PRED:  89955\n",
      "✗  Q:     801-3219   TRUE:  -2418   PRED:  -2709\n",
      "✗  Q:    3984+1770   TRUE:   5754   PRED:  59955\n",
      "✗  Q:    3735+2615   TRUE:   6350   PRED:  65359\n",
      "✗  Q:    5688+7759   TRUE:  13447   PRED:  13395\n",
      "✗  Q:    2025+3260   TRUE:   5285   PRED:  53955\n",
      "✗  Q:    8524+7403   TRUE:  15927   PRED:  15894\n",
      "✗  Q:     980+5556   TRUE:   6536   PRED:  65355\n",
      "✗  Q:    1911-2754   TRUE:   -843   PRED:  -8739\n",
      "✗  Q:    1702+1297   TRUE:   2999   PRED:  30094\n",
      "✗  Q:    6529+7415   TRUE:  13944   PRED:  13995\n",
      "\n",
      "Epoch 13/50  |  train loss=1.4307 acc=0.4450  |  val loss=1.4334 acc=0.4409\n",
      "\n",
      "================================================================================\n",
      "Samples from validation (greedy decode)\n",
      "================================================================================\n",
      "✗  Q:    3844+9270   TRUE:  13114   PRED:  13207\n",
      "✗  Q:    2565+6526   TRUE:   9091   PRED:  90118\n",
      "✗  Q:     801-3219   TRUE:  -2418   PRED:  -2443\n",
      "✗  Q:    3984+1770   TRUE:   5754   PRED:  55711\n",
      "✗  Q:    3735+2615   TRUE:   6350   PRED:  63118\n",
      "✗  Q:    5688+7759   TRUE:  13447   PRED:  13481\n",
      "✗  Q:    2025+3260   TRUE:   5285   PRED:  52471\n",
      "✗  Q:    8524+7403   TRUE:  15927   PRED:  15834\n",
      "✗  Q:     980+5556   TRUE:   6536   PRED:  69181\n",
      "✗  Q:    1911-2754   TRUE:   -843   PRED:  -1044\n",
      "✗  Q:    1702+1297   TRUE:   2999   PRED:  29139\n",
      "✗  Q:    6529+7415   TRUE:  13944   PRED:  14094\n",
      "\n",
      "Epoch 14/50  |  train loss=1.4309 acc=0.4414  |  val loss=1.4137 acc=0.4518\n",
      "\n",
      "================================================================================\n",
      "Samples from validation (greedy decode)\n",
      "================================================================================\n",
      "✗  Q:    3844+9270   TRUE:  13114   PRED:  13207\n",
      "✗  Q:    2565+6526   TRUE:   9091   PRED:  90118\n",
      "✗  Q:     801-3219   TRUE:  -2418   PRED:  -2490\n",
      "✗  Q:    3984+1770   TRUE:   5754   PRED:  57777\n",
      "✗  Q:    3735+2615   TRUE:   6350   PRED:  63107\n",
      "✗  Q:    5688+7759   TRUE:  13447   PRED:  13407\n",
      "✗  Q:    2025+3260   TRUE:   5285   PRED:  52090\n",
      "✗  Q:    8524+7403   TRUE:  15927   PRED:  15890\n",
      "✗  Q:     980+5556   TRUE:   6536   PRED:  69000\n",
      "✗  Q:    1911-2754   TRUE:   -843   PRED:  -8777\n",
      "✗  Q:    1702+1297   TRUE:   2999   PRED:  30077\n",
      "✗  Q:    6529+7415   TRUE:  13944   PRED:  14090\n",
      "\n",
      "Epoch 15/50  |  train loss=1.4117 acc=0.4516  |  val loss=1.4146 acc=0.4476\n",
      "\n",
      "================================================================================\n",
      "Samples from validation (greedy decode)\n",
      "================================================================================\n",
      "✗  Q:    3844+9270   TRUE:  13114   PRED:  13000\n",
      "✗  Q:    2565+6526   TRUE:   9091   PRED:  90169\n",
      "✗  Q:     801-3219   TRUE:  -2418   PRED:  -2493\n",
      "✗  Q:    3984+1770   TRUE:   5754   PRED:  58090\n",
      "✗  Q:    3735+2615   TRUE:   6350   PRED:  64900\n",
      "✗  Q:    5688+7759   TRUE:  13447   PRED:  13339\n",
      "✗  Q:    2025+3260   TRUE:   5285   PRED:  52930\n",
      "✗  Q:    8524+7403   TRUE:  15927   PRED:  15900\n",
      "✗  Q:     980+5556   TRUE:   6536   PRED:  69930\n",
      "✗  Q:    1911-2754   TRUE:   -843   PRED:  -8930\n",
      "✗  Q:    1702+1297   TRUE:   2999   PRED:  30093\n",
      "✗  Q:    6529+7415   TRUE:  13944   PRED:  13900\n",
      "\n",
      "Epoch 16/50  |  train loss=1.3968 acc=0.4584  |  val loss=1.4013 acc=0.4564\n",
      "\n",
      "================================================================================\n",
      "Samples from validation (greedy decode)\n",
      "================================================================================\n",
      "✗  Q:    3844+9270   TRUE:  13114   PRED:  13077\n",
      "✗  Q:    2565+6526   TRUE:   9091   PRED:  90167\n",
      "✗  Q:     801-3219   TRUE:  -2418   PRED:  -2439\n",
      "✗  Q:    3984+1770   TRUE:   5754   PRED:  57777\n",
      "✗  Q:    3735+2615   TRUE:   6350   PRED:  63906\n",
      "✗  Q:    5688+7759   TRUE:  13447   PRED:  13437\n",
      "✗  Q:    2025+3260   TRUE:   5285   PRED:  52900\n",
      "✗  Q:    8524+7403   TRUE:  15927   PRED:  15900\n",
      "✗  Q:     980+5556   TRUE:   6536   PRED:  66067\n",
      "✗  Q:    1911-2754   TRUE:   -843   PRED:  -8777\n",
      "✗  Q:    1702+1297   TRUE:   2999   PRED:  30900\n",
      "✗  Q:    6529+7415   TRUE:  13944   PRED:  14094\n",
      "\n",
      "Epoch 17/50  |  train loss=1.3910 acc=0.4593  |  val loss=1.4021 acc=0.4505\n",
      "\n",
      "================================================================================\n",
      "Samples from validation (greedy decode)\n",
      "================================================================================\n",
      "✗  Q:    3844+9270   TRUE:  13114   PRED:  13055\n",
      "✗  Q:    2565+6526   TRUE:   9091   PRED:  90828\n",
      "✗  Q:     801-3219   TRUE:  -2418   PRED:  -2481\n",
      "✗  Q:    3984+1770   TRUE:   5754   PRED:  58082\n",
      "✗  Q:    3735+2615   TRUE:   6350   PRED:  64088\n",
      "✗  Q:    5688+7759   TRUE:  13447   PRED:  13408\n",
      "✗  Q:    2025+3260   TRUE:   5285   PRED:  52882\n",
      "✗  Q:    8524+7403   TRUE:  15927   PRED:  15708\n",
      "✗  Q:     980+5556   TRUE:   6536   PRED:  64888\n",
      "✗  Q:    1911-2754   TRUE:   -843   PRED:  -9508\n",
      "✗  Q:    1702+1297   TRUE:   2999   PRED:  30508\n",
      "✗  Q:    6529+7415   TRUE:  13944   PRED:  14090\n",
      "\n",
      "Epoch 18/50  |  train loss=1.3845 acc=0.4620  |  val loss=1.3927 acc=0.4505\n",
      "\n",
      "================================================================================\n",
      "Samples from validation (greedy decode)\n",
      "================================================================================\n",
      "✗  Q:    3844+9270   TRUE:  13114   PRED:  13011\n",
      "✗  Q:    2565+6526   TRUE:   9091   PRED:  90118\n",
      "✗  Q:     801-3219   TRUE:  -2418   PRED:  -2487\n",
      "✗  Q:    3984+1770   TRUE:   5754   PRED:  57118\n",
      "✗  Q:    3735+2615   TRUE:   6350   PRED:  62082\n",
      "✗  Q:    5688+7759   TRUE:  13447   PRED:  13411\n",
      "✗  Q:    2025+3260   TRUE:   5285   PRED:  51180\n",
      "✗  Q:    8524+7403   TRUE:  15927   PRED:  15961\n",
      "✗  Q:     980+5556   TRUE:   6536   PRED:  68167\n",
      "✗  Q:    1911-2754   TRUE:   -843   PRED:  -8777\n",
      "✗  Q:    1702+1297   TRUE:   2999   PRED:  30118\n",
      "✗  Q:    6529+7415   TRUE:  13944   PRED:  13982\n",
      "\n",
      "Epoch 19/50  |  train loss=1.3690 acc=0.4708  |  val loss=1.3849 acc=0.4583\n",
      "\n",
      "================================================================================\n",
      "Samples from validation (greedy decode)\n",
      "================================================================================\n",
      "✗  Q:    3844+9270   TRUE:  13114   PRED:  13001\n",
      "✗  Q:    2565+6526   TRUE:   9091   PRED:  90105\n",
      "✗  Q:     801-3219   TRUE:  -2418   PRED:  -2441\n",
      "✗  Q:    3984+1770   TRUE:   5754   PRED:  57110\n",
      "✗  Q:    3735+2615   TRUE:   6350   PRED:  61965\n",
      "✗  Q:    5688+7759   TRUE:  13447   PRED:  13345\n",
      "✗  Q:    2025+3260   TRUE:   5285   PRED:  52000\n",
      "✗  Q:    8524+7403   TRUE:  15927   PRED:  15832\n",
      "✗  Q:     980+5556   TRUE:   6536   PRED:  66100\n",
      "✗  Q:    1911-2754   TRUE:   -843   PRED:  -8118\n",
      "✗  Q:    1702+1297   TRUE:   2999   PRED:  30100\n",
      "✗  Q:    6529+7415   TRUE:  13944   PRED:  13994\n",
      "\n",
      "Epoch 20/50  |  train loss=1.3644 acc=0.4714  |  val loss=1.3845 acc=0.4554\n",
      "\n",
      "================================================================================\n",
      "Samples from validation (greedy decode)\n",
      "================================================================================\n",
      "✗  Q:    3844+9270   TRUE:  13114   PRED:  13122\n",
      "✗  Q:    2565+6526   TRUE:   9091   PRED:  91100\n",
      "✓  Q:     801-3219   TRUE:  -2418   PRED:  -2418\n",
      "✗  Q:    3984+1770   TRUE:   5754   PRED:  56169\n",
      "✗  Q:    3735+2615   TRUE:   6350   PRED:  63188\n",
      "✗  Q:    5688+7759   TRUE:  13447   PRED:  13431\n",
      "✗  Q:    2025+3260   TRUE:   5285   PRED:  52948\n",
      "✗  Q:    8524+7403   TRUE:  15927   PRED:  15934\n",
      "✗  Q:     980+5556   TRUE:   6536   PRED:  66948\n",
      "✗  Q:    1911-2754   TRUE:   -843   PRED:  -8794\n",
      "✗  Q:    1702+1297   TRUE:   2999   PRED:  30944\n",
      "✗  Q:    6529+7415   TRUE:  13944   PRED:  14094\n",
      "\n",
      "Epoch 21/50  |  train loss=1.3523 acc=0.4781  |  val loss=1.3861 acc=0.4568\n",
      "\n",
      "================================================================================\n",
      "Samples from validation (greedy decode)\n",
      "================================================================================\n",
      "✗  Q:    3844+9270   TRUE:  13114   PRED:  13094\n",
      "✗  Q:    2565+6526   TRUE:   9091   PRED:  90944\n",
      "✗  Q:     801-3219   TRUE:  -2418   PRED:  -2498\n",
      "✗  Q:    3984+1770   TRUE:   5754   PRED:  58444\n",
      "✗  Q:    3735+2615   TRUE:   6350   PRED:  64084\n",
      "✗  Q:    5688+7759   TRUE:  13447   PRED:  13404\n",
      "✗  Q:    2025+3260   TRUE:   5285   PRED:  54084\n",
      "✗  Q:    8524+7403   TRUE:  15927   PRED:  15944\n",
      "✗  Q:     980+5556   TRUE:   6536   PRED:  66655\n",
      "✗  Q:    1911-2754   TRUE:   -843   PRED:  -8444\n",
      "✗  Q:    1702+1297   TRUE:   2999   PRED:  30965\n",
      "✗  Q:    6529+7415   TRUE:  13944   PRED:  13865\n",
      "\n",
      "Epoch 22/50  |  train loss=1.3560 acc=0.4729  |  val loss=1.3663 acc=0.4673\n",
      "\n",
      "================================================================================\n",
      "Samples from validation (greedy decode)\n",
      "================================================================================\n",
      "✗  Q:    3844+9270   TRUE:  13114   PRED:  13055\n",
      "✗  Q:    2565+6526   TRUE:   9091   PRED:  90167\n",
      "✓  Q:     801-3219   TRUE:  -2418   PRED:  -2418\n",
      "✗  Q:    3984+1770   TRUE:   5754   PRED:  57791\n",
      "✗  Q:    3735+2615   TRUE:   6350   PRED:  63057\n",
      "✗  Q:    5688+7759   TRUE:  13447   PRED:  13405\n",
      "✗  Q:    2025+3260   TRUE:   5285   PRED:  52770\n",
      "✗  Q:    8524+7403   TRUE:  15927   PRED:  15777\n",
      "✗  Q:     980+5556   TRUE:   6536   PRED:  65777\n",
      "✗  Q:    1911-2754   TRUE:   -843   PRED:  -9500\n",
      "✗  Q:    1702+1297   TRUE:   2999   PRED:  31391\n",
      "✗  Q:    6529+7415   TRUE:  13944   PRED:  13994\n",
      "\n",
      "Epoch 23/50  |  train loss=1.3483 acc=0.4776  |  val loss=1.3757 acc=0.4569\n",
      "\n",
      "================================================================================\n",
      "Samples from validation (greedy decode)\n",
      "================================================================================\n",
      "✗  Q:    3844+9270   TRUE:  13114   PRED:  13094\n",
      "✗  Q:    2565+6526   TRUE:   9091   PRED:  90149\n",
      "✗  Q:     801-3219   TRUE:  -2418   PRED:  -2594\n",
      "✗  Q:    3984+1770   TRUE:   5754   PRED:  55717\n",
      "✗  Q:    3735+2615   TRUE:   6350   PRED:  63044\n",
      "✗  Q:    5688+7759   TRUE:  13447   PRED:  13404\n",
      "✗  Q:    2025+3260   TRUE:   5285   PRED:  52094\n",
      "✗  Q:    8524+7403   TRUE:  15927   PRED:  15830\n",
      "✗  Q:     980+5556   TRUE:   6536   PRED:  64994\n",
      "✗  Q:    1911-2754   TRUE:   -843   PRED:  -8439\n",
      "✗  Q:    1702+1297   TRUE:   2999   PRED:  30399\n",
      "✗  Q:    6529+7415   TRUE:  13944   PRED:  13994\n",
      "\n",
      "Epoch 24/50  |  train loss=1.3368 acc=0.4834  |  val loss=1.3622 acc=0.4639\n",
      "\n",
      "================================================================================\n",
      "Samples from validation (greedy decode)\n",
      "================================================================================\n",
      "✗  Q:    3844+9270   TRUE:  13114   PRED:  13209\n",
      "✗  Q:    2565+6526   TRUE:   9091   PRED:  92879\n",
      "✗  Q:     801-3219   TRUE:  -2418   PRED:  -2499\n",
      "✗  Q:    3984+1770   TRUE:   5754   PRED:  57799\n",
      "✗  Q:    3735+2615   TRUE:   6350   PRED:  64047\n",
      "✗  Q:    5688+7759   TRUE:  13447   PRED:  13500\n",
      "✗  Q:    2025+3260   TRUE:   5285   PRED:  53099\n",
      "✗  Q:    8524+7403   TRUE:  15927   PRED:  15965\n",
      "✗  Q:     980+5556   TRUE:   6536   PRED:  66999\n",
      "✗  Q:    1911-2754   TRUE:   -843   PRED:  -8240\n",
      "✗  Q:    1702+1297   TRUE:   2999   PRED:  29650\n",
      "✗  Q:    6529+7415   TRUE:  13944   PRED:  14090\n",
      "\n",
      "Epoch 25/50  |  train loss=1.3303 acc=0.4860  |  val loss=1.3377 acc=0.4814\n",
      "\n",
      "================================================================================\n",
      "Samples from validation (greedy decode)\n",
      "================================================================================\n",
      "✗  Q:    3844+9270   TRUE:  13114   PRED:  13094\n",
      "✗  Q:    2565+6526   TRUE:   9091   PRED:  90945\n",
      "✗  Q:     801-3219   TRUE:  -2418   PRED:  -2442\n",
      "✗  Q:    3984+1770   TRUE:   5754   PRED:  56605\n",
      "✗  Q:    3735+2615   TRUE:   6350   PRED:  64265\n",
      "✗  Q:    5688+7759   TRUE:  13447   PRED:  13498\n",
      "✗  Q:    2025+3260   TRUE:   5285   PRED:  52955\n",
      "✗  Q:    8524+7403   TRUE:  15927   PRED:  15932\n",
      "✗  Q:     980+5556   TRUE:   6536   PRED:  66695\n",
      "✗  Q:    1911-2754   TRUE:   -843   PRED:  -7452\n",
      "✗  Q:    1702+1297   TRUE:   2999   PRED:  30565\n",
      "✗  Q:    6529+7415   TRUE:  13944   PRED:  13994\n",
      "\n",
      "Epoch 26/50  |  train loss=1.3270 acc=0.4869  |  val loss=1.3476 acc=0.4740\n",
      "\n",
      "================================================================================\n",
      "Samples from validation (greedy decode)\n",
      "================================================================================\n",
      "✗  Q:    3844+9270   TRUE:  13114   PRED:  13191\n",
      "✗  Q:    2565+6526   TRUE:   9091   PRED:  90177\n",
      "✓  Q:     801-3219   TRUE:  -2418   PRED:  -2418\n",
      "✗  Q:    3984+1770   TRUE:   5754   PRED:  57177\n",
      "✗  Q:    3735+2615   TRUE:   6350   PRED:  63917\n",
      "✗  Q:    5688+7759   TRUE:  13447   PRED:  13477\n",
      "✗  Q:    2025+3260   TRUE:   5285   PRED:  52770\n",
      "✗  Q:    8524+7403   TRUE:  15927   PRED:  15877\n",
      "✗  Q:     980+5556   TRUE:   6536   PRED:  66357\n",
      "✗  Q:    1911-2754   TRUE:   -843   PRED:  -9777\n",
      "✗  Q:    1702+1297   TRUE:   2999   PRED:  30277\n",
      "✗  Q:    6529+7415   TRUE:  13944   PRED:  14017\n",
      "\n",
      "Epoch 27/50  |  train loss=1.3234 acc=0.4878  |  val loss=1.3420 acc=0.4775\n",
      "\n",
      "================================================================================\n",
      "Samples from validation (greedy decode)\n",
      "================================================================================\n",
      "✗  Q:    3844+9270   TRUE:  13114   PRED:  13091\n",
      "✗  Q:    2565+6526   TRUE:   9091   PRED:  91110\n",
      "✗  Q:     801-3219   TRUE:  -2418   PRED:  -2441\n",
      "✗  Q:    3984+1770   TRUE:   5754   PRED:  57745\n",
      "✗  Q:    3735+2615   TRUE:   6350   PRED:  64918\n",
      "✗  Q:    5688+7759   TRUE:  13447   PRED:  13477\n",
      "✗  Q:    2025+3260   TRUE:   5285   PRED:  52000\n",
      "✗  Q:    8524+7403   TRUE:  15927   PRED:  15899\n",
      "✗  Q:     980+5556   TRUE:   6536   PRED:  66770\n",
      "✗  Q:    1911-2754   TRUE:   -843   PRED:  -8745\n",
      "✗  Q:    1702+1297   TRUE:   2999   PRED:  29167\n",
      "✗  Q:    6529+7415   TRUE:  13944   PRED:  13994\n",
      "\n",
      "Epoch 28/50  |  train loss=1.3168 acc=0.4908  |  val loss=1.3276 acc=0.4844\n",
      "\n",
      "================================================================================\n",
      "Samples from validation (greedy decode)\n",
      "================================================================================\n",
      "✗  Q:    3844+9270   TRUE:  13114   PRED:  13194\n",
      "✗  Q:    2565+6526   TRUE:   9091   PRED:  90941\n",
      "✗  Q:     801-3219   TRUE:  -2418   PRED:  -2441\n",
      "✗  Q:    3984+1770   TRUE:   5754   PRED:  57799\n",
      "✗  Q:    3735+2615   TRUE:   6350   PRED:  63999\n",
      "✗  Q:    5688+7759   TRUE:  13447   PRED:  13497\n",
      "✗  Q:    2025+3260   TRUE:   5285   PRED:  52000\n",
      "✗  Q:    8524+7403   TRUE:  15927   PRED:  15930\n",
      "✗  Q:     980+5556   TRUE:   6536   PRED:  66777\n",
      "✗  Q:    1911-2754   TRUE:   -843   PRED:  -9307\n",
      "✗  Q:    1702+1297   TRUE:   2999   PRED:  30999\n",
      "✗  Q:    6529+7415   TRUE:  13944   PRED:  14000\n",
      "\n",
      "Epoch 29/50  |  train loss=1.3120 acc=0.4929  |  val loss=1.3519 acc=0.4666\n",
      "\n",
      "================================================================================\n",
      "Samples from validation (greedy decode)\n",
      "================================================================================\n",
      "✗  Q:    3844+9270   TRUE:  13114   PRED:  12987\n",
      "✗  Q:    2565+6526   TRUE:   9091   PRED:  91100\n",
      "✗  Q:     801-3219   TRUE:  -2418   PRED:  -2445\n",
      "✗  Q:    3984+1770   TRUE:   5754   PRED:  57748\n",
      "✗  Q:    3735+2615   TRUE:   6350   PRED:  64377\n",
      "✗  Q:    5688+7759   TRUE:  13447   PRED:  13477\n",
      "✗  Q:    2025+3260   TRUE:   5285   PRED:  53774\n",
      "✗  Q:    8524+7403   TRUE:  15927   PRED:  15837\n",
      "✗  Q:     980+5556   TRUE:   6536   PRED:  66770\n",
      "✗  Q:    1911-2754   TRUE:   -843   PRED:  -7745\n",
      "✗  Q:    1702+1297   TRUE:   2999   PRED:  31167\n",
      "✗  Q:    6529+7415   TRUE:  13944   PRED:  13994\n",
      "\n",
      "Epoch 30/50  |  train loss=1.3047 acc=0.4960  |  val loss=1.3326 acc=0.4802\n",
      "\n",
      "================================================================================\n",
      "Samples from validation (greedy decode)\n",
      "================================================================================\n",
      "✗  Q:    3844+9270   TRUE:  13114   PRED:  13055\n",
      "✗  Q:    2565+6526   TRUE:   9091   PRED:  90824\n",
      "✗  Q:     801-3219   TRUE:  -2418   PRED:  -2682\n",
      "✗  Q:    3984+1770   TRUE:   5754   PRED:  58026\n",
      "✗  Q:    3735+2615   TRUE:   6350   PRED:  63988\n",
      "✗  Q:    5688+7759   TRUE:  13447   PRED:  13406\n",
      "✗  Q:    2025+3260   TRUE:   5285   PRED:  52302\n",
      "✗  Q:    8524+7403   TRUE:  15927   PRED:  15882\n",
      "✗  Q:     980+5556   TRUE:   6536   PRED:  66686\n",
      "✗  Q:    1911-2754   TRUE:   -843   PRED:  -8828\n",
      "✗  Q:    1702+1297   TRUE:   2999   PRED:  30268\n",
      "✗  Q:    6529+7415   TRUE:  13944   PRED:  13855\n",
      "\n",
      "Epoch 31/50  |  train loss=1.3007 acc=0.4985  |  val loss=1.3269 acc=0.4823\n",
      "\n",
      "================================================================================\n",
      "Samples from validation (greedy decode)\n",
      "================================================================================\n",
      "✗  Q:    3844+9270   TRUE:  13114   PRED:  13200\n",
      "✗  Q:    2565+6526   TRUE:   9091   PRED:  91052\n",
      "✗  Q:     801-3219   TRUE:  -2418   PRED:  -2490\n",
      "✗  Q:    3984+1770   TRUE:   5754   PRED:  56799\n",
      "✗  Q:    3735+2615   TRUE:   6350   PRED:  63049\n",
      "✗  Q:    5688+7759   TRUE:  13447   PRED:  13339\n",
      "✗  Q:    2025+3260   TRUE:   5285   PRED:  52000\n",
      "✗  Q:    8524+7403   TRUE:  15927   PRED:  15900\n",
      "✗  Q:     980+5556   TRUE:   6536   PRED:  64995\n",
      "✗  Q:    1911-2754   TRUE:   -843   PRED:  -7100\n",
      "✗  Q:    1702+1297   TRUE:   2999   PRED:  31990\n",
      "✗  Q:    6529+7415   TRUE:  13944   PRED:  13850\n",
      "\n",
      "Epoch 32/50  |  train loss=1.2984 acc=0.4989  |  val loss=1.3234 acc=0.4821\n",
      "\n",
      "================================================================================\n",
      "Samples from validation (greedy decode)\n",
      "================================================================================\n",
      "✗  Q:    3844+9270   TRUE:  13114   PRED:  13118\n",
      "✗  Q:    2565+6526   TRUE:   9091   PRED:  91839\n",
      "✗  Q:     801-3219   TRUE:  -2418   PRED:  -2593\n",
      "✗  Q:    3984+1770   TRUE:   5754   PRED:  57799\n",
      "✗  Q:    3735+2615   TRUE:   6350   PRED:  63999\n",
      "✗  Q:    5688+7759   TRUE:  13447   PRED:  13419\n",
      "✗  Q:    2025+3260   TRUE:   5285   PRED:  53757\n",
      "✗  Q:    8524+7403   TRUE:  15927   PRED:  15937\n",
      "✗  Q:     980+5556   TRUE:   6536   PRED:  64565\n",
      "✗  Q:    1911-2754   TRUE:   -843   PRED:  -8999\n",
      "✗  Q:    1702+1297   TRUE:   2999   PRED:  30999\n",
      "✗  Q:    6529+7415   TRUE:  13944   PRED:  13909\n",
      "\n",
      "Epoch 33/50  |  train loss=1.2921 acc=0.5015  |  val loss=1.3341 acc=0.4766\n",
      "\n",
      "================================================================================\n",
      "Samples from validation (greedy decode)\n",
      "================================================================================\n",
      "✗  Q:    3844+9270   TRUE:  13114   PRED:  13118\n",
      "✗  Q:    2565+6526   TRUE:   9091   PRED:  91830\n",
      "✗  Q:     801-3219   TRUE:  -2418   PRED:  -2455\n",
      "✗  Q:    3984+1770   TRUE:   5754   PRED:  57855\n",
      "✗  Q:    3735+2615   TRUE:   6350   PRED:  63995\n",
      "✗  Q:    5688+7759   TRUE:  13447   PRED:  13431\n",
      "✗  Q:    2025+3260   TRUE:   5285   PRED:  53535\n",
      "✗  Q:    8524+7403   TRUE:  15927   PRED:  15935\n",
      "✗  Q:     980+5556   TRUE:   6536   PRED:  66635\n",
      "✗  Q:    1911-2754   TRUE:   -843   PRED:  -8934\n",
      "✗  Q:    1702+1297   TRUE:   2999   PRED:  30398\n",
      "✗  Q:    6529+7415   TRUE:  13944   PRED:  13965\n",
      "\n",
      "Epoch 34/50  |  train loss=1.2865 acc=0.5039  |  val loss=1.3126 acc=0.4867\n",
      "\n",
      "================================================================================\n",
      "Samples from validation (greedy decode)\n",
      "================================================================================\n",
      "✗  Q:    3844+9270   TRUE:  13114   PRED:  13118\n",
      "✗  Q:    2565+6526   TRUE:   9091   PRED:  91804\n",
      "✗  Q:     801-3219   TRUE:  -2418   PRED:  -2345\n",
      "✗  Q:    3984+1770   TRUE:   5754   PRED:  57900\n",
      "✗  Q:    3735+2615   TRUE:   6350   PRED:  64377\n",
      "✗  Q:    5688+7759   TRUE:  13447   PRED:  13476\n",
      "✗  Q:    2025+3260   TRUE:   5285   PRED:  53616\n",
      "✗  Q:    8524+7403   TRUE:  15927   PRED:  15894\n",
      "✗  Q:     980+5556   TRUE:   6536   PRED:  64664\n",
      "✗  Q:    1911-2754   TRUE:   -843   PRED:  -8944\n",
      "✗  Q:    1702+1297   TRUE:   2999   PRED:  30266\n",
      "✗  Q:    6529+7415   TRUE:  13944   PRED:  13917\n",
      "\n",
      "Epoch 35/50  |  train loss=1.2914 acc=0.5019  |  val loss=1.3133 acc=0.4883\n",
      "\n",
      "================================================================================\n",
      "Samples from validation (greedy decode)\n",
      "================================================================================\n",
      "✗  Q:    3844+9270   TRUE:  13114   PRED:  13099\n",
      "✗  Q:    2565+6526   TRUE:   9091   PRED:  90180\n",
      "✗  Q:     801-3219   TRUE:  -2418   PRED:  -2610\n",
      "✗  Q:    3984+1770   TRUE:   5754   PRED:  57770\n",
      "✗  Q:    3735+2615   TRUE:   6350   PRED:  63071\n",
      "✗  Q:    5688+7759   TRUE:  13447   PRED:  13499\n",
      "✗  Q:    2025+3260   TRUE:   5285   PRED:  52000\n",
      "✗  Q:    8524+7403   TRUE:  15927   PRED:  15967\n",
      "✗  Q:     980+5556   TRUE:   6536   PRED:  66777\n",
      "✗  Q:    1911-2754   TRUE:   -843   PRED:  -8998\n",
      "✗  Q:    1702+1297   TRUE:   2999   PRED:  30910\n",
      "✗  Q:    6529+7415   TRUE:  13944   PRED:  14011\n",
      "\n",
      "Epoch 36/50  |  train loss=1.2815 acc=0.5070  |  val loss=1.3028 acc=0.4934\n",
      "\n",
      "================================================================================\n",
      "Samples from validation (greedy decode)\n",
      "================================================================================\n",
      "✗  Q:    3844+9270   TRUE:  13114   PRED:  13096\n",
      "✗  Q:    2565+6526   TRUE:   9091   PRED:  91830\n",
      "✗  Q:     801-3219   TRUE:  -2418   PRED:  -2493\n",
      "✗  Q:    3984+1770   TRUE:   5754   PRED:  56260\n",
      "✗  Q:    3735+2615   TRUE:   6350   PRED:  63307\n",
      "✗  Q:    5688+7759   TRUE:  13447   PRED:  13506\n",
      "✗  Q:    2025+3260   TRUE:   5285   PRED:  52900\n",
      "✗  Q:    8524+7403   TRUE:  15927   PRED:  15966\n",
      "✗  Q:     980+5556   TRUE:   6536   PRED:  66635\n",
      "✗  Q:    1911-2754   TRUE:   -843   PRED:  -8339\n",
      "✗  Q:    1702+1297   TRUE:   2999   PRED:  30360\n",
      "✗  Q:    6529+7415   TRUE:  13944   PRED:  14094\n",
      "\n",
      "Epoch 37/50  |  train loss=1.2771 acc=0.5080  |  val loss=1.3034 acc=0.4906\n",
      "\n",
      "================================================================================\n",
      "Samples from validation (greedy decode)\n",
      "================================================================================\n",
      "✗  Q:    3844+9270   TRUE:  13114   PRED:  13022\n",
      "✗  Q:    2565+6526   TRUE:   9091   PRED:  91102\n",
      "✗  Q:     801-3219   TRUE:  -2418   PRED:  -2452\n",
      "✗  Q:    3984+1770   TRUE:   5754   PRED:  56237\n",
      "✗  Q:    3735+2615   TRUE:   6350   PRED:  63486\n",
      "✗  Q:    5688+7759   TRUE:  13447   PRED:  13492\n",
      "✗  Q:    2025+3260   TRUE:   5285   PRED:  52592\n",
      "✗  Q:    8524+7403   TRUE:  15927   PRED:  15894\n",
      "✗  Q:     980+5556   TRUE:   6536   PRED:  66169\n",
      "✗  Q:    1911-2754   TRUE:   -843   PRED:  -8934\n",
      "✗  Q:    1702+1297   TRUE:   2999   PRED:  29552\n",
      "✗  Q:    6529+7415   TRUE:  13944   PRED:  14036\n",
      "\n",
      "Epoch 38/50  |  train loss=1.2734 acc=0.5100  |  val loss=1.3005 acc=0.4947\n",
      "\n",
      "================================================================================\n",
      "Samples from validation (greedy decode)\n",
      "================================================================================\n",
      "✗  Q:    3844+9270   TRUE:  13114   PRED:  13096\n",
      "✗  Q:    2565+6526   TRUE:   9091   PRED:  91082\n",
      "✗  Q:     801-3219   TRUE:  -2418   PRED:  -2652\n",
      "✗  Q:    3984+1770   TRUE:   5754   PRED:  57720\n",
      "✗  Q:    3735+2615   TRUE:   6350   PRED:  63770\n",
      "✗  Q:    5688+7759   TRUE:  13447   PRED:  13476\n",
      "✗  Q:    2025+3260   TRUE:   5285   PRED:  52000\n",
      "✗  Q:    8524+7403   TRUE:  15927   PRED:  15966\n",
      "✗  Q:     980+5556   TRUE:   6536   PRED:  66635\n",
      "✗  Q:    1911-2754   TRUE:   -843   PRED:  -8230\n",
      "✗  Q:    1702+1297   TRUE:   2999   PRED:  30377\n",
      "✗  Q:    6529+7415   TRUE:  13944   PRED:  14036\n",
      "\n",
      "Epoch 39/50  |  train loss=1.2783 acc=0.5066  |  val loss=1.3347 acc=0.4745\n",
      "\n",
      "================================================================================\n",
      "Samples from validation (greedy decode)\n",
      "================================================================================\n",
      "✗  Q:    3844+9270   TRUE:  13114   PRED:  13001\n",
      "✗  Q:    2565+6526   TRUE:   9091   PRED:  90048\n",
      "✗  Q:     801-3219   TRUE:  -2418   PRED:  -2739\n",
      "✗  Q:    3984+1770   TRUE:   5754   PRED:  57200\n",
      "✗  Q:    3735+2615   TRUE:   6350   PRED:  63398\n",
      "✗  Q:    5688+7759   TRUE:  13447   PRED:  13439\n",
      "✗  Q:    2025+3260   TRUE:   5285   PRED:  52000\n",
      "✗  Q:    8524+7403   TRUE:  15927   PRED:  15966\n",
      "✗  Q:     980+5556   TRUE:   6536   PRED:  66639\n",
      "✗  Q:    1911-2754   TRUE:   -843   PRED:  -1048\n",
      "✗  Q:    1702+1297   TRUE:   2999   PRED:  28600\n",
      "✗  Q:    6529+7415   TRUE:  13944   PRED:  13989\n",
      "\n",
      "Epoch 40/50  |  train loss=1.2678 acc=0.5115  |  val loss=1.3164 acc=0.4803\n",
      "\n",
      "================================================================================\n",
      "Samples from validation (greedy decode)\n",
      "================================================================================\n",
      "✗  Q:    3844+9270   TRUE:  13114   PRED:  13209\n",
      "✗  Q:    2565+6526   TRUE:   9091   PRED:  91100\n",
      "✗  Q:     801-3219   TRUE:  -2418   PRED:  -2451\n",
      "✗  Q:    3984+1770   TRUE:   5754   PRED:  57624\n",
      "✗  Q:    3735+2615   TRUE:   6350   PRED:  64304\n",
      "✗  Q:    5688+7759   TRUE:  13447   PRED:  13498\n",
      "✗  Q:    2025+3260   TRUE:   5285   PRED:  52909\n",
      "✗  Q:    8524+7403   TRUE:  15927   PRED:  15966\n",
      "✗  Q:     980+5556   TRUE:   6536   PRED:  66696\n",
      "✗  Q:    1911-2754   TRUE:   -843   PRED:  -9309\n",
      "✗  Q:    1702+1297   TRUE:   2999   PRED:  29169\n",
      "✗  Q:    6529+7415   TRUE:  13944   PRED:  14006\n",
      "\n",
      "Epoch 41/50  |  train loss=1.2607 acc=0.5155  |  val loss=1.2989 acc=0.4943\n",
      "\n",
      "================================================================================\n",
      "Samples from validation (greedy decode)\n",
      "================================================================================\n",
      "✗  Q:    3844+9270   TRUE:  13114   PRED:  13098\n",
      "✗  Q:    2565+6526   TRUE:   9091   PRED:  90044\n",
      "✗  Q:     801-3219   TRUE:  -2418   PRED:  -2490\n",
      "✗  Q:    3984+1770   TRUE:   5754   PRED:  57200\n",
      "✗  Q:    3735+2615   TRUE:   6350   PRED:  64047\n",
      "✗  Q:    5688+7759   TRUE:  13447   PRED:  13476\n",
      "✗  Q:    2025+3260   TRUE:   5285   PRED:  52900\n",
      "✗  Q:    8524+7403   TRUE:  15927   PRED:  15967\n",
      "✗  Q:     980+5556   TRUE:   6536   PRED:  66044\n",
      "✗  Q:    1911-2754   TRUE:   -843   PRED:  -9700\n",
      "✗  Q:    1702+1297   TRUE:   2999   PRED:  29189\n",
      "✗  Q:    6529+7415   TRUE:  13944   PRED:  13989\n",
      "\n",
      "Epoch 42/50  |  train loss=1.2595 acc=0.5160  |  val loss=1.2747 acc=0.5084\n",
      "\n",
      "================================================================================\n",
      "Samples from validation (greedy decode)\n",
      "================================================================================\n",
      "✗  Q:    3844+9270   TRUE:  13114   PRED:  13098\n",
      "✗  Q:    2565+6526   TRUE:   9091   PRED:  90988\n",
      "✗  Q:     801-3219   TRUE:  -2418   PRED:  -2451\n",
      "✗  Q:    3984+1770   TRUE:   5754   PRED:  58087\n",
      "✗  Q:    3735+2615   TRUE:   6350   PRED:  63775\n",
      "✗  Q:    5688+7759   TRUE:  13447   PRED:  13476\n",
      "✗  Q:    2025+3260   TRUE:   5285   PRED:  52860\n",
      "✗  Q:    8524+7403   TRUE:  15927   PRED:  15966\n",
      "✗  Q:     980+5556   TRUE:   6536   PRED:  65655\n",
      "✗  Q:    1911-2754   TRUE:   -843   PRED:  -8235\n",
      "✗  Q:    1702+1297   TRUE:   2999   PRED:  29555\n",
      "✗  Q:    6529+7415   TRUE:  13944   PRED:  13987\n",
      "\n",
      "Epoch 43/50  |  train loss=1.2627 acc=0.5138  |  val loss=1.2918 acc=0.4969\n",
      "\n",
      "================================================================================\n",
      "Samples from validation (greedy decode)\n",
      "================================================================================\n",
      "✗  Q:    3844+9270   TRUE:  13114   PRED:  13240\n",
      "✗  Q:    2565+6526   TRUE:   9091   PRED:  91104\n",
      "✗  Q:     801-3219   TRUE:  -2418   PRED:  -2494\n",
      "✗  Q:    3984+1770   TRUE:   5754   PRED:  56266\n",
      "✗  Q:    3735+2615   TRUE:   6350   PRED:  63406\n",
      "✗  Q:    5688+7759   TRUE:  13447   PRED:  13439\n",
      "✗  Q:    2025+3260   TRUE:   5285   PRED:  50464\n",
      "✗  Q:    8524+7403   TRUE:  15927   PRED:  16037\n",
      "✗  Q:     980+5556   TRUE:   6536   PRED:  66666\n",
      "✗  Q:    1911-2754   TRUE:   -843   PRED:  -8122\n",
      "✗  Q:    1702+1297   TRUE:   2999   PRED:  29555\n",
      "✗  Q:    6529+7415   TRUE:  13944   PRED:  13954\n",
      "\n",
      "Epoch 44/50  |  train loss=1.2544 acc=0.5177  |  val loss=1.2875 acc=0.5003\n",
      "\n",
      "================================================================================\n",
      "Samples from validation (greedy decode)\n",
      "================================================================================\n",
      "✗  Q:    3844+9270   TRUE:  13114   PRED:  13096\n",
      "✗  Q:    2565+6526   TRUE:   9091   PRED:  91130\n",
      "✗  Q:     801-3219   TRUE:  -2418   PRED:  -2439\n",
      "✗  Q:    3984+1770   TRUE:   5754   PRED:  57228\n",
      "✗  Q:    3735+2615   TRUE:   6350   PRED:  63338\n",
      "✗  Q:    5688+7759   TRUE:  13447   PRED:  13383\n",
      "✗  Q:    2025+3260   TRUE:   5285   PRED:  52305\n",
      "✗  Q:    8524+7403   TRUE:  15927   PRED:  15933\n",
      "✗  Q:     980+5556   TRUE:   6536   PRED:  66635\n",
      "✗  Q:    1911-2754   TRUE:   -843   PRED:  -8338\n",
      "✗  Q:    1702+1297   TRUE:   2999   PRED:  30398\n",
      "✗  Q:    6529+7415   TRUE:  13944   PRED:  13983\n",
      "\n",
      "Epoch 45/50  |  train loss=1.2469 acc=0.5214  |  val loss=1.3088 acc=0.4831\n",
      "\n",
      "================================================================================\n",
      "Samples from validation (greedy decode)\n",
      "================================================================================\n",
      "✗  Q:    3844+9270   TRUE:  13114   PRED:  13073\n",
      "✗  Q:    2565+6526   TRUE:   9091   PRED:  91100\n",
      "✗  Q:     801-3219   TRUE:  -2418   PRED:  -2451\n",
      "✗  Q:    3984+1770   TRUE:   5754   PRED:  57572\n",
      "✗  Q:    3735+2615   TRUE:   6350   PRED:  63375\n",
      "✗  Q:    5688+7759   TRUE:  13447   PRED:  13400\n",
      "✗  Q:    2025+3260   TRUE:   5285   PRED:  52750\n",
      "✗  Q:    8524+7403   TRUE:  15927   PRED:  15873\n",
      "✗  Q:     980+5556   TRUE:   6536   PRED:  66673\n",
      "✗  Q:    1911-2754   TRUE:   -843   PRED:  -7795\n",
      "✗  Q:    1702+1297   TRUE:   2999   PRED:  30375\n",
      "✗  Q:    6529+7415   TRUE:  13944   PRED:  13850\n",
      "\n",
      "Epoch 46/50  |  train loss=1.2503 acc=0.5187  |  val loss=1.2850 acc=0.4962\n",
      "\n",
      "================================================================================\n",
      "Samples from validation (greedy decode)\n",
      "================================================================================\n",
      "✗  Q:    3844+9270   TRUE:  13114   PRED:  13091\n",
      "✗  Q:    2565+6526   TRUE:   9091   PRED:  90169\n",
      "✗  Q:     801-3219   TRUE:  -2418   PRED:  -2451\n",
      "✗  Q:    3984+1770   TRUE:   5754   PRED:  55901\n",
      "✗  Q:    3735+2615   TRUE:   6350   PRED:  63046\n",
      "✗  Q:    5688+7759   TRUE:  13447   PRED:  13476\n",
      "✗  Q:    2025+3260   TRUE:   5285   PRED:  52060\n",
      "✗  Q:    8524+7403   TRUE:  15927   PRED:  15901\n",
      "✗  Q:     980+5556   TRUE:   6536   PRED:  66666\n",
      "✗  Q:    1911-2754   TRUE:   -843   PRED:  -7596\n",
      "✗  Q:    1702+1297   TRUE:   2999   PRED:  30398\n",
      "✗  Q:    6529+7415   TRUE:  13944   PRED:  13984\n",
      "\n",
      "Epoch 47/50  |  train loss=1.2419 acc=0.5238  |  val loss=1.2826 acc=0.5018\n",
      "\n",
      "================================================================================\n",
      "Samples from validation (greedy decode)\n",
      "================================================================================\n",
      "✗  Q:    3844+9270   TRUE:  13114   PRED:  13096\n",
      "✗  Q:    2565+6526   TRUE:   9091   PRED:  91105\n",
      "✗  Q:     801-3219   TRUE:  -2418   PRED:  -2559\n",
      "✗  Q:    3984+1770   TRUE:   5754   PRED:  57209\n",
      "✗  Q:    3735+2615   TRUE:   6350   PRED:  63777\n",
      "✗  Q:    5688+7759   TRUE:  13447   PRED:  13476\n",
      "✗  Q:    2025+3260   TRUE:   5285   PRED:  52416\n",
      "✗  Q:    8524+7403   TRUE:  15927   PRED:  15877\n",
      "✗  Q:     980+5556   TRUE:   6536   PRED:  66169\n",
      "✗  Q:    1911-2754   TRUE:   -843   PRED:  -8242\n",
      "✗  Q:    1702+1297   TRUE:   2999   PRED:  29189\n",
      "✗  Q:    6529+7415   TRUE:  13944   PRED:  13969\n",
      "\n",
      "Epoch 48/50  |  train loss=1.2455 acc=0.5219  |  val loss=1.3041 acc=0.4920\n",
      "\n",
      "================================================================================\n",
      "Samples from validation (greedy decode)\n",
      "================================================================================\n",
      "✗  Q:    3844+9270   TRUE:  13114   PRED:  13096\n",
      "✗  Q:    2565+6526   TRUE:   9091   PRED:  90655\n",
      "✗  Q:     801-3219   TRUE:  -2418   PRED:  -2453\n",
      "✗  Q:    3984+1770   TRUE:   5754   PRED:  57850\n",
      "✗  Q:    3735+2615   TRUE:   6350   PRED:  63775\n",
      "✗  Q:    5688+7759   TRUE:  13447   PRED:  13499\n",
      "✗  Q:    2025+3260   TRUE:   5285   PRED:  52753\n",
      "✗  Q:    8524+7403   TRUE:  15927   PRED:  15901\n",
      "✗  Q:     980+5556   TRUE:   6536   PRED:  64995\n",
      "✗  Q:    1911-2754   TRUE:   -843   PRED:  -8337\n",
      "✗  Q:    1702+1297   TRUE:   2999   PRED:  30377\n",
      "✗  Q:    6529+7415   TRUE:  13944   PRED:  14093\n",
      "\n",
      "Epoch 49/50  |  train loss=1.2396 acc=0.5240  |  val loss=1.3154 acc=0.4870\n",
      "\n",
      "================================================================================\n",
      "Samples from validation (greedy decode)\n",
      "================================================================================\n",
      "✗  Q:    3844+9270   TRUE:  13114   PRED:  13096\n",
      "✗  Q:    2565+6526   TRUE:   9091   PRED:  91144\n",
      "✗  Q:     801-3219   TRUE:  -2418   PRED:  -2551\n",
      "✗  Q:    3984+1770   TRUE:   5754   PRED:  57216\n",
      "✗  Q:    3735+2615   TRUE:   6350   PRED:  63994\n",
      "✗  Q:    5688+7759   TRUE:  13447   PRED:  13499\n",
      "✗  Q:    2025+3260   TRUE:   5285   PRED:  52022\n",
      "✗  Q:    8524+7403   TRUE:  15927   PRED:  15802\n",
      "✗  Q:     980+5556   TRUE:   6536   PRED:  64864\n",
      "✗  Q:    1911-2754   TRUE:   -843   PRED:  -8167\n",
      "✗  Q:    1702+1297   TRUE:   2999   PRED:  30360\n",
      "✗  Q:    6529+7415   TRUE:  13944   PRED:  13984\n",
      "\n",
      "Epoch 50/50  |  train loss=1.2388 acc=0.5258  |  val loss=1.2732 acc=0.5077\n",
      "\n",
      "================================================================================\n",
      "Samples from validation (greedy decode)\n",
      "================================================================================\n",
      "✗  Q:    3844+9270   TRUE:  13114   PRED:  13091\n",
      "✗  Q:    2565+6526   TRUE:   9091   PRED:  91100\n",
      "✗  Q:     801-3219   TRUE:  -2418   PRED:  -2442\n",
      "✗  Q:    3984+1770   TRUE:   5754   PRED:  57799\n",
      "✗  Q:    3735+2615   TRUE:   6350   PRED:  64286\n",
      "✗  Q:    5688+7759   TRUE:  13447   PRED:  13418\n",
      "✗  Q:    2025+3260   TRUE:   5285   PRED:  52750\n",
      "✗  Q:    8524+7403   TRUE:  15927   PRED:  15899\n",
      "✗  Q:     980+5556   TRUE:   6536   PRED:  65695\n",
      "✗  Q:    1911-2754   TRUE:   -843   PRED:  -8122\n",
      "✗  Q:    1702+1297   TRUE:   2999   PRED:  29100\n",
      "✓  Q:    6529+7415   TRUE:  13944   PRED:  13944\n",
      "\n",
      "Final sanity check (greedy):\n",
      "Q:       54-7   PRED:  77733\n",
      "Q:      10+20   PRED:  15942\n",
      "Q:  9999+9999   PRED:  19744\n",
      "Q:     0-9999   PRED:  -9962\n",
      "Q:  1234-9999   PRED:  -8733\n",
      "Q:    777+888   PRED:  16944\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "1.2\n",
    "\n",
    "a) Overall, the model does not perform well on the task.\n",
    "Although the training loss decreases and the token-level accuracy improves, the model rarely produces completely correct answers. Most predictions are only partially correct (some digits match, but the final number is wrong), which means the exact-match accuracy is very low.\n",
    "\n",
    "⸻\n",
    "\n",
    "b) The main limitations are:\n",
    "\t•\tThe model does not truly learn arithmetic rules such as carry and borrow; it only learns statistical patterns over characters.\n",
    "\t•\tThere is a gap between training and inference because of teacher forcing.\n",
    "\t•\tThe encoder compresses the entire input into a single hidden state, which limits its ability to handle longer sequences.\n",
    "\t•\tGreedy decoding makes early mistakes hard to recover from.\n",
    "\n",
    "⸻\n",
    "\n",
    "c) I would:\n",
    "\t•\tUse curriculum learning (start with fewer digits and gradually increase).\n",
    "\t•\tReverse the input sequence to reduce long-range dependencies.\n",
    "\t•\tReduce teacher forcing over time.\n",
    "\t•\tIncrease model capacity or switch to a more suitable architecture such as a Transformer.\n",
    "\n",
    "⸻\n",
    "\n",
    "d)\n",
    "Yes.\n",
    "Adding attention would allow the decoder to focus on relevant parts of the input instead of relying on a single encoder state. This would likely improve performance, although it still may not fully solve the arithmetic reasoning problem."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T16:44:25.000319Z",
     "start_time": "2025-12-30T16:37:16.173676Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"Seq2Seq Arithmetic (PyTorch) — Attention Only (Bahdanau/additive)\n",
    "\n",
    "Task:\n",
    "  Input:  string like \"54-7\" or \"10+20\" (operands up to `config['digits']` digits)\n",
    "  Output: string answer like \"47\" or \"30\" (string, padded)\n",
    "\n",
    "Model:\n",
    "  LSTM encoder–decoder WITH additive attention (Bahdanau-style)\n",
    "\n",
    "Evaluation:\n",
    "  - token accuracy (ignoring PAD)\n",
    "  - exact-match accuracy (full output string must match)\n",
    "\"\"\"\n",
    "\n",
    "# -----------------------------\n",
    "# Read required params from notebook-provided config\n",
    "# -----------------------------\n",
    "SEED = int(config.get(\"seed\", 123))\n",
    "TRAIN_SIZE = int(config[\"training_size\"])  # e.g., 40000\n",
    "VAL_SIZE = int(config.get(\"val_size\", 10000))\n",
    "DIGITS = int(config[\"digits\"])             # e.g., 4\n",
    "HIDDEN_SIZE = int(config[\"hidden_size\"])   # e.g., 128\n",
    "BATCH_SIZE = int(config[\"batch_size\"])     # e.g., 128\n",
    "EPOCHS = int(min(config[\"iterations\"], 50))  # cap at 50\n",
    "\n",
    "# optional knobs\n",
    "EMB_DIM = int(config.get(\"emb_dim\", 64))\n",
    "LR = float(config.get(\"lr\", 1e-3))\n",
    "PRINT_SAMPLES = int(config.get(\"print_samples\", 12))\n",
    "\n",
    "# reproducibility\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Task constants\n",
    "# -----------------------------\n",
    "MAX_INT = 10**DIGITS - 1\n",
    "MAX_QUERY_LEN = 2 * DIGITS + 1   # e.g., \"9999+9999\" length 9\n",
    "MAX_ANS_LEN = DIGITS + 1         # e.g., \"19998\" len 5 OR \"-9999\" len 5\n",
    "\n",
    "PAD = \" \"\n",
    "SOS = \"^\"  # start token (internal)\n",
    "\n",
    "# Use the notebook's chars and add SOS internally\n",
    "CHARS = chars + SOS\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Vocab\n",
    "# -----------------------------\n",
    "class CharVocab:\n",
    "    def __init__(self, chars_: str):\n",
    "        self.chars = sorted(set(chars_))\n",
    "        self.stoi = {c: i for i, c in enumerate(self.chars)}\n",
    "        self.itos = {i: c for c, i in self.stoi.items()}\n",
    "        self.pad_idx = self.stoi[PAD]\n",
    "        self.sos_idx = self.stoi[SOS]\n",
    "\n",
    "    @property\n",
    "    def size(self) -> int:\n",
    "        return len(self.chars)\n",
    "\n",
    "    def encode(self, s: str, maxlen: int) -> torch.Tensor:\n",
    "        idx = [self.stoi[ch] for ch in s[:maxlen]]\n",
    "        if len(idx) < maxlen:\n",
    "            idx += [self.pad_idx] * (maxlen - len(idx))\n",
    "        return torch.tensor(idx, dtype=torch.long)\n",
    "\n",
    "    def decode(self, idx: torch.Tensor) -> str:\n",
    "        return \"\".join(self.itos[int(i)] for i in idx)\n",
    "\n",
    "\n",
    "vocab = CharVocab(CHARS)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Dataset\n",
    "# -----------------------------\n",
    "class ArithmeticSeq2SeqDataset(Dataset):\n",
    "    \"\"\"Returns (x, dec_in, y) as index tensors.\n",
    "\n",
    "    x:      (T_in,) query\n",
    "    dec_in: (T_out,) decoder input = SOS + y[:-1]\n",
    "    y:      (T_out,) target\n",
    "\n",
    "    Loss ignores PAD positions.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n: int, seed: int):\n",
    "        self.n = n\n",
    "        rng = np.random.default_rng(seed)\n",
    "\n",
    "        self.questions: List[str] = []\n",
    "        self.answers: List[str] = []\n",
    "\n",
    "        seen: set[Tuple[str, int, int]] = set()\n",
    "        while len(self.questions) < n:\n",
    "            a = int(rng.integers(0, MAX_INT + 1))\n",
    "            b = int(rng.integers(0, MAX_INT + 1))\n",
    "            op = \"+\" if rng.random() < 0.5 else \"-\"\n",
    "\n",
    "            key = (op, a, b)  # do NOT sort\n",
    "            if key in seen:\n",
    "                continue\n",
    "            seen.add(key)\n",
    "\n",
    "            ans = a + b if op == \"+\" else a - b\n",
    "            q = f\"{a}{op}{b}\".ljust(MAX_QUERY_LEN)\n",
    "            y = str(ans).ljust(MAX_ANS_LEN)\n",
    "\n",
    "            self.questions.append(q)\n",
    "            self.answers.append(y)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.n\n",
    "\n",
    "    def __getitem__(self, i: int):\n",
    "        q = self.questions[i]\n",
    "        y = self.answers[i]\n",
    "\n",
    "        x = vocab.encode(q, MAX_QUERY_LEN)\n",
    "        y_tgt = vocab.encode(y, MAX_ANS_LEN)\n",
    "\n",
    "        dec_in_str = (SOS + y[:-1]).ljust(MAX_ANS_LEN)\n",
    "        dec_in = vocab.encode(dec_in_str, MAX_ANS_LEN)\n",
    "\n",
    "        return x, dec_in, y_tgt\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Metrics\n",
    "# -----------------------------\n",
    "def token_acc_ignore_pad(pred: torch.Tensor, y: torch.Tensor, pad_idx: int) -> float:\n",
    "    mask = (y != pad_idx)\n",
    "    if mask.sum().item() == 0:\n",
    "        return 0.0\n",
    "    return (((pred == y) & mask).sum().item()) / mask.sum().item()\n",
    "\n",
    "\n",
    "def exact_match_rate(pred: torch.Tensor, y: torch.Tensor, pad_idx: int) -> float:\n",
    "    \"\"\"Exact match on the full answer string (ignoring PAD at the end).\n",
    "\n",
    "    pred,y: (B,T)\n",
    "    We compare after trimming trailing PAD from the TRUE sequence length.\n",
    "    \"\"\"\n",
    "    B, _ = y.shape\n",
    "    correct = 0\n",
    "    for i in range(B):\n",
    "        true_mask = (y[i] != pad_idx)\n",
    "        if true_mask.any():\n",
    "            L = int(true_mask.nonzero(as_tuple=False)[-1].item()) + 1\n",
    "        else:\n",
    "            L = 0\n",
    "        if L == 0:\n",
    "            correct += 1\n",
    "        else:\n",
    "            correct += int(torch.equal(pred[i, :L], y[i, :L]))\n",
    "    return correct / B\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def print_samples(model, dataset: ArithmeticSeq2SeqDataset, n: int) -> None:\n",
    "    model.eval()\n",
    "    rng = np.random.default_rng(SEED + 999)\n",
    "    idxs = rng.choice(len(dataset), size=n, replace=False)\n",
    "\n",
    "    xs, qs, ys_true = [], [], []\n",
    "    for i in idxs:\n",
    "        q = dataset.questions[i]\n",
    "        y = dataset.answers[i]\n",
    "        xs.append(vocab.encode(q, MAX_QUERY_LEN))\n",
    "        qs.append(q.strip())\n",
    "        ys_true.append(y.strip())\n",
    "\n",
    "    x_batch = torch.stack(xs, dim=0).to(device)\n",
    "    y_pred_idx = model.greedy_decode(x_batch, MAX_ANS_LEN)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"Samples from validation (greedy decode)\")\n",
    "    print(\"=\" * 80)\n",
    "    for q, y_t, y_p in zip(qs, ys_true, y_pred_idx):\n",
    "        pred = vocab.decode(y_p).replace(SOS, \"\").strip()\n",
    "        ok = \"✓\" if pred == y_t else \"✗\"\n",
    "        print(f\"{ok}  Q: {q:>12}   TRUE: {y_t:>6}   PRED: {pred:>6}\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Attention model (Bahdanau additive attention)\n",
    "# -----------------------------\n",
    "class AdditiveAttention(nn.Module):\n",
    "    def __init__(self, hidden_size: int):\n",
    "        super().__init__()\n",
    "        self.W_enc = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.W_dec = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.v = nn.Linear(hidden_size, 1, bias=False)\n",
    "\n",
    "    def forward(self, enc_out: torch.Tensor, dec_h: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Compute context vector.\n",
    "\n",
    "        enc_out: (B, T_in, H)\n",
    "        dec_h:   (B, H)\n",
    "        returns context: (B, H)\n",
    "        \"\"\"\n",
    "        score = self.v(torch.tanh(self.W_enc(enc_out) + self.W_dec(dec_h).unsqueeze(1)))  # (B,T,1)\n",
    "        alpha = torch.softmax(score.squeeze(-1), dim=-1)                                  # (B,T)\n",
    "        context = torch.bmm(alpha.unsqueeze(1), enc_out).squeeze(1)                       # (B,H)\n",
    "        return context\n",
    "\n",
    "\n",
    "class Seq2SeqLSTMAttn(nn.Module):\n",
    "    def __init__(self, vocab_size: int, emb_dim: int, hidden_size: int):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.encoder = nn.LSTM(emb_dim, hidden_size, batch_first=True)\n",
    "        self.decoder_cell = nn.LSTMCell(emb_dim, hidden_size)\n",
    "        self.attn = AdditiveAttention(hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size + hidden_size, vocab_size)  # [dec_h ; context]\n",
    "\n",
    "    def forward(self, x: torch.Tensor, dec_in: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Teacher-forcing forward.\n",
    "\n",
    "        x: (B,T_in), dec_in: (B,T_out)\n",
    "        returns logits: (B,T_out,V)\n",
    "        \"\"\"\n",
    "        x_emb = self.emb(x)\n",
    "        enc_out, (h, c) = self.encoder(x_emb)  # enc_out: (B,T_in,H)\n",
    "\n",
    "        _, T_out = dec_in.shape\n",
    "        logits_all = []\n",
    "\n",
    "        h_t = h.squeeze(0)  # (B,H)\n",
    "        c_t = c.squeeze(0)  # (B,H)\n",
    "\n",
    "        dec_emb = self.emb(dec_in)  # (B,T_out,E)\n",
    "\n",
    "        for t in range(T_out):\n",
    "            h_t, c_t = self.decoder_cell(dec_emb[:, t, :], (h_t, c_t))\n",
    "            ctx = self.attn(enc_out, h_t)\n",
    "            out = torch.cat([h_t, ctx], dim=-1)\n",
    "            logits_all.append(self.fc(out).unsqueeze(1))\n",
    "\n",
    "        return torch.cat(logits_all, dim=1)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def greedy_decode(self, x: torch.Tensor, max_len: int) -> torch.Tensor:\n",
    "        x_emb = self.emb(x)\n",
    "        enc_out, (h, c) = self.encoder(x_emb)\n",
    "\n",
    "        B = x.size(0)\n",
    "        y_pred = torch.full((B, max_len), vocab.pad_idx, dtype=torch.long, device=x.device)\n",
    "\n",
    "        h_t = h.squeeze(0)\n",
    "        c_t = c.squeeze(0)\n",
    "        prev = torch.full((B,), vocab.sos_idx, dtype=torch.long, device=x.device)\n",
    "\n",
    "        for t in range(max_len):\n",
    "            prev_emb = self.emb(prev)\n",
    "            h_t, c_t = self.decoder_cell(prev_emb, (h_t, c_t))\n",
    "            ctx = self.attn(enc_out, h_t)\n",
    "            out = torch.cat([h_t, ctx], dim=-1)\n",
    "            logits = self.fc(out)\n",
    "            next_idx = torch.argmax(logits, dim=-1)\n",
    "            y_pred[:, t] = next_idx\n",
    "            prev = next_idx\n",
    "\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Train & eval\n",
    "# -----------------------------\n",
    "@torch.no_grad()\n",
    "def eval_model(model, loader: DataLoader) -> Tuple[float, float, float]:\n",
    "    model.eval()\n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index=vocab.pad_idx)\n",
    "\n",
    "    total_loss, total_tok, total_em, n_batches = 0.0, 0.0, 0.0, 0\n",
    "\n",
    "    for x, dec_in, y in loader:\n",
    "        x, dec_in, y = x.to(device), dec_in.to(device), y.to(device)\n",
    "        logits = model(x, dec_in)\n",
    "\n",
    "        loss = loss_fn(logits.reshape(-1, vocab.size), y.reshape(-1))\n",
    "        pred = torch.argmax(logits, dim=-1)\n",
    "\n",
    "        total_loss += float(loss.item())\n",
    "        total_tok += float(token_acc_ignore_pad(pred, y, vocab.pad_idx))\n",
    "        total_em += float(exact_match_rate(pred, y, vocab.pad_idx))\n",
    "        n_batches += 1\n",
    "\n",
    "    return (\n",
    "        total_loss / max(n_batches, 1),\n",
    "        total_tok / max(n_batches, 1),\n",
    "        total_em / max(n_batches, 1),\n",
    "    )\n",
    "\n",
    "\n",
    "def train_model(model, train_loader: DataLoader, val_loader: DataLoader, name: str, val_ds: ArithmeticSeq2SeqDataset) -> None:\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index=vocab.pad_idx)\n",
    "\n",
    "    print(f\"\\n===== Training: {name} =====\")\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        model.train()\n",
    "        run_loss, run_tok, run_em, n_batches = 0.0, 0.0, 0.0, 0\n",
    "\n",
    "        for x, dec_in, y in train_loader:\n",
    "            x, dec_in, y = x.to(device), dec_in.to(device), y.to(device)\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            logits = model(x, dec_in)\n",
    "            loss = loss_fn(logits.reshape(-1, vocab.size), y.reshape(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            pred = torch.argmax(logits, dim=-1)\n",
    "            run_loss += float(loss.item())\n",
    "            run_tok += float(token_acc_ignore_pad(pred, y, vocab.pad_idx))\n",
    "            run_em += float(exact_match_rate(pred, y, vocab.pad_idx))\n",
    "            n_batches += 1\n",
    "\n",
    "        tr_loss = run_loss / max(n_batches, 1)\n",
    "        tr_tok = run_tok / max(n_batches, 1)\n",
    "        tr_em = run_em / max(n_batches, 1)\n",
    "\n",
    "        va_loss, va_tok, va_em = eval_model(model, val_loader)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch:02d}/{EPOCHS} | \"\n",
    "            f\"train loss={tr_loss:.4f} tok={tr_tok:.4f} em={tr_em:.4f} | \"\n",
    "            f\"val loss={va_loss:.4f} tok={va_tok:.4f} em={va_em:.4f}\"\n",
    "        )\n",
    "\n",
    "        if epoch in (1, EPOCHS):\n",
    "            print_samples(model, val_ds, n=min(PRINT_SAMPLES, len(val_ds)))\n",
    "\n",
    "\n",
    "def run_attention_only():\n",
    "    print(f\"Device: {device}\")\n",
    "    print(f\"config: {config}\")\n",
    "    print(f\"chars:  {chars!r}\")\n",
    "    print(f\"Vocab size: {vocab.size} | Vocab: {vocab.chars}\")\n",
    "    print(f\"MAX_QUERY_LEN={MAX_QUERY_LEN}, MAX_ANS_LEN={MAX_ANS_LEN}\")\n",
    "    print(f\"TRAIN_SIZE={TRAIN_SIZE}, VAL_SIZE={VAL_SIZE}, BATCH_SIZE={BATCH_SIZE}, EPOCHS={EPOCHS}\")\n",
    "\n",
    "    train_ds = ArithmeticSeq2SeqDataset(TRAIN_SIZE, seed=SEED)\n",
    "    val_ds = ArithmeticSeq2SeqDataset(VAL_SIZE, seed=SEED + 1)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    # Attention only\n",
    "    attn = Seq2SeqLSTMAttn(vocab.size, EMB_DIM, HIDDEN_SIZE)\n",
    "    train_model(attn, train_loader, val_loader, name=\"Attention (additive/Bahdanau)\", val_ds=val_ds)\n",
    "\n",
    "    # final sanity check\n",
    "    tests = [\"54-7\", \"10+20\", \"9999+9999\", \"0-9999\", \"1234-9999\", \"777+888\"]\n",
    "    print(\"\\nFinal sanity check (greedy):\")\n",
    "    attn.eval()\n",
    "    print(\"\\n--- attention ---\")\n",
    "    for t in tests:\n",
    "        q = t.ljust(MAX_QUERY_LEN)\n",
    "        x = vocab.encode(q, MAX_QUERY_LEN).unsqueeze(0).to(device)\n",
    "        y_pred = attn.greedy_decode(x, MAX_ANS_LEN)[0]\n",
    "        pred = vocab.decode(y_pred).replace(SOS, \"\").strip()\n",
    "        print(f\"Q: {t:>10}   PRED: {pred:>6}\")\n",
    "\n",
    "\n",
    "# Run in notebook:\n",
    "run_attention_only()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "config: {'training_size': 40000, 'digits': 4, 'hidden_size': 128, 'batch_size': 128, 'iterations': 50}\n",
      "chars:  '0123456789-+ '\n",
      "Vocab size: 14 | Vocab: [' ', '+', '-', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '^']\n",
      "MAX_QUERY_LEN=9, MAX_ANS_LEN=5\n",
      "TRAIN_SIZE=40000, VAL_SIZE=10000, BATCH_SIZE=128, EPOCHS=50\n",
      "\n",
      "===== Training: Attention (additive/Bahdanau) =====\n",
      "Epoch 01/50 | train loss=2.0829 tok=0.2150 em=0.0004 | val loss=1.9440 tok=0.2552 em=0.0008\n",
      "\n",
      "================================================================================\n",
      "Samples from validation (greedy decode)\n",
      "================================================================================\n",
      "✗  Q:    3844+9270   TRUE:  13114   PRED:  11448\n",
      "✗  Q:    2565+6526   TRUE:   9091   PRED:  81042\n",
      "✗  Q:     801-3219   TRUE:  -2418   PRED:  -1086\n",
      "✗  Q:    3984+1770   TRUE:   5754   PRED:  69999\n",
      "✗  Q:    3735+2615   TRUE:   6350   PRED:  54299\n",
      "✗  Q:    5688+7759   TRUE:  13447   PRED:  14484\n",
      "✗  Q:    2025+3260   TRUE:   5285   PRED:  54222\n",
      "✗  Q:    8524+7403   TRUE:  15927   PRED:  14848\n",
      "✗  Q:     980+5556   TRUE:   6536   PRED:  98888\n",
      "✗  Q:    1911-2754   TRUE:   -843   PRED:  -1072\n",
      "✗  Q:    1702+1297   TRUE:   2999   PRED:  54287\n",
      "✗  Q:    6529+7415   TRUE:  13944   PRED:  14484\n",
      "Epoch 02/50 | train loss=1.8533 tok=0.2930 em=0.0008 | val loss=1.7827 tok=0.3226 em=0.0017\n",
      "Epoch 03/50 | train loss=1.7410 tok=0.3338 em=0.0012 | val loss=1.7123 tok=0.3384 em=0.0010\n",
      "Epoch 04/50 | train loss=1.6708 tok=0.3576 em=0.0015 | val loss=1.6559 tok=0.3590 em=0.0008\n",
      "Epoch 05/50 | train loss=1.6174 tok=0.3760 em=0.0021 | val loss=1.6078 tok=0.3744 em=0.0025\n",
      "Epoch 06/50 | train loss=1.5772 tok=0.3888 em=0.0025 | val loss=1.5523 tok=0.3925 em=0.0024\n",
      "Epoch 07/50 | train loss=1.5346 tok=0.4025 em=0.0024 | val loss=1.5786 tok=0.3824 em=0.0018\n",
      "Epoch 08/50 | train loss=1.5059 tok=0.4131 em=0.0026 | val loss=1.5078 tok=0.4073 em=0.0022\n",
      "Epoch 09/50 | train loss=1.4781 tok=0.4230 em=0.0032 | val loss=1.4750 tok=0.4194 em=0.0032\n",
      "Epoch 10/50 | train loss=1.4634 tok=0.4276 em=0.0035 | val loss=1.4473 tok=0.4348 em=0.0033\n",
      "Epoch 11/50 | train loss=1.4408 tok=0.4378 em=0.0044 | val loss=1.4404 tok=0.4290 em=0.0028\n",
      "Epoch 12/50 | train loss=1.4250 tok=0.4421 em=0.0046 | val loss=1.4409 tok=0.4240 em=0.0024\n",
      "Epoch 13/50 | train loss=1.4082 tok=0.4490 em=0.0049 | val loss=1.3993 tok=0.4525 em=0.0035\n",
      "Epoch 14/50 | train loss=1.4023 tok=0.4511 em=0.0046 | val loss=1.4232 tok=0.4382 em=0.0031\n",
      "Epoch 15/50 | train loss=1.3947 tok=0.4553 em=0.0055 | val loss=1.3690 tok=0.4683 em=0.0050\n",
      "Epoch 16/50 | train loss=1.3669 tok=0.4678 em=0.0062 | val loss=1.4122 tok=0.4437 em=0.0044\n",
      "Epoch 17/50 | train loss=1.3734 tok=0.4638 em=0.0052 | val loss=1.4115 tok=0.4399 em=0.0038\n",
      "Epoch 18/50 | train loss=1.3572 tok=0.4697 em=0.0069 | val loss=1.3736 tok=0.4550 em=0.0039\n",
      "Epoch 19/50 | train loss=1.3400 tok=0.4781 em=0.0076 | val loss=1.3500 tok=0.4688 em=0.0043\n",
      "Epoch 20/50 | train loss=1.3386 tok=0.4795 em=0.0080 | val loss=1.3820 tok=0.4489 em=0.0031\n",
      "Epoch 21/50 | train loss=1.3303 tok=0.4821 em=0.0068 | val loss=1.3326 tok=0.4756 em=0.0055\n",
      "Epoch 22/50 | train loss=1.3253 tok=0.4833 em=0.0073 | val loss=1.3552 tok=0.4666 em=0.0042\n",
      "Epoch 23/50 | train loss=1.3206 tok=0.4852 em=0.0071 | val loss=1.3200 tok=0.4849 em=0.0057\n",
      "Epoch 24/50 | train loss=1.3141 tok=0.4880 em=0.0082 | val loss=1.3421 tok=0.4687 em=0.0041\n",
      "Epoch 25/50 | train loss=1.3019 tok=0.4946 em=0.0095 | val loss=1.3451 tok=0.4715 em=0.0057\n",
      "Epoch 26/50 | train loss=1.2997 tok=0.4959 em=0.0084 | val loss=1.3045 tok=0.4899 em=0.0066\n",
      "Epoch 27/50 | train loss=1.2941 tok=0.4967 em=0.0080 | val loss=1.3829 tok=0.4588 em=0.0044\n",
      "Epoch 28/50 | train loss=1.2868 tok=0.5017 em=0.0093 | val loss=1.3565 tok=0.4658 em=0.0045\n",
      "Epoch 29/50 | train loss=1.2854 tok=0.5001 em=0.0086 | val loss=1.3009 tok=0.4906 em=0.0062\n",
      "Epoch 30/50 | train loss=1.2866 tok=0.5017 em=0.0102 | val loss=1.2964 tok=0.4924 em=0.0050\n",
      "Epoch 31/50 | train loss=1.2652 tok=0.5118 em=0.0104 | val loss=1.2781 tok=0.5023 em=0.0065\n",
      "Epoch 32/50 | train loss=1.2686 tok=0.5086 em=0.0101 | val loss=1.3201 tok=0.4774 em=0.0047\n",
      "Epoch 33/50 | train loss=1.2575 tok=0.5148 em=0.0112 | val loss=1.2843 tok=0.4963 em=0.0062\n",
      "Epoch 34/50 | train loss=1.2695 tok=0.5078 em=0.0109 | val loss=1.3727 tok=0.4593 em=0.0036\n",
      "Epoch 35/50 | train loss=1.2506 tok=0.5175 em=0.0113 | val loss=1.2663 tok=0.5069 em=0.0078\n",
      "Epoch 36/50 | train loss=1.2703 tok=0.5087 em=0.0104 | val loss=1.2825 tok=0.4988 em=0.0064\n",
      "Epoch 37/50 | train loss=1.2506 tok=0.5175 em=0.0127 | val loss=1.2869 tok=0.4971 em=0.0063\n",
      "Epoch 38/50 | train loss=1.2496 tok=0.5178 em=0.0115 | val loss=1.2895 tok=0.4966 em=0.0069\n",
      "Epoch 39/50 | train loss=1.2428 tok=0.5209 em=0.0130 | val loss=1.2777 tok=0.4981 em=0.0054\n",
      "Epoch 40/50 | train loss=1.2423 tok=0.5200 em=0.0117 | val loss=1.2801 tok=0.4996 em=0.0060\n",
      "Epoch 41/50 | train loss=1.2391 tok=0.5234 em=0.0132 | val loss=1.2863 tok=0.4982 em=0.0066\n",
      "Epoch 42/50 | train loss=1.2222 tok=0.5309 em=0.0138 | val loss=1.2588 tok=0.5107 em=0.0084\n",
      "Epoch 43/50 | train loss=1.2354 tok=0.5241 em=0.0124 | val loss=1.2508 tok=0.5122 em=0.0053\n",
      "Epoch 44/50 | train loss=1.2261 tok=0.5277 em=0.0133 | val loss=1.2390 tok=0.5197 em=0.0077\n",
      "Epoch 45/50 | train loss=1.2241 tok=0.5287 em=0.0141 | val loss=1.2686 tok=0.5041 em=0.0081\n",
      "Epoch 46/50 | train loss=1.2163 tok=0.5317 em=0.0144 | val loss=1.2343 tok=0.5245 em=0.0091\n",
      "Epoch 47/50 | train loss=1.2164 tok=0.5323 em=0.0144 | val loss=1.3174 tok=0.4856 em=0.0048\n",
      "Epoch 48/50 | train loss=1.2110 tok=0.5355 em=0.0150 | val loss=1.2632 tok=0.5111 em=0.0067\n",
      "Epoch 49/50 | train loss=1.2096 tok=0.5361 em=0.0152 | val loss=1.2816 tok=0.5007 em=0.0070\n",
      "Epoch 50/50 | train loss=1.2197 tok=0.5305 em=0.0151 | val loss=1.2736 tok=0.5023 em=0.0051\n",
      "\n",
      "================================================================================\n",
      "Samples from validation (greedy decode)\n",
      "================================================================================\n",
      "✗  Q:    3844+9270   TRUE:  13114   PRED:  13155\n",
      "✗  Q:    2565+6526   TRUE:   9091   PRED:  91143\n",
      "✗  Q:     801-3219   TRUE:  -2418   PRED:  -2351\n",
      "✗  Q:    3984+1770   TRUE:   5754   PRED:  58537\n",
      "✗  Q:    3735+2615   TRUE:   6350   PRED:  64445\n",
      "✗  Q:    5688+7759   TRUE:  13447   PRED:  13400\n",
      "✗  Q:    2025+3260   TRUE:   5285   PRED:  54484\n",
      "✗  Q:    8524+7403   TRUE:  15927   PRED:  15957\n",
      "✗  Q:     980+5556   TRUE:   6536   PRED:  65555\n",
      "✗  Q:    1911-2754   TRUE:   -843   PRED:  -8948\n",
      "✗  Q:    1702+1297   TRUE:   2999   PRED:  30443\n",
      "✗  Q:    6529+7415   TRUE:  13944   PRED:  13942\n",
      "\n",
      "Final sanity check (greedy):\n",
      "\n",
      "--- attention ---\n",
      "Q:       54-7   PRED:  49909\n",
      "Q:      10+20   PRED:  14480\n",
      "Q:  9999+9999   PRED:  19894\n",
      "Q:     0-9999   PRED:  -8969\n",
      "Q:  1234-9999   PRED:  -8733\n",
      "Q:    777+888   PRED:  16666\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "1.4).                     What it changes vs 1.1/1.3\n",
    "\t•\tLSTM seq2seq (even with attention) still struggles with algorithmic generalization and exposure bias.\n",
    "\t•\tTransformer gives direct token-to-token interactions and typically learns digit alignment/carry rules better."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T17:32:59.365022Z",
     "start_time": "2025-12-30T16:53:47.826881Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# -----------------------------\n",
    "# Read required params\n",
    "# -----------------------------\n",
    "SEED = int(config.get(\"seed\", 123))\n",
    "TRAIN_SIZE = int(config[\"training_size\"])\n",
    "VAL_SIZE = int(config.get(\"val_size\", 10000))\n",
    "DIGITS = int(config[\"digits\"])\n",
    "BATCH_SIZE = int(config[\"batch_size\"])\n",
    "EPOCHS = int(min(config[\"iterations\"], 50))\n",
    "\n",
    "# Transformer knobs (reasonable defaults)\n",
    "D_MODEL = int(config.get(\"d_model\", 128))\n",
    "NHEAD = int(config.get(\"nhead\", 8))\n",
    "NUM_LAYERS = int(config.get(\"num_layers\", 3))\n",
    "DIM_FF = int(config.get(\"dim_ff\", 512))\n",
    "DROPOUT = float(config.get(\"dropout\", 0.1))\n",
    "LR = float(config.get(\"lr\", 3e-4))\n",
    "PRINT_SAMPLES = int(config.get(\"print_samples\", 12))\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# -----------------------------\n",
    "# Task constants\n",
    "# -----------------------------\n",
    "MAX_INT = 10**DIGITS - 1\n",
    "MAX_QUERY_LEN = 2 * DIGITS + 1\n",
    "MAX_ANS_LEN = DIGITS + 1\n",
    "\n",
    "PAD = \" \"\n",
    "SOS = \"^\"\n",
    "EOS = \"$\"   # add explicit end token (helps)\n",
    "CHARS = chars + SOS + EOS\n",
    "\n",
    "# -----------------------------\n",
    "# Vocab\n",
    "# -----------------------------\n",
    "class CharVocab:\n",
    "    def __init__(self, chars_: str):\n",
    "        self.chars = sorted(set(chars_))\n",
    "        self.stoi = {c: i for i, c in enumerate(self.chars)}\n",
    "        self.itos = {i: c for c, i in self.stoi.items()}\n",
    "        self.pad_idx = self.stoi[PAD]\n",
    "        self.sos_idx = self.stoi[SOS]\n",
    "        self.eos_idx = self.stoi[EOS]\n",
    "\n",
    "    @property\n",
    "    def size(self) -> int:\n",
    "        return len(self.chars)\n",
    "\n",
    "    def encode(self, s: str, maxlen: int) -> torch.Tensor:\n",
    "        idx = [self.stoi[ch] for ch in s[:maxlen]]\n",
    "        if len(idx) < maxlen:\n",
    "            idx += [self.pad_idx] * (maxlen - len(idx))\n",
    "        return torch.tensor(idx, dtype=torch.long)\n",
    "\n",
    "    def decode(self, idx: torch.Tensor) -> str:\n",
    "        return \"\".join(self.itos[int(i)] for i in idx)\n",
    "\n",
    "vocab = CharVocab(CHARS)\n",
    "\n",
    "# -----------------------------\n",
    "# Dataset (answers have EOS)\n",
    "# -----------------------------\n",
    "class ArithmeticDataset(Dataset):\n",
    "    def __init__(self, n: int, seed: int):\n",
    "        self.n = n\n",
    "        rng = np.random.default_rng(seed)\n",
    "        self.questions: List[str] = []\n",
    "        self.answers: List[str] = []\n",
    "        seen: set[Tuple[str,int,int]] = set()\n",
    "\n",
    "        while len(self.questions) < n:\n",
    "            a = int(rng.integers(0, MAX_INT + 1))\n",
    "            b = int(rng.integers(0, MAX_INT + 1))\n",
    "            op = \"+\" if rng.random() < 0.5 else \"-\"\n",
    "            key = (op, a, b)\n",
    "            if key in seen:\n",
    "                continue\n",
    "            seen.add(key)\n",
    "\n",
    "            ans = a + b if op == \"+\" else a - b\n",
    "            q = f\"{a}{op}{b}\".ljust(MAX_QUERY_LEN)\n",
    "\n",
    "            # target string: answer + EOS, then pad to MAX_ANS_LEN+1\n",
    "            y = (str(ans) + EOS)\n",
    "            y = y.ljust(MAX_ANS_LEN + 1)\n",
    "\n",
    "            self.questions.append(q)\n",
    "            self.answers.append(y)\n",
    "\n",
    "    def __len__(self): return self.n\n",
    "\n",
    "    def __getitem__(self, i: int):\n",
    "        q = self.questions[i]\n",
    "        y = self.answers[i]\n",
    "        x = vocab.encode(q, MAX_QUERY_LEN)                # (Tin,)\n",
    "        y_tgt = vocab.encode(y, MAX_ANS_LEN + 1)          # (Tout,)\n",
    "        # decoder input: SOS + y[:-1]\n",
    "        y_in = (SOS + y[:-1]).ljust(MAX_ANS_LEN + 1)\n",
    "        y_in = vocab.encode(y_in, MAX_ANS_LEN + 1)\n",
    "        return x, y_in, y_tgt\n",
    "\n",
    "# -----------------------------\n",
    "# Positional encoding\n",
    "# -----------------------------\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, dropout: float, max_len: int = 128):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pos = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(pos * div)\n",
    "        pe[:, 1::2] = torch.cos(pos * div)\n",
    "        self.register_buffer(\"pe\", pe.unsqueeze(0))  # (1, max_len, d_model)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: (B,T,d_model)\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "# -----------------------------\n",
    "# Transformer seq2seq\n",
    "# -----------------------------\n",
    "class TransformerSeq2Seq(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab.size, D_MODEL)\n",
    "        self.pos = PositionalEncoding(D_MODEL, DROPOUT, max_len=256)\n",
    "\n",
    "        self.tf = nn.Transformer(\n",
    "            d_model=D_MODEL,\n",
    "            nhead=NHEAD,\n",
    "            num_encoder_layers=NUM_LAYERS,\n",
    "            num_decoder_layers=NUM_LAYERS,\n",
    "            dim_feedforward=DIM_FF,\n",
    "            dropout=DROPOUT,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.out = nn.Linear(D_MODEL, vocab.size)\n",
    "\n",
    "    def forward(self, src: torch.Tensor, tgt_in: torch.Tensor) -> torch.Tensor:\n",
    "        # src: (B,Tin), tgt_in: (B,Tout)\n",
    "        src_key_padding = (src == vocab.pad_idx)\n",
    "        tgt_key_padding = (tgt_in == vocab.pad_idx)\n",
    "\n",
    "        B, Tt = tgt_in.shape\n",
    "        # causal mask for decoder\n",
    "        causal = torch.triu(torch.ones(Tt, Tt, device=src.device), diagonal=1).bool()\n",
    "\n",
    "        src_e = self.pos(self.emb(src))\n",
    "        tgt_e = self.pos(self.emb(tgt_in))\n",
    "\n",
    "        h = self.tf(\n",
    "            src_e, tgt_e,\n",
    "            src_key_padding_mask=src_key_padding,\n",
    "            tgt_key_padding_mask=tgt_key_padding,\n",
    "            tgt_mask=causal\n",
    "        )\n",
    "        return self.out(h)  # (B,Tout,V)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def greedy_decode(self, src: torch.Tensor, max_len: int) -> torch.Tensor:\n",
    "        self.eval()\n",
    "        B = src.size(0)\n",
    "        ys = torch.full((B, 1), vocab.sos_idx, dtype=torch.long, device=src.device)\n",
    "\n",
    "        for _ in range(max_len):\n",
    "            logits = self(src, ys)              # (B, t, V)\n",
    "            next_tok = torch.argmax(logits[:, -1, :], dim=-1)  # (B,)\n",
    "            ys = torch.cat([ys, next_tok.unsqueeze(1)], dim=1)\n",
    "        return ys[:, 1:]  # drop SOS\n",
    "\n",
    "# -----------------------------\n",
    "# Metrics\n",
    "# -----------------------------\n",
    "def token_acc_ignore_pad(pred, y):\n",
    "    mask = (y != vocab.pad_idx)\n",
    "    if mask.sum().item() == 0: return 0.0\n",
    "    return (((pred == y) & mask).sum().item()) / mask.sum().item()\n",
    "\n",
    "def exact_match(pred, y):\n",
    "    # compare until EOS in y\n",
    "    B, T = y.shape\n",
    "    ok = 0\n",
    "    for i in range(B):\n",
    "        y_i = y[i]\n",
    "        eos_pos = (y_i == vocab.eos_idx).nonzero(as_tuple=False)\n",
    "        L = int(eos_pos[0].item()) + 1 if len(eos_pos) else T\n",
    "        ok += int(torch.equal(pred[i, :L], y_i[:L]))\n",
    "    return ok / B\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_epoch(model, loader):\n",
    "    model.eval()\n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index=vocab.pad_idx)\n",
    "    tot_loss = tot_tok = tot_em = 0.0\n",
    "    n = 0\n",
    "    for x, y_in, y in loader:\n",
    "        x, y_in, y = x.to(device), y_in.to(device), y.to(device)\n",
    "        logits = model(x, y_in)\n",
    "        loss = loss_fn(logits.reshape(-1, vocab.size), y.reshape(-1))\n",
    "        pred = torch.argmax(logits, dim=-1)\n",
    "        tot_loss += float(loss.item())\n",
    "        tot_tok += float(token_acc_ignore_pad(pred, y))\n",
    "        tot_em += float(exact_match(pred, y))\n",
    "        n += 1\n",
    "    return tot_loss/n, tot_tok/n, tot_em/n\n",
    "\n",
    "@torch.no_grad()\n",
    "def print_samples(model, ds, n=10):\n",
    "    model.eval()\n",
    "    rng = np.random.default_rng(SEED + 777)\n",
    "    idxs = rng.choice(len(ds), size=n, replace=False)\n",
    "    xs, qs, ys = [], [], []\n",
    "    for i in idxs:\n",
    "        q = ds.questions[i]\n",
    "        y = ds.answers[i]\n",
    "        xs.append(vocab.encode(q, MAX_QUERY_LEN))\n",
    "        qs.append(q.strip())\n",
    "        ys.append(y.replace(EOS, \"\").strip())\n",
    "    x = torch.stack(xs).to(device)\n",
    "    pred = model.greedy_decode(x, max_len=MAX_ANS_LEN + 1)\n",
    "    print(\"\\nSamples:\")\n",
    "    for q, y_true, y_pred in zip(qs, ys, pred):\n",
    "        s = vocab.decode(y_pred)\n",
    "        s = s.split(EOS)[0].strip()\n",
    "        print(f\"Q:{q:>12}  TRUE:{y_true:>6}  PRED:{s:>6}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Train\n",
    "# -----------------------------\n",
    "def run_transformer():\n",
    "    train_ds = ArithmeticDataset(TRAIN_SIZE, seed=SEED)\n",
    "    val_ds = ArithmeticDataset(VAL_SIZE, seed=SEED+1)\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    model = TransformerSeq2Seq().to(device)\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index=vocab.pad_idx)\n",
    "\n",
    "    print(\"Training Transformer seq2seq...\")\n",
    "    for ep in range(1, EPOCHS+1):\n",
    "        model.train()\n",
    "        tot = 0.0\n",
    "        nb = 0\n",
    "        for x, y_in, y in train_loader:\n",
    "            x, y_in, y = x.to(device), y_in.to(device), y.to(device)\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            logits = model(x, y_in)\n",
    "            loss = loss_fn(logits.reshape(-1, vocab.size), y.reshape(-1))\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            opt.step()\n",
    "            tot += float(loss.item())\n",
    "            nb += 1\n",
    "\n",
    "        va_loss, va_tok, va_em = eval_epoch(model, val_loader)\n",
    "        print(f\"Epoch {ep:02d}/{EPOCHS} | train loss={tot/nb:.4f} | val loss={va_loss:.4f} tok={va_tok:.4f} em={va_em:.4f}\")\n",
    "\n",
    "        if ep in (1, EPOCHS):\n",
    "            print_samples(model, val_ds, n=min(PRINT_SAMPLES, len(val_ds)))\n",
    "\n",
    "    return model\n",
    "\n",
    "# Run:\n",
    "model_tf = run_transformer()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Transformer seq2seq...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/netanelazran/miniconda3/lib/python3.12/site-packages/torch/nn/modules/transformer.py:505: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. We recommend specifying layout=torch.jagged when constructing a nested tensor, as this layout receives active development, has better operator coverage, and works with torch.compile. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/NestedTensorImpl.cpp:182.)\n",
      "  output = torch._nested_tensor_from_mask(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01/50 | train loss=1.8769 | val loss=1.7263 tok=0.3610 em=0.0003\n",
      "\n",
      "Samples:\n",
      "Q:     45+7528  TRUE:  7573  PRED: 10000\n",
      "Q:   9327-6960  TRUE:  2367  PRED:   100\n",
      "Q:    1307+419  TRUE:  1726  PRED:  4005\n",
      "Q:   2245+7017  TRUE:  9262  PRED: 10000\n",
      "Q:   1117+8892  TRUE: 10009  PRED: 10000\n",
      "Q:   5883-6613  TRUE:  -730  PRED: -1107\n",
      "Q:   1116+6332  TRUE:  7448  PRED:  7005\n",
      "Q:   5014-7456  TRUE: -2442  PRED:  -157\n",
      "Q:   8517+5383  TRUE: 13900  PRED: 10455\n",
      "Q:    619+7957  TRUE:  8576  PRED: 10455\n",
      "Q:   1609+7288  TRUE:  8897  PRED:  9000\n",
      "Q:   3841+8137  TRUE: 11978  PRED: 10405\n",
      "Epoch 02/50 | train loss=1.6594 | val loss=1.6350 tok=0.4109 em=0.0005\n",
      "Epoch 03/50 | train loss=1.5561 | val loss=1.6205 tok=0.4268 em=0.0002\n",
      "Epoch 04/50 | train loss=1.5041 | val loss=1.6122 tok=0.4374 em=0.0011\n",
      "Epoch 05/50 | train loss=1.4695 | val loss=1.6206 tok=0.4316 em=0.0001\n",
      "Epoch 06/50 | train loss=1.4444 | val loss=1.6088 tok=0.4453 em=0.0010\n",
      "Epoch 07/50 | train loss=1.4267 | val loss=1.6157 tok=0.4520 em=0.0012\n",
      "Epoch 08/50 | train loss=1.4073 | val loss=1.6100 tok=0.4518 em=0.0009\n",
      "Epoch 09/50 | train loss=1.3949 | val loss=1.6051 tok=0.4545 em=0.0006\n",
      "Epoch 10/50 | train loss=1.3762 | val loss=1.5914 tok=0.4644 em=0.0014\n",
      "Epoch 11/50 | train loss=1.3586 | val loss=1.5648 tok=0.4748 em=0.0017\n",
      "Epoch 12/50 | train loss=1.3445 | val loss=1.5654 tok=0.4718 em=0.0017\n",
      "Epoch 13/50 | train loss=1.3327 | val loss=1.5710 tok=0.4757 em=0.0014\n",
      "Epoch 14/50 | train loss=1.3207 | val loss=1.5588 tok=0.4790 em=0.0015\n",
      "Epoch 15/50 | train loss=1.3122 | val loss=1.5700 tok=0.4820 em=0.0025\n",
      "Epoch 16/50 | train loss=1.3015 | val loss=1.5620 tok=0.4834 em=0.0027\n",
      "Epoch 17/50 | train loss=1.2951 | val loss=1.5600 tok=0.4867 em=0.0016\n",
      "Epoch 18/50 | train loss=1.2893 | val loss=1.5533 tok=0.4844 em=0.0021\n",
      "Epoch 19/50 | train loss=1.2798 | val loss=1.5594 tok=0.4919 em=0.0022\n",
      "Epoch 20/50 | train loss=1.2741 | val loss=1.5767 tok=0.4786 em=0.0010\n",
      "Epoch 21/50 | train loss=1.2673 | val loss=1.5515 tok=0.4917 em=0.0018\n",
      "Epoch 22/50 | train loss=1.2625 | val loss=1.5484 tok=0.4925 em=0.0025\n",
      "Epoch 23/50 | train loss=1.2584 | val loss=1.5472 tok=0.4934 em=0.0022\n",
      "Epoch 24/50 | train loss=1.2499 | val loss=1.5494 tok=0.4902 em=0.0020\n",
      "Epoch 25/50 | train loss=1.2473 | val loss=1.5562 tok=0.4907 em=0.0009\n",
      "Epoch 26/50 | train loss=1.2432 | val loss=1.5432 tok=0.4963 em=0.0028\n",
      "Epoch 27/50 | train loss=1.2392 | val loss=1.5690 tok=0.4903 em=0.0019\n",
      "Epoch 28/50 | train loss=1.2342 | val loss=1.5547 tok=0.4895 em=0.0014\n",
      "Epoch 29/50 | train loss=1.2301 | val loss=1.5487 tok=0.5022 em=0.0019\n",
      "Epoch 30/50 | train loss=1.2266 | val loss=1.5445 tok=0.4982 em=0.0024\n",
      "Epoch 31/50 | train loss=1.2227 | val loss=1.5525 tok=0.5043 em=0.0036\n",
      "Epoch 32/50 | train loss=1.2196 | val loss=1.5417 tok=0.5028 em=0.0037\n",
      "Epoch 33/50 | train loss=1.2162 | val loss=1.5328 tok=0.5048 em=0.0034\n",
      "Epoch 34/50 | train loss=1.2122 | val loss=1.5422 tok=0.5072 em=0.0020\n",
      "Epoch 35/50 | train loss=1.2019 | val loss=1.5281 tok=0.5083 em=0.0030\n",
      "Epoch 36/50 | train loss=1.1900 | val loss=1.5145 tok=0.5230 em=0.0054\n",
      "Epoch 37/50 | train loss=1.1671 | val loss=1.4684 tok=0.5314 em=0.0046\n",
      "Epoch 38/50 | train loss=1.1408 | val loss=1.4719 tok=0.5370 em=0.0040\n",
      "Epoch 39/50 | train loss=1.1243 | val loss=1.4523 tok=0.5483 em=0.0046\n",
      "Epoch 40/50 | train loss=1.1038 | val loss=1.4471 tok=0.5498 em=0.0059\n",
      "Epoch 41/50 | train loss=1.0917 | val loss=1.4606 tok=0.5521 em=0.0047\n",
      "Epoch 42/50 | train loss=1.0818 | val loss=1.4318 tok=0.5603 em=0.0048\n",
      "Epoch 43/50 | train loss=1.0717 | val loss=1.4163 tok=0.5706 em=0.0062\n",
      "Epoch 44/50 | train loss=1.0632 | val loss=1.4621 tok=0.5668 em=0.0059\n",
      "Epoch 45/50 | train loss=1.0540 | val loss=1.4687 tok=0.5689 em=0.0074\n",
      "Epoch 46/50 | train loss=1.0438 | val loss=1.4565 tok=0.5748 em=0.0072\n",
      "Epoch 47/50 | train loss=1.0386 | val loss=1.4524 tok=0.5789 em=0.0076\n",
      "Epoch 48/50 | train loss=1.0276 | val loss=1.4438 tok=0.5852 em=0.0075\n",
      "Epoch 49/50 | train loss=1.0206 | val loss=1.4411 tok=0.5897 em=0.0089\n",
      "Epoch 50/50 | train loss=1.0144 | val loss=1.4711 tok=0.5901 em=0.0098\n",
      "\n",
      "Samples:\n",
      "Q:     45+7528  TRUE:  7573  PRED:  8484\n",
      "Q:   9327-6960  TRUE:  2367  PRED:  2439\n",
      "Q:    1307+419  TRUE:  1726  PRED:  5494\n",
      "Q:   2245+7017  TRUE:  9262  PRED:  9385\n",
      "Q:   1117+8892  TRUE: 10009  PRED: 10004\n",
      "Q:   5883-6613  TRUE:  -730  PRED:  -733\n",
      "Q:   1116+6332  TRUE:  7448  PRED:  7444\n",
      "Q:   5014-7456  TRUE: -2442  PRED: -2430\n",
      "Q:   8517+5383  TRUE: 13900  PRED: 13983\n",
      "Q:    619+7957  TRUE:  8576  PRED: 15504\n",
      "Q:   1609+7288  TRUE:  8897  PRED:  8944\n",
      "Q:   3841+8137  TRUE: 11978  PRED: 12024\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Model Comparison (1.1 vs 1.3 vs 1.4)\n",
    "\n",
    "I compared three models on the arithmetic seq2seq task using validation results:\n",
    "• Baseline LSTM (1.1, no attention)\n",
    "• LSTM with additive attention (1.3)\n",
    "• Transformer encoder–decoder (1.4)\n",
    "\n",
    "The comparison was based on:\n",
    "• Exact-match accuracy (EM) – main metric\n",
    "• Token accuracy – secondary\n",
    "• Validation loss – supporting\n",
    "\n",
    "The baseline LSTM (1.1) achieved a validation loss of around 1.29 with approximately 50% token accuracy, but produced almost no exact-match outputs, indicating that it often predicts individual digits correctly but fails to generate a fully correct numerical result.\n",
    "\n",
    "The LSTM with attention (1.3) showed a small improvement, reaching validation losses of about 1.25–1.28, token accuracy in the range of 51–53%, and an exact-match accuracy of approximately 0.7–1.0%. This suggests that attention helps the model align input digits with output positions, but the improvement in full correctness remains limited.\n",
    "\n",
    "The Transformer model (1.4) achieved the best performance overall, with token accuracy of around 60% and the highest exact-match accuracy of roughly 1.1%, despite having a higher validation loss (around 1.45). Since exact-match accuracy is the most meaningful metric for this task, this result indicates that the Transformer produces more fully correct answers than the LSTM-based models.\n",
    "\n",
    "In conclusion, the Transformer outperforms the models from 1.1 and 1.3 on the metric that matters most for arithmetic reasoning—exact-match accuracy—likely due to its self-attention mechanism, which avoids the recurrent bottleneck and better models digit-level dependencies such as carry and borrow."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "---"
   ],
   "metadata": {
    "id": "voVYROYNlO49"
   }
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X-d0eIM6FeaM"
   },
   "source": [
    "## Part 2: A language translation model with attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "80jhFbWPMW_a"
   },
   "source": [
    "In this part of the problem set we are going to implement a translation with a Sequence to Sequence Network and Attention model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TgL38lJGTYaF"
   },
   "source": [
    "0) Please go over the NLP From Scratch: Translation with a Sequence to Sequence Network and Attention [tutorial](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html). This attention model is very similar to what was learned in class (Luong), but a bit different. What are the main differences between  Badahnau and Luong attention mechanisms?    \n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "solutions 0.)  The main differences between Bahdanau and Luong attention are:\n",
    "\n",
    "Bahdanau (additive) attention computes alignment scores before generating the decoder output, using the previous decoder hidden state and a small feedforward network. This makes it more expressive but computationally heavier.\n",
    "Luong attention computes attention after the decoder hidden state is produced and uses simpler multiplicative scoring functions (such as dot or general), which are more efficient.\n",
    "\n",
    "In summary, Bahdanau attention is more flexible, while Luong attention is simpler and faster. The attention mechanism used in the tutorial is closer to Luong attention, with minor differences in how the context vector is combined with the decoder state."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "1.a) Using `!wget`, `!unzip` , download and extract the [hebrew-english](https://www.manythings.org/anki/) sentence pairs text file to the Colab `content/`  folder (or local folder if not using Colab).\n",
    "1.b) The `heb.txt` must be parsed and cleaned (see tutorial for requirements or change the code as you see fit).   \n"
   ],
   "metadata": {
    "id": "KBX873GJlDl9"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "2.a) Use the tutorial example to build  and train a Hebrew to English translation model with attention (using the parameters in the code cell below). Apply the same `eng_prefixes` filter to limit the train/test data.   \n",
    "2.b) Evaluate your trained model randomly on 20 sentences.  \n",
    "2.c) Show the attention plot for 5 random sentences.  \n"
   ],
   "metadata": {
    "id": "AvIIlNvPlGWB"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "3) Do you think this model performs well? Why or why not? What are its limitations/disadvantages? What would you do to improve it?  \n"
   ],
   "metadata": {
    "id": "qcqtVxkclIWG"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "4) Using any neural network architecture of your liking, build  a model with the aim to beat the model in 2.a. Compare your results in a meaningful way, and add a short explanation to why you think/thought your suggested network is better."
   ],
   "metadata": {
    "id": "i2VSrRNtlJub"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "c-tVmomvXcKk",
    "ExecuteTime": {
     "end_time": "2025-12-30T19:56:45.012675Z",
     "start_time": "2025-12-30T19:56:44.993948Z"
    }
   },
   "source": [
    "# use the following parameters:\n",
    "MAX_LENGTH = 10\n",
    "hidden_size = 128\n",
    "epochs = 50"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J9-C4pLEXzCF"
   },
   "source": [
    "SOLUTION:"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "### MISSING\n",
    "!curl -L -o heb-eng.zip http://www.manythings.org/anki/heb-eng.zip\n",
    "!unzip -o heb-eng.zip -d heb_eng"
   ],
   "metadata": {
    "id": "_WrHkLD6p813",
    "ExecuteTime": {
     "end_time": "2025-12-30T19:56:49.867815Z",
     "start_time": "2025-12-30T19:56:46.562995Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\r\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\r\n",
      "100 4636k  100 4636k    0     0  1636k      0  0:00:02  0:00:02 --:--:-- 1635k\r\n",
      "Archive:  heb-eng.zip\r\n",
      "  inflating: heb_eng/_about.txt      \r\n",
      "  inflating: heb_eng/heb.txt         \r\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T21:57:35.221186Z",
     "start_time": "2025-12-30T21:57:35.211065Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## import part 2 ##\n",
    "import re\n",
    "import unicodedata\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "from typing import List, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cpu\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "source": [
    "def unicode_to_ascii(s: str) -> str:\n",
    "    # keep English side consistent with tutorial\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize(\"NFD\", s)\n",
    "        if unicodedata.category(c) != \"Mn\"\n",
    "    )\n",
    "\n",
    "def normalize_en(s: str) -> str:\n",
    "    s = unicode_to_ascii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def normalize_he(s: str) -> str:\n",
    "    # keep Hebrew letters, basic punctuation; normalize whitespace\n",
    "    s = s.strip()\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s\n",
    "\n",
    "def read_heb_eng_pairs(path: str, max_lines: int | None = None):\n",
    "    pairs = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if max_lines is not None and i >= max_lines:\n",
    "                break\n",
    "            parts = line.rstrip(\"\\n\").split(\"\\t\")\n",
    "            if len(parts) < 2:\n",
    "                continue\n",
    "            en, he = parts[0], parts[1]\n",
    "            en_n = normalize_en(en)\n",
    "            he_n = normalize_he(he)\n",
    "\n",
    "            # basic filtering\n",
    "            if not en_n or not he_n:\n",
    "                continue\n",
    "            pairs.append((en_n, he_n))\n",
    "    return pairs\n",
    "\n",
    "heb_path = \"heb_eng/heb.txt\"\n",
    "pairs = read_heb_eng_pairs(heb_path)\n",
    "\n",
    "print(\"Num pairs:\", len(pairs))\n",
    "print(\"Example:\", pairs[0])"
   ],
   "metadata": {
    "id": "gSQwZfVqp9gn",
    "ExecuteTime": {
     "end_time": "2025-12-30T20:01:01.049193Z",
     "start_time": "2025-12-30T20:00:59.939271Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num pairs: 136845\n",
      "Example: ('go .', 'לך!')\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T21:35:34.703935Z",
     "start_time": "2025-12-30T20:03:31.257765Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# =========================\n",
    "# Hebrew -> English Seq2Seq with Attention (Tutorial-style)\n",
    "# -----------------------------\n",
    "# Normalization\n",
    "# -----------------------------\n",
    "def unicode_to_ascii(s: str) -> str:\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize(\"NFD\", s)\n",
    "        if unicodedata.category(c) != \"Mn\"\n",
    "    )\n",
    "\n",
    "def normalize_en(s: str) -> str:\n",
    "    s = unicode_to_ascii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def normalize_he(s: str) -> str:\n",
    "    s = s.strip()\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s\n",
    "\n",
    "def read_heb_eng_pairs(path: str, max_lines: int | None = None) -> List[Tuple[str, str]]:\n",
    "    # returns pairs as (en, he) normalized\n",
    "    pairs_local = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if max_lines is not None and i >= max_lines:\n",
    "                break\n",
    "            parts = line.rstrip(\"\\n\").split(\"\\t\")\n",
    "            if len(parts) < 2:\n",
    "                continue\n",
    "            en, he = parts[0], parts[1]\n",
    "            en_n = normalize_en(en)\n",
    "            he_n = normalize_he(he)\n",
    "            if not en_n or not he_n:\n",
    "                continue\n",
    "            pairs_local.append((en_n, he_n))\n",
    "    return pairs_local\n",
    "\n",
    "# -----------------------------\n",
    "# Filter (same eng_prefixes requirement)\n",
    "# -----------------------------\n",
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s \",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    ")\n",
    "\n",
    "def filter_pair(he: str, en: str) -> bool:\n",
    "    # note: en already normalized to lowercase in normalize_en()\n",
    "    return (len(he.split()) < MAX_LENGTH and\n",
    "            len(en.split()) < MAX_LENGTH and\n",
    "            en.startswith(eng_prefixes))\n",
    "\n",
    "def filter_pairs(pairs_he_en: List[Tuple[str, str]]) -> List[Tuple[str, str]]:\n",
    "    return [(he, en) for (he, en) in pairs_he_en if filter_pair(he, en)]\n",
    "\n",
    "# -----------------------------\n",
    "# Load + flip to Hebrew->English + filter\n",
    "# -----------------------------\n",
    "heb_path = \"heb_eng/heb.txt\"\n",
    "pairs_en_he = read_heb_eng_pairs(heb_path)\n",
    "\n",
    "print(\"Total raw pairs:\", len(pairs_en_he))\n",
    "print(\"Raw example (en, he):\", pairs_en_he[0])\n",
    "\n",
    "# Flip: (en, he) -> (he, en)\n",
    "pairs_he_en = [(he, en) for (en, he) in pairs_en_he]\n",
    "\n",
    "pairs_he_en = filter_pairs(pairs_he_en)\n",
    "print(\"Filtered pairs:\", len(pairs_he_en))\n",
    "if len(pairs_he_en) == 0:\n",
    "    raise ValueError(\n",
    "        \"No pairs matched eng_prefixes after filtering. \"\n",
    "        \"This usually means the data does not contain the specified prefixes under MAX_LENGTH.\"\n",
    "    )\n",
    "print(\"Filtered example (he, en):\", pairs_he_en[0])\n",
    "\n",
    "# random.shuffle(pairs_he_en)\n",
    "# pairs_he_en = pairs_he_en[:50000]\n",
    "\n",
    "# -----------------------------\n",
    "# Lang vocab (according tutorial)\n",
    "# -----------------------------\n",
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name: str):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
    "        self.n_words = 2\n",
    "\n",
    "    def add_sentence(self, sentence: str):\n",
    "        for w in sentence.split(\" \"):\n",
    "            self.add_word(w)\n",
    "\n",
    "    def add_word(self, word: str):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "\n",
    "def indexes_from_sentence(lang: Lang, sentence: str):\n",
    "    return [lang.word2index[w] for w in sentence.split(\" \")]\n",
    "\n",
    "def tensor_from_sentence(lang: Lang, sentence: str):\n",
    "    idxs = indexes_from_sentence(lang, sentence)\n",
    "    idxs.append(EOS_token)\n",
    "    return torch.tensor(idxs, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "def tensors_from_pair(pair: Tuple[str, str], input_lang: Lang, output_lang: Lang):\n",
    "    he, en = pair\n",
    "    return tensor_from_sentence(input_lang, he), tensor_from_sentence(output_lang, en)\n",
    "\n",
    "input_lang = Lang(\"heb\")\n",
    "output_lang = Lang(\"eng\")\n",
    "\n",
    "for he, en in pairs_he_en:\n",
    "    input_lang.add_sentence(he)\n",
    "    output_lang.add_sentence(en)\n",
    "\n",
    "print(\"Vocab sizes:\", input_lang.n_words, output_lang.n_words)\n",
    "\n",
    "# -----------------------------\n",
    "# Models (Encoder GRU + Attention Decoder GRU)\n",
    "# -----------------------------\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_size_: int):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size_\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size_)\n",
    "        self.gru = nn.GRU(hidden_size_, hidden_size_)\n",
    "\n",
    "    def forward(self, input_token, hidden):\n",
    "        emb = self.embedding(input_token).view(1, 1, -1)\n",
    "        out, hidden = self.gru(emb, hidden)\n",
    "        return out, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
    "\n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size_: int, output_size: int, dropout_p=0.1):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size_\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size_)\n",
    "        self.attn = nn.Linear(hidden_size_ * 2, MAX_LENGTH)\n",
    "        self.attn_combine = nn.Linear(hidden_size_ * 2, hidden_size_)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.gru = nn.GRU(hidden_size_, hidden_size_)\n",
    "        self.out = nn.Linear(hidden_size_, output_size)\n",
    "\n",
    "    def forward(self, input_token, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input_token).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        attn_weights = torch.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1\n",
    "        )  # (1, MAX_LENGTH)\n",
    "\n",
    "        attn_applied = torch.bmm(\n",
    "            attn_weights.unsqueeze(0),      # (1,1,MAX_LENGTH)\n",
    "            encoder_outputs.unsqueeze(0)    # (1,MAX_LENGTH,H)\n",
    "        )  # (1,1,H)\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)      # (1,2H)\n",
    "        output = self.attn_combine(output).unsqueeze(0)            # (1,1,H)\n",
    "        output = torch.relu(output)\n",
    "\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = torch.log_softmax(self.out(output[0]), dim=1)     # (1,V)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "# -----------------------------\n",
    "# Training helpers\n",
    "# -----------------------------\n",
    "learning_rate = 0.01\n",
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "def train_step(input_tensor, target_tensor, encoder, decoder, enc_opt, dec_opt, criterion):\n",
    "    encoder_hidden = encoder.init_hidden()\n",
    "    enc_opt.zero_grad(set_to_none=True)\n",
    "    dec_opt.zero_grad(set_to_none=True)\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(MAX_LENGTH, hidden_size, device=device)\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        enc_out, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "        if ei < MAX_LENGTH:\n",
    "            encoder_outputs[ei] = enc_out[0, 0]\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    loss = 0.0\n",
    "    use_teacher = random.random() < teacher_forcing_ratio\n",
    "\n",
    "    if use_teacher:\n",
    "        for di in range(target_length):\n",
    "            dec_out, decoder_hidden, _ = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            loss += criterion(dec_out, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]\n",
    "    else:\n",
    "        for di in range(target_length):\n",
    "            dec_out, decoder_hidden, _ = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = dec_out.topk(1)\n",
    "            decoder_input = topi.detach()\n",
    "            loss += criterion(dec_out, target_tensor[di])\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "    enc_opt.step()\n",
    "    dec_opt.step()\n",
    "    return loss.item() / max(target_length, 1)\n",
    "\n",
    "def as_minutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return f\"{m}m {int(s)}s\"\n",
    "\n",
    "def time_since(start, progress):\n",
    "    now = time.time()\n",
    "    s = now - start\n",
    "    es = s / progress if progress > 0 else 0\n",
    "    rs = es - s\n",
    "    return f\"{as_minutes(s)} (- {as_minutes(rs)})\"\n",
    "\n",
    "# -----------------------------\n",
    "# Train\n",
    "# -----------------------------\n",
    "encoder = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "decoder = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n",
    "\n",
    "enc_opt = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "dec_opt = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for ep in range(1, epochs + 1):\n",
    "    random.shuffle(pairs_he_en)\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for pair in pairs_he_en:\n",
    "        inp, tgt = tensors_from_pair(pair, input_lang, output_lang)\n",
    "        total_loss += train_step(inp, tgt, encoder, decoder, enc_opt, dec_opt, criterion)\n",
    "\n",
    "    avg_loss = total_loss / max(len(pairs_he_en), 1)\n",
    "    print(f\"Epoch {ep:02d}/{epochs} | loss={avg_loss:.4f} | {time_since(start, ep/epochs)}\")\n",
    "\n",
    "print(\"Done.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total raw pairs: 136845\n",
      "Raw example (en, he): ('go .', 'לך!')\n",
      "Filtered pairs: 9225\n",
      "Filtered example (he, en): ('אני בסדר.', 'i m ok .')\n",
      "Vocab sizes: 7359 3050\n",
      "Epoch 01/50 | loss=2.8753 | 1m 51s (- 91m 19s)\n",
      "Epoch 02/50 | loss=2.4817 | 3m 42s (- 89m 5s)\n",
      "Epoch 03/50 | loss=2.2554 | 5m 31s (- 86m 37s)\n",
      "Epoch 04/50 | loss=2.0626 | 7m 23s (- 85m 4s)\n",
      "Epoch 05/50 | loss=1.9041 | 9m 21s (- 84m 14s)\n",
      "Epoch 06/50 | loss=1.7618 | 11m 18s (- 82m 52s)\n",
      "Epoch 07/50 | loss=1.6348 | 13m 12s (- 81m 8s)\n",
      "Epoch 08/50 | loss=1.5161 | 15m 7s (- 79m 25s)\n",
      "Epoch 09/50 | loss=1.4053 | 16m 57s (- 77m 13s)\n",
      "Epoch 10/50 | loss=1.3155 | 18m 58s (- 75m 55s)\n",
      "Epoch 11/50 | loss=1.2337 | 20m 54s (- 74m 7s)\n",
      "Epoch 12/50 | loss=1.1753 | 22m 43s (- 71m 58s)\n",
      "Epoch 13/50 | loss=1.1012 | 24m 25s (- 69m 32s)\n",
      "Epoch 14/50 | loss=1.0534 | 26m 14s (- 67m 28s)\n",
      "Epoch 15/50 | loss=1.0193 | 28m 4s (- 65m 30s)\n",
      "Epoch 16/50 | loss=0.9681 | 29m 56s (- 63m 36s)\n",
      "Epoch 17/50 | loss=1.0654 | 31m 43s (- 61m 35s)\n",
      "Epoch 18/50 | loss=1.1829 | 33m 31s (- 59m 35s)\n",
      "Epoch 19/50 | loss=1.0981 | 35m 18s (- 57m 37s)\n",
      "Epoch 20/50 | loss=1.0880 | 37m 4s (- 55m 37s)\n",
      "Epoch 21/50 | loss=1.0441 | 38m 53s (- 53m 42s)\n",
      "Epoch 22/50 | loss=1.1176 | 40m 37s (- 51m 42s)\n",
      "Epoch 23/50 | loss=1.1178 | 42m 28s (- 49m 51s)\n",
      "Epoch 24/50 | loss=1.0696 | 44m 17s (- 47m 58s)\n",
      "Epoch 25/50 | loss=1.0698 | 46m 18s (- 46m 18s)\n",
      "Epoch 26/50 | loss=1.2108 | 48m 20s (- 44m 37s)\n",
      "Epoch 27/50 | loss=1.2562 | 50m 17s (- 42m 50s)\n",
      "Epoch 28/50 | loss=1.6356 | 52m 11s (- 41m 0s)\n",
      "Epoch 29/50 | loss=2.1992 | 54m 3s (- 39m 8s)\n",
      "Epoch 30/50 | loss=2.6474 | 55m 52s (- 37m 14s)\n",
      "Epoch 31/50 | loss=2.5006 | 57m 45s (- 35m 23s)\n",
      "Epoch 32/50 | loss=2.4214 | 59m 37s (- 33m 32s)\n",
      "Epoch 33/50 | loss=2.2804 | 61m 25s (- 31m 38s)\n",
      "Epoch 34/50 | loss=2.1888 | 63m 16s (- 29m 46s)\n",
      "Epoch 35/50 | loss=2.0915 | 65m 7s (- 27m 54s)\n",
      "Epoch 36/50 | loss=2.0317 | 67m 0s (- 26m 3s)\n",
      "Epoch 37/50 | loss=1.9680 | 68m 55s (- 24m 13s)\n",
      "Epoch 38/50 | loss=1.9461 | 70m 51s (- 22m 22s)\n",
      "Epoch 39/50 | loss=1.8671 | 72m 41s (- 20m 30s)\n",
      "Epoch 40/50 | loss=1.8152 | 74m 28s (- 18m 37s)\n",
      "Epoch 41/50 | loss=1.7699 | 76m 9s (- 16m 42s)\n",
      "Epoch 42/50 | loss=1.7101 | 77m 51s (- 14m 49s)\n",
      "Epoch 43/50 | loss=1.6571 | 79m 36s (- 12m 57s)\n",
      "Epoch 44/50 | loss=1.6281 | 81m 17s (- 11m 5s)\n",
      "Epoch 45/50 | loss=1.5804 | 83m 2s (- 9m 13s)\n",
      "Epoch 46/50 | loss=1.5651 | 84m 49s (- 7m 22s)\n",
      "Epoch 47/50 | loss=1.5663 | 86m 30s (- 5m 31s)\n",
      "Epoch 48/50 | loss=1.5118 | 88m 17s (- 3m 40s)\n",
      "Epoch 49/50 | loss=1.4827 | 90m 11s (- 1m 50s)\n",
      "Epoch 50/50 | loss=1.4346 | 92m 2s (- 0m 0s)\n",
      "Done.\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T21:56:22.338945Z",
     "start_time": "2025-12-30T21:56:22.303568Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# =========================\n",
    "# 2.b) Evaluate on 20 random sentences\n",
    "# =========================\n",
    "# - pairs_he_en : list of (he, en) after filter_pairs\n",
    "# - input_lang, output_lang\n",
    "# - encoder, decoder (trained)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_sentence(encoder, decoder, input_lang, output_lang, sentence_he: str, max_length: int = MAX_LENGTH):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      decoded_words: list[str] (predicted English tokens, without EOS)\n",
    "      attn_matrix: torch.Tensor shape (out_len, MAX_LENGTH)  (attention weights per output step)\n",
    "    \"\"\"\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "    # to tensor\n",
    "    input_tensor = tensor_from_sentence(input_lang, sentence_he)  # (L,1)\n",
    "    input_length = input_tensor.size(0)\n",
    "\n",
    "    encoder_hidden = encoder.init_hidden()\n",
    "    encoder_outputs = torch.zeros(MAX_LENGTH, hidden_size, device=device)\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        enc_out, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "        if ei < MAX_LENGTH:\n",
    "            encoder_outputs[ei] = enc_out[0, 0]\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    decoded_words = []\n",
    "    attn_matrix = torch.zeros(MAX_LENGTH, MAX_LENGTH, device=device)  # (max_out, MAX_LENGTH)\n",
    "\n",
    "    for di in range(MAX_LENGTH):\n",
    "        dec_out, decoder_hidden, attn_weights = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "        attn_matrix[di] = attn_weights.squeeze(0)\n",
    "\n",
    "        topv, topi = dec_out.topk(1)\n",
    "        next_idx = topi.item()\n",
    "\n",
    "        if next_idx == EOS_token:\n",
    "            break\n",
    "\n",
    "        decoded_words.append(output_lang.index2word[next_idx])\n",
    "        decoder_input = topi.detach()  # (1,1)\n",
    "\n",
    "    return decoded_words, attn_matrix[: max(len(decoded_words), 1)]  # trim rows\n",
    "\n",
    "\n",
    "def evaluate_random_20(pairs_he_en, encoder, decoder, input_lang, output_lang, n: int = 20, seed: int = 123):\n",
    "    rng = random.Random(seed)\n",
    "    sample = rng.sample(pairs_he_en, k=min(n, len(pairs_he_en)))\n",
    "\n",
    "    correct = 0\n",
    "    print(\"\\n=== Random evaluation (20 sentences) ===\\n\")\n",
    "\n",
    "    for i, (he, en_true) in enumerate(sample, 1):\n",
    "        pred_words, _ = evaluate_sentence(encoder, decoder, input_lang, output_lang, he, MAX_LENGTH)\n",
    "        en_pred = \" \".join(pred_words)\n",
    "\n",
    "        # exact string match (same tokenization as your training data)\n",
    "        is_ok = (en_pred.strip() == en_true.strip())\n",
    "        correct += int(is_ok)\n",
    "\n",
    "        print(f\"[{i:02d}] HE:   {he}\")\n",
    "        print(f\"     TRUE: {en_true}\")\n",
    "        print(f\"     PRED: {en_pred}\")\n",
    "        print(f\"     OK:   {is_ok}\\n\")\n",
    "\n",
    "    acc = correct / len(sample) if sample else 0.0\n",
    "    print(f\"Exact-match on these {len(sample)} samples: {correct}/{len(sample)} = {acc:.3f}\")\n",
    "    return acc\n",
    "\n",
    "\n",
    "# Run 2.b:\n",
    "evaluate_random_20(pairs_he_en, encoder, decoder, input_lang, output_lang, n=20, seed=SEED)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Random evaluation (20 sentences) ===\n",
      "\n",
      "[01] HE:   חזרת שנית.\n",
      "     TRUE: you re back again .\n",
      "     PRED: you are a good .\n",
      "     OK:   False\n",
      "\n",
      "[02] HE:   אני מצפה לקבלת מכתבך.\n",
      "     TRUE: i m looking forward to getting your letter .\n",
      "     PRED: i m impressed with my friends .\n",
      "     OK:   False\n",
      "\n",
      "[03] HE:   אני סומכת על עזרתכם.\n",
      "     TRUE: i m depending on your help .\n",
      "     PRED: i m depending on .\n",
      "     OK:   False\n",
      "\n",
      "[04] HE:   אתה במצב רוח די מרומם היום.\n",
      "     TRUE: you re in quite a mood today .\n",
      "     PRED: you re quite today .\n",
      "     OK:   False\n",
      "\n",
      "[05] HE:   אנחנו לא נשארים כאן.\n",
      "     TRUE: we re not staying here .\n",
      "     PRED: we re not here .\n",
      "     OK:   False\n",
      "\n",
      "[06] HE:   אתם לא מאושרים, נכון?\n",
      "     TRUE: you re not happy are you ?\n",
      "     PRED: you re not aren t you ?\n",
      "     OK:   False\n",
      "\n",
      "[07] HE:   אתה על כל הכביש.\n",
      "     TRUE: you re all over the road .\n",
      "     PRED: you re doing the .\n",
      "     OK:   False\n",
      "\n",
      "[08] HE:   את מקשקשת.\n",
      "     TRUE: you re babbling .\n",
      "     PRED: you re babbling .\n",
      "     OK:   True\n",
      "\n",
      "[09] HE:   הוא לומד ממש עכשיו.\n",
      "     TRUE: he s now studying .\n",
      "     PRED: he s studying now .\n",
      "     OK:   False\n",
      "\n",
      "[10] HE:   אני רק חוזר למקורות.\n",
      "     TRUE: i m just getting back to basics .\n",
      "     PRED: i m just getting just a . . . .\n",
      "     OK:   False\n",
      "\n",
      "[11] HE:   נהיה בסדר.\n",
      "     TRUE: we re going to be fine .\n",
      "     PRED: we re going to be .\n",
      "     OK:   False\n",
      "\n",
      "[12] HE:   אני לא משוחדת.\n",
      "     TRUE: i m unprejudiced .\n",
      "     PRED: i m not .\n",
      "     OK:   False\n",
      "\n",
      "[13] HE:   הם שוטים.\n",
      "     TRUE: they re idiots .\n",
      "     PRED: they re fools .\n",
      "     OK:   False\n",
      "\n",
      "[14] HE:   אני טוב מאד בזה.\n",
      "     TRUE: i m very good at it .\n",
      "     PRED: i m very good at this .\n",
      "     OK:   False\n",
      "\n",
      "[15] HE:   הוא תמיד מחפש שבחים.\n",
      "     TRUE: he is always looking for praise .\n",
      "     PRED: he s looking for a .\n",
      "     OK:   False\n",
      "\n",
      "[16] HE:   אתה לא מעודכן במיוחד.\n",
      "     TRUE: you re not very well informed .\n",
      "     PRED: you re not very good a .\n",
      "     OK:   False\n",
      "\n",
      "[17] HE:   אני עדיין מחכה לתשובה.\n",
      "     TRUE: i m still waiting for a reply .\n",
      "     PRED: i m still waiting for for for for for for\n",
      "     OK:   False\n",
      "\n",
      "[18] HE:   כפות הרגליים שלה פונות פנימה.\n",
      "     TRUE: she is pigeon toed .\n",
      "     PRED: he is an .\n",
      "     OK:   False\n",
      "\n",
      "[19] HE:   אני אמין.\n",
      "     TRUE: i m reliable .\n",
      "     PRED: i m reliable .\n",
      "     OK:   True\n",
      "\n",
      "[20] HE:   אני לא מופתע מזה.\n",
      "     TRUE: i m not that surprised by it .\n",
      "     PRED: i m not able .\n",
      "     OK:   False\n",
      "\n",
      "Exact-match on these 20 samples: 2/20 = 0.100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "(b) The evaluation shows that the model produces exact translations for only a small number of sentences . This suggests that while the model has begun to learn basic source–target correspondences, its overall translation performance is still limited, especially under greedy decoding and a strict exact-match evaluation criterion"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-30T21:56:33.224295Z",
     "start_time": "2025-12-30T21:56:32.620102Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "@torch.no_grad()\n",
    "def show_attention(input_sentence, output_words, attn_matrix):\n",
    "    \"\"\"\n",
    "    input_sentence: str (Hebrew)\n",
    "    output_words: list[str] (predicted English tokens)\n",
    "    attn_matrix: Tensor (out_len, MAX_LENGTH)\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(8, 6))\n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "    attn = attn_matrix[:len(output_words)].cpu().numpy()\n",
    "    cax = ax.matshow(attn, cmap=\"bone\")\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # ticks\n",
    "    ax.set_xticks(range(len(input_sentence.split())))\n",
    "    ax.set_yticks(range(len(output_words)))\n",
    "\n",
    "    ax.set_xticklabels(input_sentence.split(), rotation=90)\n",
    "    ax.set_yticklabels(output_words)\n",
    "\n",
    "    ax.set_xlabel(\"Input (Hebrew)\")\n",
    "    ax.set_ylabel(\"Output (English)\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def attention_random_5(pairs_he_en, encoder, decoder, input_lang, output_lang, seed=123):\n",
    "    rng = random.Random(seed)\n",
    "    sample = rng.sample(pairs_he_en, k=5)\n",
    "\n",
    "    for i, (he, en_true) in enumerate(sample, 1):\n",
    "        pred_words, attn = evaluate_sentence(\n",
    "            encoder, decoder, input_lang, output_lang, he, MAX_LENGTH\n",
    "        )\n",
    "\n",
    "        print(f\"\\n[{i}]\")\n",
    "        print(\"HE:   \", he)\n",
    "        print(\"TRUE: \", en_true)\n",
    "        print(\"PRED: \", \" \".join(pred_words))\n",
    "\n",
    "        show_attention(he, pred_words, attn)\n",
    "\n",
    "\n",
    "# Run 2.c:\n",
    "attention_random_5(pairs_he_en, encoder, decoder, input_lang, output_lang)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1]\n",
      "HE:    חזרת שנית.\n",
      "TRUE:  you re back again .\n",
      "PRED:  you are a good .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[2]\n",
      "HE:    אני מצפה לקבלת מכתבך.\n",
      "TRUE:  i m looking forward to getting your letter .\n",
      "PRED:  i m impressed with my friends .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[3]\n",
      "HE:    אני סומכת על עזרתכם.\n",
      "TRUE:  i m depending on your help .\n",
      "PRED:  i m depending on .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[4]\n",
      "HE:    אתה במצב רוח די מרומם היום.\n",
      "TRUE:  you re in quite a mood today .\n",
      "PRED:  you re quite today .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[5]\n",
      "HE:    אנחנו לא נשארים כאן.\n",
      "TRUE:  we re not staying here .\n",
      "PRED:  we re not here .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "(c) For five randomly selected examples, we visualized the attention weights as a heatmap over source (Hebrew) and target (English) tokens. Each row corresponds to a decoded English token and each column corresponds to an input Hebrew token; darker cells indicate higher attention weight. The plots provide an interpretable alignment showing which Hebrew tokens the decoder relied on when generating each English token, which is expected behavior for an attention-based seq2seq model."
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-31T02:07:09.075703Z",
     "start_time": "2025-12-30T21:57:47.443874Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# -----------------------------\n",
    "# Reproducibility\n",
    "# -----------------------------\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)\n",
    "\n",
    "# -----------------------------\n",
    "# Data download\n",
    "# -----------------------------\n",
    "\n",
    "# IMPORTANT: this matches your unzip command: `-d heb_eng`\n",
    "if not os.path.exists(\"heb_eng/heb.txt\"):\n",
    "    print(\"Downloading dataset...\")\n",
    "    os.system(\"curl -L -o heb-eng.zip http://www.manythings.org/anki/heb-eng.zip\")\n",
    "    os.system(\"unzip -o -q heb-eng.zip -d heb_eng\")\n",
    "\n",
    "heb_path = \"heb_eng/heb.txt\"\n",
    "assert os.path.exists(heb_path), f\"File not found: {heb_path}\"\n",
    "\n",
    "# -----------------------------\n",
    "# Normalization\n",
    "# -----------------------------\n",
    "\n",
    "def unicode_to_ascii(s: str) -> str:\n",
    "    # keep English side consistent with the tutorial\n",
    "    return \"\".join(\n",
    "        c for c in unicodedata.normalize(\"NFD\", s)\n",
    "        if unicodedata.category(c) != \"Mn\"\n",
    "    )\n",
    "\n",
    "\n",
    "def normalize_en(s: str) -> str:\n",
    "    s = unicode_to_ascii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\\\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    s = re.sub(r\"\\\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "\n",
    "def normalize_he(s: str) -> str:\n",
    "\n",
    "    s = s.strip()\n",
    "    s = re.sub(r\"\\\\s+\", \" \", s)\n",
    "    return s\n",
    "\n",
    "\n",
    "def read_heb_eng_pairs(path: str, max_lines: int | None = None) -> List[Tuple[str, str]]:\n",
    "    # file format: en \\t he\n",
    "    pairs_local: List[Tuple[str, str]] = []\n",
    "    with open(path, \"r\", encoding=\"utf-8-sig\", errors=\"replace\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if max_lines is not None and i >= max_lines:\n",
    "                break\n",
    "            parts = line.rstrip(\"\\n\").split(\"\\t\")\n",
    "            if len(parts) < 2:\n",
    "                continue\n",
    "            en_raw, he_raw = parts[0], parts[1]\n",
    "            en = normalize_en(en_raw)\n",
    "            he = normalize_he(he_raw)\n",
    "            if en and he:\n",
    "                pairs_local.append((he, en))  # Hebrew -> English\n",
    "    return pairs_local\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Filter (eng_prefixes requirement)\n",
    "# -----------------------------\n",
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is \", \"he s \",\n",
    "    \"she is \", \"she s \",\n",
    "    \"you are \", \"you re \",\n",
    "    \"we are \", \"we re \",\n",
    "    \"they are \", \"they re \",\n",
    ")\n",
    "\n",
    "\n",
    "def filter_pair(he: str, en: str) -> bool:\n",
    "    return (\n",
    "        len(he.split()) < MAX_LENGTH\n",
    "        and len(en.split()) < MAX_LENGTH\n",
    "        and en.startswith(eng_prefixes)\n",
    "    )\n",
    "\n",
    "\n",
    "def filter_pairs(pairs: List[Tuple[str, str]]) -> List[Tuple[str, str]]:\n",
    "    return [(he, en) for (he, en) in pairs if filter_pair(he, en)]\n",
    "\n",
    "\n",
    "pairs_he_en = read_heb_eng_pairs(heb_path)\n",
    "print(\"Total raw pairs:\", len(pairs_he_en))\n",
    "print(\"Raw example (he, en):\", pairs_he_en[0])\n",
    "\n",
    "pairs_he_en = filter_pairs(pairs_he_en)\n",
    "print(\"Filtered pairs:\", len(pairs_he_en))\n",
    "print(\"Filtered example (he, en):\", pairs_he_en[0])\n",
    "\n",
    "if len(pairs_he_en) == 0:\n",
    "    raise ValueError(\n",
    "        \"No pairs matched eng_prefixes after filtering. \"\n",
    "        \"Try increasing MAX_LENGTH or removing eng_prefixes filter to verify data loading.\"\n",
    "    )\n",
    "\n",
    "# random.shuffle(pairs_he_en)\n",
    "# pairs_he_en = pairs_he_en[:50000]\n",
    "\n",
    "# -----------------------------\n",
    "# Vocabulary\n",
    "# -----------------------------\n",
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name: str):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
    "        self.n_words = 2\n",
    "\n",
    "    def add_sentence(self, sentence: str):\n",
    "        for w in sentence.split(\" \"):\n",
    "            self.add_word(w)\n",
    "\n",
    "    def add_word(self, word: str):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "\n",
    "\n",
    "def indexes_from_sentence(lang: Lang, sentence: str):\n",
    "    return [lang.word2index[w] for w in sentence.split(\" \")]\n",
    "\n",
    "\n",
    "def tensor_from_sentence(lang: Lang, sentence: str):\n",
    "    idxs = indexes_from_sentence(lang, sentence)\n",
    "    idxs.append(EOS_token)\n",
    "    return torch.tensor(idxs, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "\n",
    "def tensors_from_pair(pair: Tuple[str, str], input_lang: Lang, output_lang: Lang):\n",
    "    he, en = pair\n",
    "    return tensor_from_sentence(input_lang, he), tensor_from_sentence(output_lang, en)\n",
    "\n",
    "\n",
    "input_lang = Lang(\"heb\")\n",
    "output_lang = Lang(\"eng\")\n",
    "for he, en in pairs_he_en:\n",
    "    input_lang.add_sentence(he)\n",
    "    output_lang.add_sentence(en)\n",
    "\n",
    "print(\"Vocab sizes:\", input_lang.n_words, output_lang.n_words)\n",
    "\n",
    "# -----------------------------\n",
    "# Models (GRU encoder + Attention decoder)\n",
    "# -----------------------------\n",
    "\n",
    "\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_size_: int):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size_\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size_)\n",
    "        self.gru = nn.GRU(hidden_size_, hidden_size_)\n",
    "\n",
    "    def forward(self, input_token, hidden):\n",
    "        emb = self.embedding(input_token).view(1, 1, -1)\n",
    "        out, hidden = self.gru(emb, hidden)\n",
    "        return out, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
    "\n",
    "\n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size_: int, output_size: int, dropout_p=0.1):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size_\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size_)\n",
    "        self.attn = nn.Linear(hidden_size_ * 2, MAX_LENGTH)\n",
    "        self.attn_combine = nn.Linear(hidden_size_ * 2, hidden_size_)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.gru = nn.GRU(hidden_size_, hidden_size_)\n",
    "        self.out = nn.Linear(hidden_size_, output_size)\n",
    "\n",
    "    def forward(self, input_token, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input_token).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        attn_weights = torch.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1\n",
    "        )  # (1, MAX_LENGTH)\n",
    "\n",
    "        attn_applied = torch.bmm(\n",
    "            attn_weights.unsqueeze(0),\n",
    "            encoder_outputs.unsqueeze(0),\n",
    "        )  # (1, 1, H)\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)      # (1, 2H)\n",
    "        output = self.attn_combine(output).unsqueeze(0)            # (1, 1, H)\n",
    "        output = torch.relu(output)\n",
    "\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = torch.log_softmax(self.out(output[0]), dim=1)     # (1, V)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Training helpers\n",
    "# -----------------------------\n",
    "learning_rate = 0.001          # more stable than 0.01 for this setup\n",
    "teacher_forcing_ratio = 0.7    # higher helps early convergence\n",
    "clip = 1.0                     # gradient clipping\n",
    "\n",
    "\n",
    "def train_step(input_tensor, target_tensor, encoder, decoder, enc_opt, dec_opt, criterion):\n",
    "    encoder_hidden = encoder.init_hidden()\n",
    "    enc_opt.zero_grad(set_to_none=True)\n",
    "    dec_opt.zero_grad(set_to_none=True)\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(MAX_LENGTH, hidden_size, device=device)\n",
    "\n",
    "    # Encode\n",
    "    for ei in range(min(input_length, MAX_LENGTH)):\n",
    "        enc_out, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = enc_out[0, 0]\n",
    "\n",
    "    # Decode\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    loss = 0.0\n",
    "    use_teacher = random.random() < teacher_forcing_ratio\n",
    "\n",
    "    if use_teacher:\n",
    "        for di in range(target_length):\n",
    "            dec_out, decoder_hidden, _ = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            loss += criterion(dec_out, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]\n",
    "    else:\n",
    "        for di in range(target_length):\n",
    "            dec_out, decoder_hidden, _ = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = dec_out.topk(1)\n",
    "            decoder_input = topi.detach()\n",
    "            loss += criterion(dec_out, target_tensor[di])\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    # Clip gradients (stability)\n",
    "    torch.nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n",
    "    torch.nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n",
    "\n",
    "    enc_opt.step()\n",
    "    dec_opt.step()\n",
    "\n",
    "    return loss.item() / max(target_length, 1)\n",
    "\n",
    "\n",
    "def as_minutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return f\"{m}m {int(s)}s\"\n",
    "\n",
    "\n",
    "def time_since(start, progress):\n",
    "    now = time.time()\n",
    "    s = now - start\n",
    "    es = s / progress if progress > 0 else 0\n",
    "    rs = es - s\n",
    "    return f\"{as_minutes(s)} (- {as_minutes(rs)})\"\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Train\n",
    "# -----------------------------\n",
    "encoder = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "decoder = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n",
    "\n",
    "# Adam generally converges much faster/more stably than SGD here\n",
    "enc_opt = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "dec_opt = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for ep in range(1, epochs + 1):\n",
    "    random.shuffle(pairs_he_en)\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for pair in pairs_he_en:\n",
    "        inp, tgt = tensors_from_pair(pair, input_lang, output_lang)\n",
    "        total_loss += train_step(inp, tgt, encoder, decoder, enc_opt, dec_opt, criterion)\n",
    "\n",
    "    avg_loss = total_loss / max(len(pairs_he_en), 1)\n",
    "    print(f\"Epoch {ep:02d}/{epochs} | loss={avg_loss:.4f} | {time_since(start, ep/epochs)}\")\n",
    "\n",
    "print(\"Done.\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cpu\n",
      "Total raw pairs: 136845\n",
      "Raw example (he, en): ('לך!', 'go')\n",
      "Filtered pairs: 9615\n",
      "Filtered example (he, en): ('אני בסדר.', 'i m ok')\n",
      "Vocab sizes: 7769 3184\n",
      "Epoch 01/50 | loss=2.9631 | 2m 51s (- 140m 25s)\n",
      "Epoch 02/50 | loss=2.3815 | 5m 47s (- 138m 52s)\n",
      "Epoch 03/50 | loss=2.0432 | 8m 38s (- 135m 23s)\n",
      "Epoch 04/50 | loss=1.7775 | 11m 28s (- 132m 0s)\n",
      "Epoch 05/50 | loss=1.5751 | 14m 18s (- 128m 48s)\n",
      "Epoch 06/50 | loss=1.4014 | 17m 8s (- 125m 43s)\n",
      "Epoch 07/50 | loss=1.2593 | 19m 56s (- 122m 31s)\n",
      "Epoch 08/50 | loss=1.1527 | 22m 42s (- 119m 13s)\n",
      "Epoch 09/50 | loss=1.0550 | 25m 25s (- 115m 51s)\n",
      "Epoch 10/50 | loss=0.9793 | 28m 11s (- 112m 44s)\n",
      "Epoch 11/50 | loss=0.9223 | 30m 59s (- 109m 52s)\n",
      "Epoch 12/50 | loss=0.8579 | 33m 51s (- 107m 11s)\n",
      "Epoch 13/50 | loss=0.8245 | 36m 36s (- 104m 12s)\n",
      "Epoch 14/50 | loss=0.7907 | 39m 21s (- 101m 11s)\n",
      "Epoch 15/50 | loss=0.7582 | 42m 6s (- 98m 16s)\n",
      "Epoch 16/50 | loss=0.7317 | 44m 58s (- 95m 33s)\n",
      "Epoch 17/50 | loss=0.7043 | 47m 46s (- 92m 44s)\n",
      "Epoch 18/50 | loss=0.6899 | 50m 37s (- 90m 0s)\n",
      "Epoch 19/50 | loss=0.6712 | 53m 29s (- 87m 17s)\n",
      "Epoch 20/50 | loss=0.6772 | 56m 15s (- 84m 23s)\n",
      "Epoch 21/50 | loss=0.6574 | 59m 1s (- 81m 31s)\n",
      "Epoch 22/50 | loss=0.6439 | 61m 46s (- 78m 37s)\n",
      "Epoch 23/50 | loss=0.6446 | 64m 30s (- 75m 44s)\n",
      "Epoch 24/50 | loss=0.6295 | 67m 6s (- 72m 41s)\n",
      "Epoch 25/50 | loss=0.6199 | 69m 38s (- 69m 38s)\n",
      "Epoch 26/50 | loss=0.6166 | 72m 20s (- 66m 46s)\n",
      "Epoch 27/50 | loss=0.6249 | 74m 54s (- 63m 48s)\n",
      "Epoch 28/50 | loss=0.6176 | 77m 27s (- 60m 51s)\n",
      "Epoch 29/50 | loss=0.6197 | 79m 59s (- 57m 55s)\n",
      "Epoch 30/50 | loss=0.6111 | 88m 4s (- 58m 43s)\n",
      "Epoch 31/50 | loss=0.6109 | 105m 45s (- 64m 49s)\n",
      "Epoch 32/50 | loss=0.5887 | 123m 24s (- 69m 25s)\n",
      "Epoch 33/50 | loss=0.6062 | 141m 3s (- 72m 40s)\n",
      "Epoch 34/50 | loss=0.5943 | 158m 44s (- 74m 41s)\n",
      "Epoch 35/50 | loss=0.5834 | 171m 23s (- 73m 27s)\n",
      "Epoch 36/50 | loss=0.5909 | 178m 54s (- 69m 34s)\n",
      "Epoch 37/50 | loss=0.5818 | 196m 35s (- 69m 4s)\n",
      "Epoch 38/50 | loss=0.5853 | 204m 7s (- 64m 27s)\n",
      "Epoch 39/50 | loss=0.5841 | 221m 46s (- 62m 33s)\n",
      "Epoch 40/50 | loss=0.5870 | 224m 14s (- 56m 3s)\n",
      "Epoch 41/50 | loss=0.5861 | 226m 43s (- 49m 46s)\n",
      "Epoch 42/50 | loss=0.5843 | 229m 10s (- 43m 39s)\n",
      "Epoch 43/50 | loss=0.5686 | 231m 37s (- 37m 42s)\n",
      "Epoch 44/50 | loss=0.5874 | 234m 5s (- 31m 55s)\n",
      "Epoch 45/50 | loss=0.5732 | 236m 35s (- 26m 17s)\n",
      "Epoch 46/50 | loss=0.5879 | 239m 6s (- 20m 47s)\n",
      "Epoch 47/50 | loss=0.5736 | 241m 38s (- 15m 25s)\n",
      "Epoch 48/50 | loss=0.5822 | 244m 12s (- 10m 10s)\n",
      "Epoch 49/50 | loss=0.5731 | 246m 46s (- 5m 2s)\n",
      "Epoch 50/50 | loss=0.5783 | 249m 20s (- 0m 0s)\n",
      "Done.\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Model Improvement Summary (Compared to 2.a)\n",
    "\n",
    "The improved model demonstrates better performance than the baseline in 2.a, as evidenced by faster and more stable convergence and a substantially lower final training loss. The loss decreases smoothly from 2.96 to approximately 0.5783 and stabilizes in later epochs, indicating effective optimization and stable learning dynamics.\n",
    "\n",
    "This improvement is primarily due to changes in the training strategy rather than the model architecture. Specifically, replacing SGD with the Adam optimizer (learning rate 0.001) enables adaptive parameter updates and better handling of noisy gradients typical in seq2seq models with attention. In addition, gradient clipping and a higher teacher forcing ratio further stabilize training and accelerate convergence.\n",
    "\n",
    "Overall, the same GRU-based encoder–decoder with additive attention is trained more effectively, leading to improved optimization behavior compared to the baseline model."
   ]
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-12-31T09:40:03.154380Z"
    }
   },
   "cell_type": "code",
   "source": "!jupyter nbconvert --to html --execute --output PS3_Attention_Please_2025_ID_314992595.html PS3_Attention_Please_2025_ID_314992595.ipynb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook PS3_Attention_Please_2025_ID_314992595.ipynb to html\r\n"
     ]
    }
   ],
   "execution_count": null
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
