{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n3aXk1o_qRLu"
   },
   "source": [
    "# PyTorch - the Basics!\n",
    "\n",
    "Advanced Learning 2025\n",
    "\n",
    "Ibrahim Bashir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O88JcDDEeVKl"
   },
   "source": [
    "For SUBMISSION:\n",
    "~~~\n",
    "STUDENT ID: 314992595\n",
    "~~~\n",
    "~~~\n",
    "STUDENT GIT LINK: https://github.com/netanelazran11\n",
    "~~~\n",
    "In Addition, don't forget to add your ID to the files:\n",
    "`PS0_PyTorch_basics_2025_ID_[000000000].ipynb`  \n",
    "`PS0_PyTorch_basics_2025_ID_[000000000].html`  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fCByaAJ1q-Gc"
   },
   "source": [
    "## 1. What is PyTorch?\n",
    "\n",
    "\n",
    "PyTorch is a popular open-source library for machine learning, particularly well-suited for deep learning applications.    \n",
    "Here's a breakdown of its key features:\n",
    "\n",
    "    \n",
    "\n",
    "*   **Deep Learning Framework:**  PyTorch provides tools and functionalities to  build and train complex neural networks.\n",
    "*   **Pythonic Interface:**  Known for its Python-like syntax, PyTorch is considered user-friendly and easy to learn.\n",
    "* **Flexibility:**  PyTorch offers both dynamic computational graphs (eager execution) and static graphs (graph mode) for model development.\n",
    "* **Production Ready:**  PyTorch provides features like TorchScript to transition models from development to production seamlessly.\n",
    "* **Scalability:**  PyTorch supports distributed training, enabling you to leverage multiple CPUs or GPUs to train models faster.\n",
    "* **Rich Ecosystem:**  A growing ecosystem of libraries and tools built on PyTorch expands its capabilities for tasks like computer vision, natural language processing, and model interpretability.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y0NFG56arhsD"
   },
   "source": [
    "We start by importing PyTorch's main objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "phCzuWkUqPRk"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jL-emQP0xtkr"
   },
   "source": [
    "### PyTorch Main Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AKs60PcGucH7"
   },
   "source": [
    "`torch.nn`:\n",
    "\n",
    "In PyTorch, the `torch.nn` module provides the essential building blocks you need to construct and train neural networks. It offers a comprehensive collection of classes and functions that streamline the deep learning development process.  \n",
    "  \n",
    "*Key Components:*  \n",
    "\n",
    "**Modules**:   \n",
    "These are the fundamental units that perform specific operations on data. They can be combined to create complex neural network architectures. Common examples include:   \n",
    "\n",
    "* Linear Layers (nn.Linear): Apply linear transformations (y = xA^T + b) for feeding data forward through the network.\n",
    "* Convolutional Layers (nn.Conv2d): Perform convolutions, especially useful for processing image data.\n",
    "* Activation Layers (nn.ReLU, nn.Sigmoid): Introduce non-linearity into the network, allowing it to learn complex patterns.\n",
    "* Normalization Layers (nn.BatchNorm2d): Normalize inputs to layers for faster and more stable training.\n",
    "* Recurrent Layers (nn.LSTM, nn.GRU): Handle sequential data like text or time series.\n",
    "* Dropout Layers (nn.Dropout): Introduce randomness by randomly dropping out neurons during training to prevent overfitting.\n",
    "* Many More: PyTorch offers a vast selection of modules catering to diverse neural network architectures.\n",
    "\n",
    "**Containers**:   \n",
    "These classes help you organize and structure your modules into hierarchical networks. They include:\n",
    "* nn.Sequential: Stacks modules in a linear sequence, making it easy to define simple neural networks.\n",
    "* nn.ModuleList: Holds other modules in a list, allowing for more flexible network structures.\n",
    "* nn.ModuleDict: Manages sub-modules with dictionary-like access for complex topologies.\n",
    "\n",
    "**Loss Functions:**   \n",
    "Functions that measure the error between the network's predictions and the ground truth labels. These guide the training process by calculating the gradients used to update the network's weights. Common examples include:\n",
    "* nn.CrossEntropyLoss: For multi-class classification problems.\n",
    "* nn.MSELoss: For mean squared error calculations in regression tasks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Tw5uR0na0s8J"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YzHi6U5zvteu"
   },
   "source": [
    "`torch.utils.data.DataLoader`:\n",
    "\n",
    "In PyTorch, `torch.utils.data.DataLoader` is a powerful tool that simplifies how you load and manage data for training your deep learning models. It acts as an iterator, efficiently providing batches of data during the training process.\n",
    "\n",
    "Here's a breakdown of its key functionalities:\n",
    "\n",
    "Data Management Abstraction:\n",
    "\n",
    "    Decouples data loading logic from your model training code, promoting cleaner and more maintainable code.\n",
    "    Handles complexities like batching, shuffling, and multi-processing data loading, freeing you from writing repetitive code.\n",
    "\n",
    "Efficient Batching:\n",
    "\n",
    "    Groups data samples (images, text, etc.) into batches of a specified size (batch_size). Batching improves computational efficiency by utilizing vectorized operations on GPUs.\n",
    "    Provides an optional collate_fn argument that allows you to customize how samples within a batch are combined. This can be useful for tasks like padding sequences to have the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "sbHK6GkR0vIH",
    "outputId": "562f9bf5-8bf6-4c65-9810-7a01b925ed8d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.7.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sut-qO6F0x1X"
   },
   "source": [
    "In our course we use PyTorch version >=2.8.0.   \n",
    "`cu126` refers to CUDA 12.6, a software layer that gives direct access to the GPU's virtual instruction set and parallel computational elements for the execution of compute kernels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HE9DKe-9s2aH"
   },
   "source": [
    "## 2. Tensors\n",
    "\n",
    "In PyTorch, tensors are the fundamental data structure. They are similar to NumPy arrays but with some key advantages for deep learning:\n",
    "\n",
    "* **Multi-dimensional Arrays**: Like NumPy arrays, tensors can have multiple dimensions, making them suitable for representing data like images (2D), videos (3D), or sequences of text (1D).\n",
    "\n",
    "* **Hardware Acceleration**: PyTorch tensors can be moved to and run on GPUs or other hardware accelerators, significantly speeding up computations compared to CPUs for deep learning tasks.\n",
    "\n",
    "* **Automatic Differentiation**:  A core feature in deep learning, automatic differentiation allows PyTorch to calculate gradients efficiently, which is essential for training neural networks.  Regular NumPy arrays don't inherently support this.\n",
    "\n",
    "* **Rich Functionality**: PyTorch offers a variety of operations specifically designed for tensors, making it convenient to manipulate and analyze data for deep learning models.\n",
    "\n",
    "In essence, tensors in PyTorch act as the workhorses for your deep learning models. They store and process the data that gets fed into your network, undergoes computations, and ultimately leads to predictions or outputs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PrthmFbIfsXR"
   },
   "source": [
    "**PyTorch tensor operations:**\n",
    "\n",
    "PyTorch tensor operations are the fundamental building blocks for working with data in your deep learning models. These operations allow you to manipulate, analyze, and transform tensors in various ways. Here's a breakdown of some common categories:\n",
    "\n",
    "1. Arithmetic Operations:\n",
    "\n",
    "These operations perform element-wise calculations between tensors or a tensor and a scalar value. They include:\n",
    "\n",
    "    Addition (+)\n",
    "    Subtraction (-)\n",
    "    Multiplication (*)\n",
    "    Division (/)\n",
    "    Exponentiation (**)\n",
    "\n",
    "These operations can be used for simple calculations or combined to create more complex expressions.\n",
    "\n",
    "2. Comparison Operations:\n",
    "\n",
    "These operations compare elements between tensors or a tensor and a scalar value, resulting in a tensor of booleans (True or False) indicating the comparison outcome. Examples include:\n",
    "\n",
    "    Equal (==)\n",
    "    Not equal (!=)\n",
    "    Greater than (>)\n",
    "    Less than (<)\n",
    "    Greater than or equal (>=)\n",
    "    Less than or equal (<=)\n",
    "\n",
    "Comparison operations are useful for filtering data or making decisions within your model.\n",
    "\n",
    "3. Broadcasting:\n",
    "\n",
    "A powerful feature in PyTorch, broadcasting allows operations between tensors of different shapes as long as they are compatible. For instance, you can add a scalar value to a tensor, or add a one-dimensional tensor to a two-dimensional tensor (as long as the dimensions match). PyTorch automatically expands the smaller tensor to match the larger one for element-wise operations.\n",
    "\n",
    "4. In-place Operations:\n",
    "\n",
    "Certain operations modify the original tensor they are applied to, denoted by a trailing underscore (_). Examples include:\n",
    "\n",
    "    x.add_(y) (equivalent to x = x + y)\n",
    "    x.sub_(y) (equivalent to x = x - y)\n",
    "    x.mul_(y) (equivalent to x = x * y)\n",
    "\n",
    "These operations can be memory-efficient when modifying existing tensors is desired.\n",
    "\n",
    "5. Linear Algebra Operations:\n",
    "\n",
    "PyTorch provides functions for common linear algebra operations on tensors, including:\n",
    "\n",
    "    torch.matmul(a, b): Matrix multiplication between tensors a and b.\n",
    "    torch.sum(input, dim=None): Sums the elements of a tensor along a specified dimension.\n",
    "    torch.mean(input, dim=None): Computes the mean of the elements of a tensor along a specified dimension.\n",
    "\n",
    "These operations are essential for various deep learning tasks like calculating activation outputs or loss functions.\n",
    "\n",
    "6. Tensor Reshaping and Indexing:\n",
    "\n",
    "PyTorch offers functionalities to manipulate the shape and access specific elements of tensors:\n",
    "\n",
    "    x.view(new_shape): Reshapes the tensor x into a new shape while keeping the total number of elements the same.\n",
    "    x[index]: Accesses specific elements or sub-tensors using indexing syntax (similar to NumPy).\n",
    "\n",
    "Reshaping and indexing are crucial for preparing data for specific layers in your neural network architecture.\n",
    "\n",
    "7. Element-wise Operations:\n",
    "\n",
    "These operations apply a function to each element of a tensor independently. PyTorch provides a rich set of element-wise functions like:\n",
    "\n",
    "    torch.relu(x): Applies the rectified linear unit (ReLU) activation function.\n",
    "    torch.sigmoid(x): Applies the sigmoid activation function.\n",
    "    torch.tanh(x): Applies the hyperbolic tangent (tanh) activation function.\n",
    "\n",
    "Element-wise operations are fundamental for introducing non-linearity and transforming data in deep learning models.\n",
    "\n",
    "8. Random Operations:\n",
    "\n",
    "PyTorch offers functions for generating random tensors or modifying existing ones with randomness:\n",
    "\n",
    "    torch.rand(shape): Generates a random tensor filled with uniformly distributed values between 0 and 1.\n",
    "    torch.randn(shape): Generates a random tensor filled with values from a standard normal distribution.\n",
    "\n",
    "These operations are useful for data augmentation techniques or initializing weights in your network.\n",
    "\n",
    "By understanding and effectively using these PyTorch tensor operations, you can build and manipulate your deep learning models with greater flexibility and control."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H9nYSKhmoAQy"
   },
   "source": [
    "Some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DMm7DspXrWpj",
    "outputId": "2cd2cba7-f0b4-4066-8431-065e23f37477"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tens: tensor([[1, 2, 3, 4],\n",
      "        [5, 6, 7, 8]])\n",
      "Shape of tensor: torch.Size([2, 4])\n",
      "Datatype of tensor: torch.int64\n",
      "Device tensor is stored on: cpu\n"
     ]
    }
   ],
   "source": [
    "lst_of_lsts = [[1, 2, 3, 4],[5, 6, 7, 8]]\n",
    "tens = torch.tensor(lst_of_lsts)\n",
    "print(f\"tens: {tens}\")\n",
    "print(f\"Shape of tensor: {tens.shape}\")\n",
    "print(f\"Datatype of tensor: {tens.dtype}\")\n",
    "print(f\"Device tensor is stored on: {tens.device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cLgpeDPeoDpG",
    "outputId": "ea153c08-f56a-4d8f-bf99-972c677fa4ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Tensor: \n",
      " tensor([[[0.2634, 0.1835, 0.0945],\n",
      "         [0.1841, 0.0031, 0.9912]],\n",
      "\n",
      "        [[0.4545, 0.1200, 0.2069],\n",
      "         [0.4867, 0.4730, 0.2539]]]) \n",
      "\n",
      "Ones Tensor: \n",
      " tensor([[[1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.]]]) \n",
      "\n",
      "Zeros Tensor: \n",
      " tensor([[[0., 0., 0.],\n",
      "         [0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.],\n",
      "         [0., 0., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "shape = (2,2,3)\n",
    "rand_tensor = torch.rand(shape)\n",
    "ones_tensor = torch.ones(shape)\n",
    "zeros_tensor = torch.zeros(shape)\n",
    "\n",
    "print(f\"Random Tensor: \\n {rand_tensor} \\n\")\n",
    "print(f\"Ones Tensor: \\n {ones_tensor} \\n\")\n",
    "print(f\"Zeros Tensor: \\n {zeros_tensor}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5ADu57CsoykL",
    "outputId": "9bd7efee-bc5c-411f-d8b1-2e6e80bf5ee9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t4.shape: torch.Size([300, 3])\n",
      "t5.shape: torch.Size([100, 9])\n"
     ]
    }
   ],
   "source": [
    "shpe = (100,3)\n",
    "t1 = torch.rand(shpe)\n",
    "t2 = torch.rand(shpe)\n",
    "t3 = torch.rand(shpe)\n",
    "t4=torch.cat([t1,t2,t3],dim=0)\n",
    "print(f\"t4.shape: {t4.shape}\")\n",
    "t5=torch.cat([t1,t2,t3],dim=1)\n",
    "print(f\"t5.shape: {t5.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NwV4Fnweo1OT",
    "outputId": "9c222254-3b94-4e6d-ac99-9f36505271d8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 8.2894, -1.1621, -0.0932,  ...,  1.3159,  0.5383,  0.5684],\n",
       "        [-1.1621,  8.1911,  0.3559,  ..., -1.0021,  0.9418, -2.0080],\n",
       "        [-0.0932,  0.3559,  6.9770,  ...,  1.7100,  0.1715,  0.9191],\n",
       "        ...,\n",
       "        [ 1.3159, -1.0021,  1.7100,  ...,  7.3448, -0.3688, -0.4370],\n",
       "        [ 0.5383,  0.9418,  0.1715,  ..., -0.3688,  9.0259, -0.1577],\n",
       "        [ 0.5684, -2.0080,  0.9191,  ..., -0.4370, -0.1577,  9.2777]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.rand(100,100)\n",
    "X_cent = X - X.mean(dim=1, keepdim=True)\n",
    "covX = X_cent.T@X_cent\n",
    "covX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2qKr9lixtGgO"
   },
   "source": [
    "## Data & Handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9TJNXJdfuHxH"
   },
   "source": [
    "### PyTorch Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YZEnT7gHzWUp"
   },
   "source": [
    "Deep networks are versatile tools that can be adapted to various data types by\n",
    "leveraging appropriate pre-processing techniques and network architectures.\n",
    "PyTorch, like other deep learning libraries, can handle a wide array of data:\n",
    "  \n",
    "\n",
    "\n",
    "* images\n",
    "* audio\n",
    "* text data\n",
    "* tabluar (numerical, categorical, mixed)\n",
    "* multimodal Data\n",
    "* other\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GbZ5lmtzuKlb"
   },
   "source": [
    "PyTorch also offers built-in vision specific datasets as part of the `torchvision.datasets` [module](https://pytorch.org/vision/stable/datasets.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "9kGSr-D-y42B"
   },
   "outputs": [],
   "source": [
    "from torchvision import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ML5of6dVzEAY",
    "outputId": "9caf1a2b-1720-4e08-daf1-83527df63278"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CIFAR10',\n",
       " 'CIFAR100',\n",
       " 'CLEVRClassification',\n",
       " 'CREStereo',\n",
       " 'Caltech101',\n",
       " 'Caltech256',\n",
       " 'CarlaStereo',\n",
       " 'CelebA',\n",
       " 'Cityscapes',\n",
       " 'CocoCaptions',\n",
       " 'CocoDetection',\n",
       " 'Country211',\n",
       " 'DTD',\n",
       " 'DatasetFolder',\n",
       " 'EMNIST',\n",
       " 'ETH3DStereo',\n",
       " 'EuroSAT',\n",
       " 'FER2013',\n",
       " 'FGVCAircraft',\n",
       " 'FakeData',\n",
       " 'FallingThingsStereo',\n",
       " 'FashionMNIST',\n",
       " 'Flickr30k',\n",
       " 'Flickr8k',\n",
       " 'Flowers102',\n",
       " 'FlyingChairs',\n",
       " 'FlyingThings3D',\n",
       " 'Food101',\n",
       " 'GTSRB',\n",
       " 'HD1K',\n",
       " 'HMDB51',\n",
       " 'INaturalist',\n",
       " 'ImageFolder',\n",
       " 'ImageNet',\n",
       " 'Imagenette',\n",
       " 'InStereo2k',\n",
       " 'KMNIST',\n",
       " 'Kinetics',\n",
       " 'Kitti',\n",
       " 'Kitti2012Stereo',\n",
       " 'Kitti2015Stereo',\n",
       " 'KittiFlow',\n",
       " 'LFWPairs',\n",
       " 'LFWPeople',\n",
       " 'LSUN',\n",
       " 'LSUNClass',\n",
       " 'MNIST',\n",
       " 'Middlebury2014Stereo',\n",
       " 'MovingMNIST',\n",
       " 'Omniglot',\n",
       " 'OxfordIIITPet',\n",
       " 'PCAM',\n",
       " 'PhotoTour',\n",
       " 'Places365',\n",
       " 'QMNIST',\n",
       " 'RenderedSST2',\n",
       " 'SBDataset',\n",
       " 'SBU',\n",
       " 'SEMEION',\n",
       " 'STL10',\n",
       " 'SUN397',\n",
       " 'SVHN',\n",
       " 'SceneFlowStereo',\n",
       " 'Sintel',\n",
       " 'SintelStereo',\n",
       " 'StanfordCars',\n",
       " 'UCF101',\n",
       " 'USPS',\n",
       " 'VOCDetection',\n",
       " 'VOCSegmentation',\n",
       " 'VisionDataset',\n",
       " 'WIDERFace',\n",
       " '__all__',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__getattr__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '_optical_flow',\n",
       " '_stereo_matching',\n",
       " 'caltech',\n",
       " 'celeba',\n",
       " 'cifar',\n",
       " 'cityscapes',\n",
       " 'clevr',\n",
       " 'coco',\n",
       " 'country211',\n",
       " 'dtd',\n",
       " 'eurosat',\n",
       " 'fakedata',\n",
       " 'fer2013',\n",
       " 'fgvc_aircraft',\n",
       " 'flickr',\n",
       " 'flowers102',\n",
       " 'folder',\n",
       " 'food101',\n",
       " 'gtsrb',\n",
       " 'hmdb51',\n",
       " 'imagenet',\n",
       " 'imagenette',\n",
       " 'inaturalist',\n",
       " 'kinetics',\n",
       " 'kitti',\n",
       " 'lfw',\n",
       " 'lsun',\n",
       " 'mnist',\n",
       " 'moving_mnist',\n",
       " 'omniglot',\n",
       " 'oxford_iiit_pet',\n",
       " 'pcam',\n",
       " 'phototour',\n",
       " 'places365',\n",
       " 'rendered_sst2',\n",
       " 'sbd',\n",
       " 'sbu',\n",
       " 'semeion',\n",
       " 'stanford_cars',\n",
       " 'stl10',\n",
       " 'sun397',\n",
       " 'svhn',\n",
       " 'ucf101',\n",
       " 'usps',\n",
       " 'utils',\n",
       " 'video_utils',\n",
       " 'vision',\n",
       " 'voc',\n",
       " 'widerface']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WQjlYm3I3bIj",
    "outputId": "4f2208c5-404e-479c-e569-ef682483aa5c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 26.4M/26.4M [00:01<00:00, 14.7MB/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 29.5k/29.5k [00:00<00:00, 377kB/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4.42M/4.42M [00:00<00:00, 6.48MB/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5.15k/5.15k [00:00<00:00, 3.90MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset FashionMNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: data\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: ToTensor()"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision.transforms import ToTensor\n",
    "train_source = training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "train_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 203
    },
    "id": "uq7YInbH3z4K",
    "outputId": "5cbee41b-e768-4953-e38a-7a8dedaea46d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torchvision.datasets.mnist.FashionMNIST"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CI0VZXWe5oL_",
    "outputId": "f5af6fdc-f1ca-427f-ecb5-c8871d190939"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of the data: torch.Size([60000, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "print(f\"the shape of the data: {train_source.data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QvCx0iKV5uPl"
   },
   "source": [
    "We can visualize the data using matplotlib:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "UpM3phXL41eL"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 502
    },
    "id": "UGVwuqTP4w6B",
    "outputId": "e311a725-aa01-4e02-9af6-37bbbe935548"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels_map = {\n",
    "    0: \"T-Shirt\",\n",
    "    1: \"Trouser\",\n",
    "    2: \"Pullover\",\n",
    "    3: \"Dress\",\n",
    "    4: \"Coat\",\n",
    "    5: \"Sandal\",\n",
    "    6: \"Shirt\",\n",
    "    7: \"Sneaker\",\n",
    "    8: \"Bag\",\n",
    "    9: \"Ankle Boot\",\n",
    "}\n",
    "figure = plt.figure(figsize=(8, 8))\n",
    "cols, rows = 5, 2\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = torch.randint(len(train_source), size=(1,)).item()\n",
    "    img, label = train_source[sample_idx]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(labels_map[label])\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WUArfaqcuF9p"
   },
   "source": [
    "### DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ptoV6URJtMyg"
   },
   "source": [
    "As mentioned above, the `torch.utils.data.DataLoader` is responsible for handling data during the deep learning model training.\n",
    "\n",
    "Key Parameters:\n",
    "\n",
    "* dataset (Dataset): The PyTorch dataset you want to load data from (your custom class representing the data and how to access samples and labels).\n",
    "* batch_size (int, optional): The number of samples in a batch (default: 1).\n",
    "* shuffle (bool, optional): Whether to shuffle the data at the beginning of each epoch (default: False).\n",
    "* sampler (Sampler, optional): A custom sampler object for controlling data loading order (default: None).\n",
    "* collate_fn (callable, optional): A function to customize how samples within a batch are combined (default: None). Useful for padding sequences.\n",
    "* pin_memory=True (bool, optional): If using a GPU, pin fetched data tensors in pinned memory for faster transfer (default: False)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "id": "4XyudTrorICU",
    "outputId": "668b9111-3282-4415-a9dd-21393d0dead3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.utils.data.dataloader.DataLoader"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = DataLoader(train_source, batch_size=64, shuffle=True)\n",
    "type(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FdKFXjkH6NPg"
   },
   "source": [
    "\n",
    "\n",
    "> The Dataset retrieves our dataset’s features and labels one sample at a time. While training a model, we typically want to pass samples in “minibatches”, reshuffle the data at every epoch to reduce model overfitting, and use Python’s multiprocessing to speed up data retrieval.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vBYbBhat6DBf",
    "outputId": "da419003-dfe2-4a56-eb14-1a2ad320b760"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch shape: torch.Size([64, 1, 28, 28])\n",
      "batch shape: torch.Size([64, 1, 28, 28])\n",
      "batch shape: torch.Size([64, 1, 28, 28])\n",
      "batch shape: torch.Size([64, 1, 28, 28])\n",
      "batch shape: torch.Size([64, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "  btch = next(iter(train))\n",
    "  print(f\"batch shape: {btch[0].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KOok4gOvaBuW"
   },
   "source": [
    "## Practice Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B3Fby9XPaJjR"
   },
   "source": [
    "Produce the following tensors:  \n",
    "\n",
    "1. Create a tensor from the list `[1, 2, 3]`\n",
    "2. Create a tensor from a NumPy array of shape (50,10)\n",
    "3. Specify a data type and device option for a tensor with `[7, 8, 9]`, `[.5,0,.7]`\n",
    "4. Create a zero tensor of size (3, 4)\n",
    "5. Create a ones tensor of size (2, 2, 2) with dtype float\n",
    "6. Create a tensor of size (5, 5) with random values from a normal distribution\n",
    "7. Create a new tensor by cloning an existing tensor\n",
    "8. Create a new tensor by reshaping an existing tensor\n",
    "9. Create a new tensor by concatenating two existing tensors\n",
    "10.  Perform operations with different data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "ASqrBB6KeImk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[7.0000, 8.0000, 9.0000],\n",
      "        [0.5000, 0.0000, 0.7000]])\n",
      "tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]])\n",
      "tensor([[[1., 1.],\n",
      "         [1., 1.]],\n",
      "\n",
      "        [[1., 1.],\n",
      "         [1., 1.]]])\n",
      "Original Tensor:\n",
      " tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "Cloned Tensor:\n",
      " tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "Original shape: torch.Size([2, 4])\n",
      "Reshaped Tensor:\n",
      " tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [5, 6],\n",
      "        [7, 8]])\n",
      "New shape: torch.Size([4, 2])\n",
      "Concatenated along dim=0:\n",
      " tensor([[ 1,  2,  3],\n",
      "        [ 4,  5,  6],\n",
      "        [ 7,  8,  9],\n",
      "        [10, 11, 12]])\n",
      "Int Tensor dtype: torch.int32\n",
      "Float Tensor dtype: torch.float32\n",
      "Result Tensor: tensor([1.5000, 2.5000, 3.5000])\n",
      "Result dtype: torch.float32\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "import torch\n",
    "#1\n",
    "tensor_list = torch.tensor([1, 2, 3])\n",
    "#2\n",
    "import numpy as np\n",
    "np_array = np.array([50, 10])\n",
    "#3\n",
    "tensor = torch.tensor(\n",
    "    [[7, 8, 9],\n",
    "     [0.5, 0, 0.7]],\n",
    "    dtype=torch.float32,     \n",
    "    device='cpu'        #can change to gpu \n",
    ")\n",
    "print(tensor)\n",
    "#4 \n",
    "zeros = torch.zeros(3,4)\n",
    "print(zeros)\n",
    "#5\n",
    "ones = torch.ones([2,2,2] ,\n",
    "                 dtype = torch.float32)\n",
    "print(ones)\n",
    "#6\n",
    "rand_tensor = torch.randn(5,5)\n",
    "#7\n",
    "original = torch.tensor([[1, 2], [3, 4]])\n",
    "\n",
    "clone_tensor = original.clone()\n",
    "\n",
    "print(\"Original Tensor:\\n\", original)\n",
    "print(\"Cloned Tensor:\\n\", clone_tensor)\n",
    "\n",
    "#8\n",
    "\n",
    "original = torch.tensor([\n",
    "    [1, 2, 3, 4],\n",
    "    [5, 6, 7, 8]\n",
    "])\n",
    "\n",
    "print(\"Original shape:\", original.shape)\n",
    "\n",
    "reshaped = original.reshape(4, 2)\n",
    "\n",
    "print(\"Reshaped Tensor:\\n\", reshaped)\n",
    "print(\"New shape:\", reshaped.shape)\n",
    "\n",
    "#9 \n",
    "\n",
    "tensor_a = torch.tensor([[1, 2, 3],\n",
    "                         [4, 5, 6]])\n",
    "\n",
    "tensor_b = torch.tensor([[7, 8, 9],\n",
    "                         [10, 11, 12]])\n",
    "\n",
    "concat_0 = torch.cat((tensor_a, tensor_b), dim=0)\n",
    "print(\"Concatenated along dim=0:\\n\", concat_0)\n",
    "\n",
    "#10\n",
    "\n",
    "\n",
    "# integer\n",
    "int_tensor = torch.tensor([1, 2, 3], dtype=torch.int32)\n",
    "\n",
    "#  (float)\n",
    "float_tensor = torch.tensor([0.5, 0.5, 0.5], dtype=torch.float32)\n",
    "\n",
    "result = int_tensor + float_tensor\n",
    "\n",
    "print(\"Int Tensor dtype:\", int_tensor.dtype)\n",
    "print(\"Float Tensor dtype:\", float_tensor.dtype)\n",
    "print(\"Result Tensor:\", result)\n",
    "print(\"Result dtype:\", result.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TvL6c_cEboV_"
   },
   "source": [
    "1. Inspect the  attributes of some of the tensors you created with `shape,dtype,numel`.\n",
    "2. Indexing to access specific elements: `[[1, 2, 3], [4, 5, 6]]`, get the '2' element.\n",
    "\n",
    "3. Modify modify the '5' element to '10'\n",
    "4. Apply element-wise addition to two tensors.\n",
    "5. Apply element-wise multiplication to two tensors\n",
    "6. Apply a square root to a tensor.\n",
    "7. Create a tensor with random values from a uniform distribution between 0 and 1.\n",
    "8. Create a tensor with random values from a normal distribution with mean 0 and standard deviation 1\n",
    "9. Create a tensor with random values from a discrete uniform distribution between 1 and 10.\n",
    "10. Reshape a tensor from size `(2, 3)` to `(3, 2)`\n",
    "11. Transpose a tensor\n",
    "12. Concatenate tensors along a specific dimension\n",
    "13. Stack tensors along a new dimension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "d_eeWx27eLGJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor_list → shape: torch.Size([3]) , dtype: torch.int64 , numel: 3\n",
      "tensor → shape: torch.Size([2, 3]) , dtype: torch.float32 , numel: 6\n",
      "zeros → shape: torch.Size([3, 4]) , dtype: torch.float32 , numel: 12\n",
      "ones → shape: torch.Size([2, 2, 2]) , dtype: torch.float32 , numel: 8\n",
      "rand_tensor → shape: torch.Size([5, 5]) , dtype: torch.float32 , numel: 25\n",
      "tensor(2)\n",
      "tensor([[ 1,  2,  3],\n",
      "        [ 4, 10,  6]])\n",
      "Tensor A:\n",
      " tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "Tensor B:\n",
      " tensor([[ 7,  8,  9],\n",
      "        [10, 11, 12]])\n",
      "Element-wise Addition:\n",
      " tensor([[ 8, 10, 12],\n",
      "        [14, 16, 18]])\n",
      "Element-wise Multiplication:\n",
      " tensor([[ 7, 16, 27],\n",
      "        [40, 55, 72]])\n",
      "Original Tensor:\n",
      " tensor([[ 7, 16, 27],\n",
      "        [40, 55, 72]])\n",
      "After Square Root:\n",
      " tensor([[2.6458, 4.0000, 5.1962],\n",
      "        [6.3246, 7.4162, 8.4853]])\n",
      "Uniform Distribution Tensor:\n",
      " tensor([[0.1087, 0.4980, 0.8176],\n",
      "        [0.4351, 0.4320, 0.4507],\n",
      "        [0.5777, 0.6524, 0.6613]])\n",
      "Normal Distribution Tensor:\n",
      " tensor([[ 0.2358,  0.6143,  3.0696],\n",
      "        [ 0.1295,  1.0860, -1.5281],\n",
      "        [-0.3642,  0.9178, -1.7745]])\n",
      "Discrete Uniform Tensor:\n",
      " tensor([[7, 8, 4],\n",
      "        [6, 5, 4],\n",
      "        [3, 8, 4]])\n",
      "Original shape: torch.Size([2, 3])\n",
      "Reshaped tensor:\n",
      "tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [5, 6]])\n",
      "New shape: torch.Size([3, 2])\n",
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n",
      "Concatenate along dim=0:\n",
      "tensor([[ 1,  2,  3],\n",
      "        [ 4,  5,  6],\n",
      "        [ 7,  8,  9],\n",
      "        [10, 11, 12]])\n",
      "Shape: torch.Size([4, 3])\n",
      "Stacked along new dim 0:\n",
      "tensor([[[ 1,  2,  3],\n",
      "         [ 4,  5,  6]],\n",
      "\n",
      "        [[ 7,  8,  9],\n",
      "         [10, 11, 12]]])\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "#1\n",
    "\n",
    "print(\"tensor_list → shape:\", tensor_list.shape, \", dtype:\", tensor_list.dtype, \", numel:\", tensor_list.numel())\n",
    "print(\"tensor → shape:\", tensor.shape, \", dtype:\", tensor.dtype, \", numel:\", tensor.numel())\n",
    "print(\"zeros → shape:\", zeros.shape, \", dtype:\", zeros.dtype, \", numel:\", zeros.numel())\n",
    "print(\"ones → shape:\", ones.shape, \", dtype:\", ones.dtype, \", numel:\", ones.numel())\n",
    "print(\"rand_tensor → shape:\", rand_tensor.shape, \", dtype:\", rand_tensor.dtype, \", numel:\", rand_tensor.numel())\n",
    "\n",
    "#2 \n",
    "element_tensor = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "print(element_tensor[0][1])\n",
    "\n",
    "#3\n",
    "element_tensor[1][1] = 10\n",
    "print(element_tensor)\n",
    "\n",
    "#4\n",
    "import torch\n",
    "\n",
    "tensor_a = torch.tensor([[1, 2, 3],\n",
    "                         [4, 5, 6]])\n",
    "\n",
    "tensor_b = torch.tensor([[7, 8, 9],\n",
    "                         [10, 11, 12]])\n",
    "\n",
    "sum_tensor = tensor_a + tensor_b\n",
    "\n",
    "print(\"Tensor A:\\n\", tensor_a)\n",
    "print(\"Tensor B:\\n\", tensor_b)\n",
    "print(\"Element-wise Addition:\\n\", sum_tensor)\n",
    "\n",
    "#5\n",
    "elementwise_mult = tensor_a * tensor_b\n",
    "print(\"Element-wise Multiplication:\\n\", elementwise_mult)\n",
    "#6\n",
    "sqrt_tensor = torch.sqrt(elementwise_mult)\n",
    "\n",
    "print(\"Original Tensor:\\n\", elementwise_mult)\n",
    "print(\"After Square Root:\\n\", sqrt_tensor)\n",
    "\n",
    "#7\n",
    "\n",
    "uniform_tensor = torch.rand(3, 3)   # ברירת מחדל: Uniform(0, 1)\n",
    "print(\"Uniform Distribution Tensor:\\n\", uniform_tensor)\n",
    "#8\n",
    "normal_tensor = torch.randn(3, 3)   # Normal(0, 1)\n",
    "print(\"Normal Distribution Tensor:\\n\", normal_tensor)\n",
    "#9 \n",
    "\n",
    "discrete_uniform_tensor = torch.randint(low=1, high=11, size=(3, 3))\n",
    "print(\"Discrete Uniform Tensor:\\n\", discrete_uniform_tensor)\n",
    "\n",
    "#10 \n",
    "import torch\n",
    "\n",
    "x = torch.tensor([[1, 2, 3],\n",
    "                  [4, 5, 6]])\n",
    "\n",
    "print(\"Original shape:\", x.shape)\n",
    "\n",
    "y = x.reshape(3, 2)\n",
    "\n",
    "print(\"Reshaped tensor:\")\n",
    "print(y)\n",
    "print(\"New shape:\", y.shape)\n",
    "#11\n",
    "\n",
    "y = x.t()\n",
    "print(y)\n",
    "\n",
    "#12\n",
    "a = torch.tensor([[1, 2, 3],\n",
    "                  [4, 5, 6]])\n",
    "\n",
    "b = torch.tensor([[7, 8, 9],\n",
    "                  [10, 11, 12]])\n",
    "\n",
    "# Concatenate along dimension 0 (rows)\n",
    "cat_dim0 = torch.cat((a, b), dim=0)\n",
    "print(\"Concatenate along dim=0:\")\n",
    "print(cat_dim0)\n",
    "print(\"Shape:\", cat_dim0.shape)\n",
    "\n",
    "#13 \n",
    "c = torch.stack((a, b), dim=0)\n",
    "print(\"Stacked along new dim 0:\")\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q5Ag4aLvd186"
   },
   "source": [
    "1. Sum of all elements in a tensor.\n",
    "2. Mean along a specific dimension\n",
    "3. Find the minimum and maximum values in a tensor\n",
    "4. Find the indices of minimum and maximum values in a tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "YS_zcuzPeL2y"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(21)\n",
      "Overall mean: tensor(3.5000)\n",
      "Mean along columns: tensor([2.5000, 3.5000, 4.5000])\n",
      "Min value: tensor(1.)\n",
      "Max value: tensor(6.)\n",
      "Index of min: tensor(0)\n",
      "Index of max: tensor(5)\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "#1\n",
    "import torch\n",
    "\n",
    "x = torch.tensor([[1, 2, 3],\n",
    "                  [4, 5, 6]])\n",
    "\n",
    "total = x.sum()\n",
    "print(total)\n",
    "\n",
    "#2\n",
    "x = x.float()\n",
    "mean_all = x.mean()\n",
    "print(\"Overall mean:\", mean_all)\n",
    "\n",
    "mean_cols = x.mean(dim=0)\n",
    "print(\"Mean along columns:\", mean_cols)\n",
    "\n",
    "#3\n",
    "x_min = x.min()\n",
    "x_max = x.max()\n",
    "\n",
    "print(\"Min value:\", x_min)\n",
    "print(\"Max value:\", x_max)\n",
    "#4\n",
    "idx_min = x.argmin()\n",
    "idx_max = x.argmax()\n",
    "\n",
    "print(\"Index of min:\", idx_min)\n",
    "print(\"Index of max:\", idx_max)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZUhF40qoc9lh"
   },
   "source": [
    "Broadcasting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WZ3jXo9F7YyB",
    "outputId": "543e3f5f-4eea-44cd-88ff-dec7da30f9f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Broadcasting with scalars:\n",
      "tensor([6, 7, 8])\n",
      "Broadcasting with different shapes:\n",
      "tensor([[11, 22, 33],\n",
      "        [14, 25, 36]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Broadcasting with scalars\n",
    "tensor_a = torch.tensor([1, 2, 3])\n",
    "scalar_b = 5\n",
    "result_broadcast_scalar = tensor_a + scalar_b\n",
    "print(\"Broadcasting with scalars:\")\n",
    "print(result_broadcast_scalar)  # Output: tensor([6, 7, 8])\n",
    "\n",
    "# Broadcasting with different shapes\n",
    "tensor_c = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "tensor_d = torch.tensor([10, 20, 30])\n",
    "result_broadcast_shape = tensor_c + tensor_d\n",
    "print(\"Broadcasting with different shapes:\")\n",
    "print(result_broadcast_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CTTx-zG3dFzd",
    "outputId": "3d7074f6-1d0e-4f19-d999-7b75699d04fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Broadcasting with multidimensional tensors:\n",
      "tensor([[11, 22, 33],\n",
      "        [14, 25, 36]])\n",
      "RuntimeError: The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 1\n"
     ]
    }
   ],
   "source": [
    "# Broadcasting with multidimensional tensors\n",
    "tensor_a = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "tensor_b = torch.tensor([10, 20, 30])\n",
    "result_broadcast = tensor_a + tensor_b\n",
    "print(\"Broadcasting with multidimensional tensors:\")\n",
    "print(result_broadcast)\n",
    "\n",
    "# Common broadcasting pitfalls\n",
    "tensor_c = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "tensor_d = torch.tensor([10, 20])\n",
    "# The following line will raise a RuntimeError\n",
    "try:\n",
    "    result_pitfall = tensor_c + tensor_d\n",
    "except RuntimeError as e:\n",
    "    print(\"RuntimeError:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d8XnR20UdL9x"
   },
   "source": [
    "Device specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UmOPlcDndLei",
    "outputId": "8218740c-386b-4509-dcd2-1875b62b1c6d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is GPU available? False\n",
      "Tensor on GPU: tensor([1, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "# Code examples for device configuration and availability\n",
    "\n",
    "# Check if GPU is available\n",
    "is_gpu_available = torch.cuda.is_available()\n",
    "print(\"Is GPU available?\", is_gpu_available)\n",
    "\n",
    "# Specify device for tensor operations\n",
    "device = torch.device('cuda' if is_gpu_available else 'cpu')\n",
    "tensor_gpu = torch.tensor([1, 2, 3], device=device)\n",
    "print(\"Tensor on GPU:\", tensor_gpu)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aTb9_VWsgCkv"
   },
   "source": [
    "Dataloader practice:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A-XSmZ7Tffq2"
   },
   "source": [
    "Using the Fashion MNIST dataloader object, iterate over the batches, and calculate an interesting statistic. Make sure the statistic represent the entire data, not just a single batch. Hint- find a way to \"update\" the statistic along the batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "ED4PArw7e43C"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is GPU available? False\n",
      "Total images: 60000\n",
      "Mean per channel: [0.28604060411453247]\n",
      "Std  per channel: [0.3530242443084717]\n",
      "Class counts: [6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000]\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# 1) Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Is GPU available?\", torch.cuda.is_available())\n",
    "\n",
    "# 2) Dataset & DataLoader (train set)\n",
    "transform = transforms.ToTensor()  # converts to [0,1], shape [C,H,W]\n",
    "train_ds = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_ds, batch_size=256, shuffle=False, num_workers=2, pin_memory=torch.cuda.is_available())\n",
    "\n",
    "# 3) Running stats: mean/std via cumulative sums (efficient & precise)\n",
    "n_channels = 1  # FashionMNIST is grayscale\n",
    "count = 0\n",
    "sum_ = torch.zeros(n_channels, dtype=torch.float64)\n",
    "sum_sq = torch.zeros(n_channels, dtype=torch.float64)\n",
    "\n",
    "# Class distribution\n",
    "num_classes = 10\n",
    "class_counts = torch.zeros(num_classes, dtype=torch.long)\n",
    "\n",
    "for images, labels in train_loader:\n",
    "    # images: [B, C, H, W] in [0,1]\n",
    "    b = images.size(0)\n",
    "    class_counts += torch.bincount(labels, minlength=num_classes)\n",
    "\n",
    "    # reshape to [B, C, H*W] \n",
    "    imgs = images.to(torch.float64)\n",
    "    sum_   += imgs.sum(dim=(0, 2, 3))\n",
    "    sum_sq += (imgs * imgs).sum(dim=(0, 2, 3))\n",
    "    count  += b * imgs.shape[2] * imgs.shape[3]  \n",
    "\n",
    "# 4) Final stats\n",
    "mean = (sum_ / count).to(torch.float32)\n",
    "var  = (sum_sq / count - mean.to(torch.float64)**2).to(torch.float32)\n",
    "std  = torch.sqrt(var.clamp(min=0))\n",
    "\n",
    "print(f\"Total images: {len(train_ds)}\")\n",
    "print(f\"Mean per channel: {mean.tolist()}\")\n",
    "print(f\"Std  per channel: {std.tolist()}\")\n",
    "print(\"Class counts:\", class_counts.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NrhHnCUOXeTY"
   },
   "source": [
    "## How to export an `HTML` file of your `ipynb`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-yvoSJjmXqah"
   },
   "source": [
    "1. Save your notebook and make sure all the cells have the expected output.\n",
    "2. Download your notebook to your local machine.   `File-->Download-->Download ipynb`\n",
    "3.  Reupload it so Colab can see it :  Click on the Files icon on the far left bar--> you should see your current kernel folder --> click `upload to session storage` --> upload your file.\n",
    "4. Execute the following: `!jupyter nbconvert --to html /content/NOTEBOOKFILE.ipynb`\n",
    "5. You should see the html file produced in your kernel's current folder. You can download it locally.\n",
    "[link text](https://)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZKpF6F5E741a"
   },
   "source": [
    " END OF PyTorch BASICS"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
